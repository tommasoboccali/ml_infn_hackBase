{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d831f4d",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/tommasoboccali/ml_infn_hackBase/blob/main/btaggingTom_May26_nocelloutput_students.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-blocking",
   "metadata": {
    "id": "spoken-blocking"
   },
   "source": [
    "# CMS bTagging Exercise\n",
    "\n",
    "The aim of the exercise is to prepare a ML tool which is able to discriminate as much as possible between jets coming from the hadronization of light quarks and gluons, and b quarks; jets from c quarks being an intermediate case.\n",
    "\n",
    "In this exercise you will\n",
    "- explore the content of a simulate dataset from CMS containing per jet features to be used t|o test the discrimination\n",
    "- try to plot quantities and understand what can be used\n",
    "- train various (?) network type in order to perform the job, and test their abilities; this can be done at various complexity levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6523cf8",
   "metadata": {
    "id": "e6523cf8"
   },
   "source": [
    "#### The exercise is largely (completely?) based on the work by L.Giannini (UCSD). See the ML_INFN knowledge basis ([here](https://confluence.infn.it/display/MLINFN/1.+Btagging+in+CMS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-programmer",
   "metadata": {
    "id": "meaningful-programmer"
   },
   "source": [
    "First of all, you need to access the data. We provide 2 files in numpy format; in most use cases you will use only the first splitting the samples in train and test. the second can be used in more complex cases.\n",
    "In order to get the data files, you can use Linux Shell commands (**\"cp\", \"wget\", ...**) to put the input files in a given place. Here, we put the two files (**\"test93_0_20000.npz\" and \"test93_20000_40000.npz\"**) in a local directory, from a public web server.\n",
    "\n",
    "In order not to repeat the download N times, we check if the file is alredy available.\n",
    "\n",
    "The \"! command\" syntax in Jupyter executes a shell command, so we can use it to download the file and check later if it is ok. It should be ~ 40 MB. If it is less than that, you should remove it and retry (**\"!rm -f test93_0_20000.npz\"**).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba90112",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First of all, since this is a workgroup, we disable autosave in order not to mess the notebooks; the output should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2648af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-rebecca",
   "metadata": {
    "id": "compatible-rebecca"
   },
   "outputs": [],
   "source": [
    "# next line can be needed if you get a strange error when declaring an LSTM. After uncommenting it, you need to restart the otebook from scratch\n",
    "#\n",
    "#!pip install -U numpy==1.19\n",
    "import os\n",
    "if not os.path.isfile('test93_0_20000.npz'):\n",
    " !wget  --no-check-certificate http://www.pi.infn.it/~boccali/test93_0_20000.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-norwegian",
   "metadata": {
    "id": "substantial-norwegian"
   },
   "outputs": [],
   "source": [
    "!ls -l test*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-relay",
   "metadata": {
    "id": "improved-relay"
   },
   "source": [
    "The numpy file contains 2 arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-stamp",
   "metadata": {
    "id": "raising-stamp"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "f=numpy.load(\"test93_0_20000.npz\")\n",
    "\n",
    "print (f.files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-conflict",
   "metadata": {
    "id": "southwest-conflict"
   },
   "source": [
    "In the first part of this exercise you will use only the first one (**\"arr_0\"**); we want to see its shape. Remember that a numpy array has 1 line per entry, and the columns are the various features someone (CMS in this case) has prepared per entry.\n",
    "In particular, an \"entry\" here is a single hadronic jet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-enforcement",
   "metadata": {
    "id": "legal-enforcement"
   },
   "outputs": [],
   "source": [
    "## EDIT which is the shape of arr_0? How do you print it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-beauty",
   "metadata": {
    "id": "simplified-beauty"
   },
   "source": [
    "So the file contains 20000 jets, and for each of them 52 \"features\". The file does NOT contain an explanation of what they are, which needs to be provided externally. In this particular case, these are the 52 numbers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-trash",
   "metadata": {
    "id": "abroad-trash"
   },
   "source": [
    "## Ntuple content\n",
    "\n",
    "### Jet kinematics\n",
    "0 - **\"jet_pt\"** : the transverse momentum of the jet\n",
    "\n",
    "1 - **\"jet_eta\"** : the radipity of the jet (see https://en.wikipedia.org/wiki/Pseudorapidity)\n",
    "\n",
    "### Jet constituents in terms of tracks and vertices\n",
    "2 - **\"nCpfcand\"** : number of charged object candidates in the CMS particle Flow reconstruction\n",
    "\n",
    "3 - **\"nNpfcand\"** : number of neutral object candidates in the CMS particle Flow reconstruction\n",
    "\n",
    "4 - **\"nsv\"** : number of secondary vertices in the jet\n",
    "\n",
    "5 - **\"npv\"** : number of primary vertices in the event\n",
    "\n",
    "6 - **\"n_seeds\"** : number of tracks associated to the jet\n",
    "\n",
    "### b tagging discriminataing variables\n",
    "\n",
    "7 - **\"TagVarCSV_trackSumJetEtRatio** : input variables used in input to algorithms  (for CSV algorithm, here), see *https://cds.cern.ch/record/2298594/files/arXiv:1712.07158.pdf* \n",
    "\n",
    "8 - **\"TagVarCSV_trackSumJetDeltaR\"** : as above\n",
    "\n",
    "9 - **\"TagVarCSV_vertexCategory\"** : as above\n",
    "\n",
    "10 - **\"TagVarCSV_trackSip2dValAboveCharm\"** : as above\n",
    "\n",
    "11 - **\"TagVarCSV_trackSip2dSigAboveCharm\"** : as above\n",
    "\n",
    "12 - **\"TagVarCSV_trackSip3dValAboveCharm\"** : as above\n",
    "\n",
    "13 - **\"TagVarCSV_trackSip3dSigAboveCharm\"** : as above\n",
    "\n",
    "14 - **\"TagVarCSV_jetNSelectedTracks\"** : as above\n",
    "\n",
    "15 - **\"TagVarCSV_jetNTracksEtaRel\"** : as above\n",
    "\n",
    "\n",
    "### other kinematics variables of the jet\n",
    "16 - **\"jet_corr_pt\"** : jet corrected Pt (corrected for known miscalibrations)\n",
    "\n",
    "17 - **\"jet_phi\"** :  jet phi angle\n",
    "\n",
    "18 - **\"jet_mass\"** : jet mass, omputed from its constituents\n",
    "\n",
    "### detailed Monte Carlo truth of the event (categories are NOT exclusive!)\n",
    "19 - **\"isB\"** : is the jet coming from the decay of a B hadron?\n",
    "\n",
    "20 - **\"isGBB\"** : is the jet coming from the a gluon splitting to BB?\n",
    "\n",
    "21 - **\"isBB\"** : is the jet coming from a BB pair (not from gluon splitting)?\n",
    "\n",
    "22 - **\"isLeptonicB\"** : is the jet coming from a leptonic decay of a B hadron?\n",
    "\n",
    "23 - **\"isLeptonicB_C\"** : is the jet coming from a leptonic decay of a D hadron coming from a B hadron?\n",
    "\n",
    "24 - **\"isC\"** : is the jet coming from a charm hadron decay?\n",
    "\n",
    "25 - **\"isGCC\"** : is the jet coming from the a gluon splitting to CC?\n",
    "\n",
    "26 - **\"isCC\"** : is the jet coming from a CC pair (not from gluon splitting)?\n",
    "\n",
    "27 - **\"isUD\"** : is the jet coming from a Up or Down hadron(no heavier quarks)?\n",
    "\n",
    "28 - **\"isS\"** : is the jet coming from a Strange hadron(no heavier quarks)?\n",
    "\n",
    "29 - **\"isG\"** : is the jet coming from a a gluon?\n",
    "\n",
    "30 - **\"isUndefined\"** : special case in which there was no  clear identification\n",
    "\n",
    "31 - **\"isPhysB\"** : same as above, with an alternative definition\n",
    "\n",
    "32 - **\"isPhysGBB\"** : same as above, with an alternative definition\n",
    "\n",
    "33 - **\"isPhysBB\"** : same as above, with an alternative definition\n",
    "\n",
    "34 - **\"isPhysLeptonicB\"** : same as above, with an alternative definition\n",
    "\n",
    "35 - **\"isPhysLeptonicB_C\"** : same as above, with an alternative definition\n",
    "\n",
    "36 - **\"isPhysC\"** : same as above, with an alternative definition\n",
    "\n",
    "37 - **\"isPhysGCC\"** : same as above, with an alternative definition\n",
    "\n",
    "38 - **\"isPhysCC\"** : same as above, with an alternative definition\n",
    "\n",
    "39 - **\"isPhysUD\"** : same as above, with an alternative definition\n",
    "\n",
    "40 - **\"isPhysS\"** : same as above, with an alternative definition\n",
    "\n",
    "41 - **\"isPhysG\"** : same as above, with an alternative definition\n",
    "\n",
    "42 - **\"isPhysUndefined\"** : same as above, with an alternative definition\n",
    "\n",
    "### premade combinations for definig categories (6 of them)\n",
    "\n",
    "43 - **\"isB*1\"** : as above\n",
    "\n",
    "44 - **\"isBB+isGBB\"** : from B or GBB\n",
    "\n",
    "45 - **\"isLeptonicB+isLeptonicB_C\"** : there is a lepton from the B decay (either direct or via a Charm)\n",
    "\n",
    "46 - **\"isC+isGCC+isCC\"** : there is a Charm in the jet\n",
    "\n",
    "47 - **\"isUD+isS\"** : it is light\n",
    "\n",
    "48 - **\"isG*1\"** : from gluon\n",
    "\n",
    "49 - **\"isUndefined*1\"** : see above\n",
    "\n",
    "### truth as integer (used in the notebook)\n",
    "\n",
    "50 - **\"5x(isB+isBB+isGBB+isLeptonicB+isLeptonicB_C)+4x(isC+isGCC+isCC)+1x(isUD+isS)+21xisG+0xisUndefined\"** : expected to be 5 if there is a B, for if it is a C, 1 if it il light, 21 if from gluon\n",
    "\n",
    "### alternative definition\n",
    "51 - **\"5x(isPhysB+isPhysBB+isPhysGBB+isPhysLeptonicB+isPhysLeptonicB_C)+4x(isPhysC+isPhysGCC+isPhysCC)+1x(isPhysUD+isPhysS)+21xisPhysG+0xisPhysUndefined\"** : the alternative definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-stack",
   "metadata": {
    "id": "public-stack"
   },
   "source": [
    "We are not going to use all the variables (which ones can be your choice).\n",
    "Let's start by preparing a standard way to plot quantities, as a function of the True origin of the jet.\n",
    "\n",
    "We can use, fro example, \"arr_0[-2]\" (which is entry 50) as a definition of the origin:\n",
    "\n",
    "it basically does  \"5 \\*(any source of b quarks) + 4 \\*(any source of c quarks)+ 1 \\*(any source of light quarks) + 20\\*(any source of gluons)\".\n",
    "\n",
    "In the case of gluons, there is always also at least a light quark (so we expect 21, not 20). Consider this a quirk of the way CMS is labelling.\n",
    "\n",
    "Let's scan them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-account",
   "metadata": {
    "id": "judicial-account"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "jetvars=f['arr_0']\n",
    "print (\"MC Truth\",jetvars[:,-2])\n",
    "pyplot.hist(jetvars[:,-2],bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-volume",
   "metadata": {
    "id": "surgical-volume"
   },
   "source": [
    "You see there are many jets from gluons (20+), many from light quarks (1), and some from charm (4) and bottom (5) quarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-robertson",
   "metadata": {
    "id": "lucky-robertson"
   },
   "source": [
    "Let's do some better plots, for example coloring and putting labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-friendly",
   "metadata": {
    "id": "placed-friendly"
   },
   "outputs": [],
   "source": [
    "bins=range(0,24)\n",
    "pyplot.hist(jetvars[jetvars[:,-2]==21][:,-2], color=\"blue\", bins=bins, label=\"gluon jets\")\n",
    "pyplot.hist(jetvars[jetvars[:,-2]==5][:,-2], color=\"red\", bins=bins, label=\"b jets\")\n",
    "pyplot.hist(jetvars[jetvars[:,-2]==4][:,-2], color=\"green\", bins=bins, label=\"c jets\")\n",
    "pyplot.hist(jetvars[jetvars[:,-2]==1][:,-2], color=\"cyan\", bins=bins, label=\"light jets\")\n",
    "pyplot.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-artwork",
   "metadata": {
    "id": "incorporated-artwork"
   },
   "source": [
    "Now, we would like to plot some quantities for the 4 categories, and see that (if!) they have discrimination power. In order to do this, we need some \"tools\" to proper visualize it.\n",
    "\n",
    "We create a plotter function, which we can use to plot any of the 52 features by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-translator",
   "metadata": {
    "id": "valid-translator"
   },
   "outputs": [],
   "source": [
    "# Plotting function - you may use it or not\n",
    "# maybe you just want to check what it's doing\n",
    "\n",
    "def plotter_wbins(  data, categories, labels, colors, name=\"name___\", log=False, mybins=None, density=False):\n",
    "\n",
    "    mean=numpy.mean(data)\n",
    "    stdev=numpy.std(data)\n",
    "    lenbin=stdev/(len(data)**0.5)    \n",
    "    \n",
    "    if (lenbin==0) :\n",
    "        return\n",
    "    \n",
    "    binning=numpy.arange(mean-2*stdev, mean+2*stdev, lenbin*10)    \n",
    "    if mybins is not None:\n",
    "      binning=mybins\n",
    "\n",
    "    pyplot.clf()\n",
    "    \n",
    "    for i in range(len(categories)):\n",
    "        cat=categories[i]\n",
    "        color=colors[i]\n",
    "        label=labels[i]\n",
    "        \n",
    "        pyplot.hist(data[cat], color=color, label=label, histtype='step', bins=binning, log=log, density=density)\n",
    "\n",
    "    pyplot.legend()\n",
    "    pyplot.xlabel(name)\n",
    "    pyplot.ylabel(\"entries\")\n",
    "    pyplot.savefig(name+\".png\")\n",
    "    # Prepare function inputs\n",
    "\n",
    "isB=jetvars[:,-2]==5\n",
    "isC=jetvars[:,-2]==4\n",
    "isL=jetvars[:,-2]==1\n",
    "isG=jetvars[:,-2]==21\n",
    "\n",
    "categories=[isB, isC, isL, isG]\n",
    "labels=[\"b jets\",\"c jets\",\"light jets\",\"gluon jets\"]\n",
    "colors=[\"red\",\"green\",\"cyan\",\"blue\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-michigan",
   "metadata": {
    "id": "fancy-michigan"
   },
   "source": [
    "With this, we can plot a feature. For example, we plot arr_0[0] (which is the jet_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6IRkBFsMtjOc",
   "metadata": {
    "id": "6IRkBFsMtjOc"
   },
   "source": [
    "TB: best worst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-grade",
   "metadata": {
    "id": "matched-grade"
   },
   "source": [
    "You can try a few features, and see if you consider the difference enogh evident to try and use it in a discriminator. \n",
    "The features from 7 to 15 are the best. Do you understand why? is this expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-certification",
   "metadata": {
    "id": "compound-certification"
   },
   "source": [
    "In some cases, plotting in log y scale helps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-montana",
   "metadata": {
    "id": "supposed-montana"
   },
   "outputs": [],
   "source": [
    "plotter_wbins(jetvars[:,0], categories, labels, colors, name=\"jetPt\"+str(2), log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-packaging",
   "metadata": {
    "id": "israeli-packaging"
   },
   "source": [
    "Another useful way to look into discrimitating features, is to plot their \"density\", which means single histogram areas are nomalized to 1. See that in the previous function definition, you can can pass a density parmeter. Try it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-collectible",
   "metadata": {
    "id": "imported-collectible"
   },
   "source": [
    "You can have a look at, for example, the first ~15 features and identify those which seem more promising in your case.\n",
    "\n",
    "When you report in the afternoon session, please shoe a few of them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-peninsula",
   "metadata": {
    "id": "referenced-peninsula"
   },
   "source": [
    "A better way to understand the discriminating power of a feature is by plotting a ROC (https://en.wikipedia.org/wiki/Receiver_operating_characteristic) curve using that; please beware that in a standard ROC efficiency and rejection are plotted on axes (x,y). In our case, we plot efficience and efficiency on the background, the latter being 1/rejection. So the area which matter for us the the area ABOVE the curve, not below.\n",
    "\n",
    "What we need to do, is to put increasing cuts on that variable, which select some B and some \"not B\" events. By changin the cut, one can construct a plot in the plane \"B efficiency\" vs \"Non B efficiency\". Clearly a feature is better at selecting B events if it obtains an higher B efficiency at a fixed value of non B efficiency, or alternatively a lower non B efficiency at a fixed B efficiency.  \n",
    "\n",
    "There are many ways to quantify if a ROC is good; one of them is called Area Under the Curve (AUC, https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Let's prepare a tool which does that, with scikit-learn which already has a ROC and AUC feature.\n",
    "Let's plot it for feature #11, which should have discriminating power\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pLMThqvynJfh",
   "metadata": {
    "id": "pLMThqvynJfh"
   },
   "outputs": [],
   "source": [
    "## EDIT try and find some features you see useful for:\n",
    "## b vs light discrimination\n",
    "## b vs c discrimination\n",
    "\n",
    "## and some useless features ... do you understand why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-match",
   "metadata": {
    "id": "convinced-match"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Au3OWND_e95V",
   "metadata": {
    "id": "Au3OWND_e95V"
   },
   "source": [
    "How does it work?  From the documentation: [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) says **roc_curve(y_true, y_score, ...)**\n",
    "\n",
    "We need to pass \n",
    "   * y_true: the true labels (in our case the jet category) \n",
    "   * y_score: can either be probability estimates of the positive class (so a structure identical in structure to the previous one, but with probabilities instead of 0 and 1s), confidence values, or non-thresholded measure of decisions - in our case it is the latter: the variable on which to apply different cuts to scan the parametner space.\n",
    "\n",
    "The tool will prepare\n",
    "   * false positive rate: our y axes\n",
    "   * true positive rate : our x axes\n",
    "   * the thresholds\n",
    "   \n",
    "by scanning y_score in steps (there is a default). If you need (you don't...) the values chosen are in \"thresholds\"\n",
    "   \n",
    "\n",
    "We can create out of it a function which prepares the categories and feed them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-liquid",
   "metadata": {
    "id": "sharp-liquid"
   },
   "outputs": [],
   "source": [
    "def plot_roc_curve(isB,isC,isL, isG,variable):\n",
    "\n",
    "    #do you understand what it does? try and print it to understand ... for example, which is the length of isB[isB+isC]? what does it mean?\n",
    "    isBvsC = isB[isB+isC]\n",
    "    isBvsUSDG = isB[isB+isL+isG]\n",
    "\n",
    "    # B vs ALL roc curve\n",
    "\n",
    "    fpr, tpr, threshold = roc_curve(isB,variable)\n",
    "    auc1 = auc(fpr, tpr)\n",
    "\n",
    "    # B vs c\n",
    "\n",
    "    fpr2, tpr2, threshold = roc_curve(isBvsC,variable[isB+isC])\n",
    "    auc2 = auc(fpr2, tpr2)\n",
    "\n",
    "    # B vs light/gluon\n",
    "\n",
    "    fpr3, tpr3, threshold = roc_curve(isBvsUSDG, variable[isB+isL+isG])\n",
    "    auc3 = auc(fpr3, tpr3)\n",
    "\n",
    "    # print AUC\n",
    "\n",
    "    print (\"AUCs\", auc1, auc2, auc3)\n",
    "    \n",
    "    return tpr,fpr,tpr2,fpr2,tpr3,fpr3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q7dicVJjhGRb",
   "metadata": {
    "id": "q7dicVJjhGRb"
   },
   "outputs": [],
   "source": [
    "print(len (isB))\n",
    "print(len(isB[isB+isC]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-warrant",
   "metadata": {
    "id": "respiratory-warrant"
   },
   "outputs": [],
   "source": [
    "tpr,fpr,tpr2,fpr2,tpr3,fpr3 = plot_roc_curve(isB,isC,isL, isG,jetvars[:,11])\n",
    "\n",
    "pyplot.plot(tpr,fpr,label=\"vs all\")\n",
    "pyplot.plot(tpr2,fpr2,label=\"vs c\")\n",
    "pyplot.plot(tpr3,fpr3,label=\"vs l\")\n",
    "pyplot.xlabel(\"sig. efficiency\")\n",
    "pyplot.ylabel(\"bkg. mistag rate\")\n",
    "pyplot.ylim(0.000001,1)\n",
    "pyplot.grid(True)\n",
    "pyplot.legend(loc='lower right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "U8o2YaDIukQG",
   "metadata": {
    "id": "U8o2YaDIukQG"
   },
   "outputs": [],
   "source": [
    "tpr,fpr,tpr2,fpr2,tpr3,fpr3 = plot_roc_curve(isB,isC,isL, isG,jetvars[:,13])\n",
    "\n",
    "pyplot.plot(tpr,fpr,label=\"vs all\")\n",
    "pyplot.plot(tpr2,fpr2,label=\"vs c\")\n",
    "pyplot.plot(tpr3,fpr3,label=\"vs l\")\n",
    "pyplot.xlabel(\"sig. efficiency\")\n",
    "pyplot.ylabel(\"bkg. mistag rate\")\n",
    "pyplot.ylim(0.000001,1)\n",
    "pyplot.grid(True)\n",
    "pyplot.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-uncle",
   "metadata": {
    "id": "accessory-uncle"
   },
   "source": [
    "Since we expect that the y axes (non B efficiency) should be small, it is customary to plot it in log scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-prairie",
   "metadata": {
    "id": "impressed-prairie"
   },
   "outputs": [],
   "source": [
    "pyplot.plot(tpr,fpr,label=\"vs all\")\n",
    "pyplot.plot(tpr2,fpr2,label=\"vs c\")\n",
    "pyplot.plot(tpr3,fpr3,label=\"vs l\")\n",
    "pyplot.semilogy()\n",
    "pyplot.xlabel(\"sig. efficiency\")\n",
    "pyplot.ylabel(\"bkg. mistag rate\")\n",
    "pyplot.ylim(0.001,1)\n",
    "pyplot.grid(True)\n",
    "pyplot.legend(loc='lower right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-compound",
   "metadata": {
    "id": "russian-compound"
   },
   "source": [
    "Can you explain what it means? For example, what does the green point at x=0.2, y~0.0015 indicate? What do you expect from a GOOD algorithm wrt a BAD algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-lecture",
   "metadata": {
    "id": "domestic-lecture"
   },
   "source": [
    "Repeat the exercise with different input features. You will find that some of them have nearly no discriminant power. Can you find one? Can you explain if that is expected or not?\n",
    "\n",
    "### answer: feature 17 is the jet_phi, you expect no distinction between anything (it is just a flat distribuition). and indeed you get a ROC which is diagonal in the two efficiencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-convenience",
   "metadata": {
    "id": "friendly-convenience"
   },
   "source": [
    "Let's build a Dense Neural Network (DNN) to generate a new, more powerful disciminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-looking",
   "metadata": {
    "id": "fitting-looking"
   },
   "source": [
    "You can choose which features to use, among these with a larg(ish) discriminating power. You can chose your list, but:\n",
    "\n",
    "- does it make sense to use any feature above #18? Why?\n",
    "- from what you know about ML, is it important to choose only good ones? What happens if you include one with scarce / absent discriminating power?\n",
    "\n",
    "So this is the plan here: \n",
    "- define a subset of the features\n",
    "- build with Keras a dense neural network using that in input\n",
    "- define train and test datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-monday",
   "metadata": {
    "id": "adjustable-monday"
   },
   "source": [
    "tarting for the \"jetvars == arr_0\" used above, prepare a new numpy array with only the features you want to consider. Just for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-white",
   "metadata": {
    "id": "sudden-white"
   },
   "outputs": [],
   "source": [
    "# choose some feature on your side, avoid MC truth ! (Is it clear why???)\n",
    "# for example\n",
    "# jetInfo = jetvars[:,[0,1]] uses only the first and second, in general try and use as many as possible: even if you do not really see the use of them, there can be hidden correlations ML can see!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-massage",
   "metadata": {
    "id": "convinced-massage"
   },
   "source": [
    "Check that this is what you expect: a numpy array with one jet per line , containing the colums you decided to select. Print the first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-chambers",
   "metadata": {
    "id": "auburn-chambers"
   },
   "outputs": [],
   "source": [
    "## EDIT print the first column, what is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-touch",
   "metadata": {
    "id": "chinese-touch"
   },
   "source": [
    "Prepare also a syncronized numpy array with the terget (the Monte Carlo Truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-amazon",
   "metadata": {
    "id": "sharing-amazon"
   },
   "outputs": [],
   "source": [
    "jetCategories=jetvars[:,-9:-3]\n",
    "## EDIT in which other way you could write -9:-3?\n",
    "print (jetCategories)\n",
    "print (jetvars[:,[48]])\n",
    "\n",
    "# why only 0 or 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-ordinary",
   "metadata": {
    "id": "lovely-ordinary"
   },
   "source": [
    "Do you understand what this means? Which features are selected in jetCategories?\n",
    "\n",
    "If we train a network with these output features, we are not simply trying the \"B or not B\" categories, but a finer graded categorization. Feel free to change it if you want different results / grain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-pharmacy",
   "metadata": {
    "id": "parliamentary-pharmacy"
   },
   "source": [
    "At this point, you need to build the DNN network. First of all, import the needed libraries. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-shipping",
   "metadata": {
    "id": "durable-shipping"
   },
   "source": [
    "But before, a technical detail. If you are running on a system with GPU, you may want to disable it if you do not need it. This is done via next command, not compulsory. It sets the \"visible GPUs\" to None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-company",
   "metadata": {
    "id": "bronze-company"
   },
   "outputs": [],
   "source": [
    "# forget about next line!\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-natural",
   "metadata": {
    "id": "ranging-natural"
   },
   "outputs": [],
   "source": [
    "# keras imports\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Conv2D, Dropout, Flatten, Concatenate, Reshape, BatchNormalization\n",
    "#from keras.layers import MaxPooling2D, MaxPooling3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-printing",
   "metadata": {
    "id": "smart-printing"
   },
   "source": [
    "You need to define standard parameters like batch_sizes and number of training epochs. For example (but please test more!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-virginia",
   "metadata": {
    "id": "promotional-virginia"
   },
   "outputs": [],
   "source": [
    "#  suggest to use a reasnobale batch size (100, 1000) and a reasonable n epochs (start < 100)\n",
    "#  choose  n_epochs = 100  not too big\n",
    "\n",
    "## EDIT batch_size =  ...\n",
    "## EDIT n_epochs =  ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-camera",
   "metadata": {
    "id": "better-camera"
   },
   "source": [
    "On top of this, it is usually handy to crosschech which are the expected shapes for input and output. Do they make sense to you? Can you explain them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-hierarchy",
   "metadata": {
    "id": "likely-hierarchy"
   },
   "outputs": [],
   "source": [
    "## EDIT  print shape and understand them \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-reflection",
   "metadata": {
    "id": "placed-reflection"
   },
   "source": [
    "Now it is your turn to code! Given the input and output, prepare a dense network with more than 1 (say 4) dense layers with for example 30, 30, 20, 10 layers.\n",
    "Please remember to normalize the Batches after the input layer!\n",
    "\n",
    "Also, why do we use a different activation for the last (output) layer? There is not a too strong motivation, bul looking for example [here](https://en.wikipedia.org/wiki/Softmax_function), SoftMax is commonly used in \"multiclass classification methods\" (you can even try another one!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-highland",
   "metadata": {
    "id": "hairy-highland"
   },
   "outputs": [],
   "source": [
    "inputLayer = Input(shape=(19,))\n",
    "x = BatchNormalization()(inputLayer)\n",
    "\n",
    "#\n",
    "# define a network with O(5) layers, and (30) neurons per layer\n",
    "#\n",
    " ## EDIT\n",
    "\n",
    "#\n",
    "# define an output layer ... which is the # of neurons you need there? (hint: check jetCategories shape, which is the MC truth we are going to use in training ...)\n",
    "#\n",
    "\n",
    "## EDIT\n",
    "\n",
    "####\n",
    "\n",
    "model = Model(inputs=inputLayer, outputs=outputLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QpQ3e1Vywz0h",
   "metadata": {
    "id": "QpQ3e1Vywz0h"
   },
   "source": [
    "At this point you ned to \"compile\" the model with model.compili. You need to specify at least\n",
    "- the **loss** choice. There are [many](https://keras.io/api/losses/probabilistic_losses/), but **categorical_crossentropy** is quite typical when doing \"categorization\". It is simply the \"crossentropy loss between the labels and predictions\"\n",
    "- an **optimizer**, which is used to minimize the loss. Also here, there are [many](https://keras.io/api/optimizers/), but **Adam** is a good first choice: \"Adam optimization is a stochastic gradient descent method ...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-height",
   "metadata": {
    "id": "pursuant-height"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-paint",
   "metadata": {
    "id": "ordinary-paint"
   },
   "source": [
    "What loss can we use? \n",
    "\n",
    "We can  visualize the network, using a standard keras tool model.summary(), which creates a table with layers and parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-journalist",
   "metadata": {
    "id": "civil-journalist"
   },
   "outputs": [],
   "source": [
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-leeds",
   "metadata": {
    "id": "affecting-leeds"
   },
   "source": [
    "You can also try and get a graphical visualization via plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-italian",
   "metadata": {
    "id": "clear-italian"
   },
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-motivation",
   "metadata": {
    "id": "square-motivation"
   },
   "source": [
    "Question / Exercise: see the number of free parameters in the dense networks. Can you explain them? You should (hint: remember the bias neuron and what a batch normalization does ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charming-harassment",
   "metadata": {
    "id": "charming-harassment"
   },
   "source": [
    "Now we need to train the network with the samples + their expected outputs. We should use [model.fit](https://www.tensorflow.org/guide/keras/train_and_evaluate) keras function, passing:\n",
    "- the input feature numpy array \n",
    "- the output feature numpy array (the \"truth\")\n",
    "- the number of epochs and the batchsize\n",
    "- a partitioning between train and validation  (say 70% and 30%)\n",
    "\n",
    "model.fit returns and history object we can use to see how the training process proceeded\n",
    "\n",
    "You need now to fit the model; you need to define a validation/test split. Suggested 70%/30% or similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-clock",
   "metadata": {
    "id": "sustained-clock"
   },
   "outputs": [],
   "source": [
    "# history = model.fit ( .....) fill the \"...\"\n",
    "## EDIT history = model.fit( ..., ..., epochs=..., batch_size=., verbose = 1, validation_split=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-volunteer",
   "metadata": {
    "id": "danish-volunteer"
   },
   "source": [
    "Did it work? was it enough? We need to see how the trainig went, on both training and validation samples. In order to do so, we can look into the \"[history](https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/)\" object. In particular, see **history.history['loss']** and **history.history['val_loss']** (which are the losses on train and validation). Plot them as a function of the epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-assembly",
   "metadata": {
    "id": "seventh-assembly"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-suicide",
   "metadata": {
    "id": "handmade-suicide"
   },
   "source": [
    "Are you satified? Is the validation loss lower or higher than the training one? Is the model really improving after epoch (say) 50?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-limitation",
   "metadata": {
    "id": "imported-limitation"
   },
   "source": [
    "We should try and see if the algorithm (network) we prepared has good performance. The best way is to produce a ROC curve and see if it is better than the one we produced above. It should be, we used more input features!\n",
    "\n",
    "The easiest way is to use the second file, containing other 20000 jets, to get an independet measurement. We reuse the code segment above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-faith",
   "metadata": {
    "id": "designing-faith"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.isfile('test93_20000_40000.npz'):\n",
    " !wget  --no-check-certificate http://www.pi.infn.it/~boccali/test93_20000_40000.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-involvement",
   "metadata": {
    "id": "accepted-involvement"
   },
   "outputs": [],
   "source": [
    "!ls -l test*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-floor",
   "metadata": {
    "id": "deluxe-floor"
   },
   "source": [
    "Build analogous numpy vectors from the second test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-colonial",
   "metadata": {
    "id": "grateful-colonial"
   },
   "outputs": [],
   "source": [
    "test=numpy.load(\"test93_20000_40000.npz\")\n",
    "\n",
    "testALL=test[\"arr_0\"]\n",
    "\n",
    "#the TEST features / MC truth must have the same shape as the ones used in training!\n",
    "\n",
    "## EDIT \n",
    "## jetInfoTEST=testALL[ ... ]\n",
    "## jetCategoriesTEST=testALL[ ... ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-tomorrow",
   "metadata": {
    "id": "varied-tomorrow"
   },
   "source": [
    "Let's check the shapes are correct; do they make sense to you? (remember numpy arrays are [a,b) !!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-trailer",
   "metadata": {
    "id": "under-trailer"
   },
   "outputs": [],
   "source": [
    "## EDIT print share make sure they are the same as those w/o TEST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-scholarship",
   "metadata": {
    "id": "competitive-scholarship"
   },
   "source": [
    "Now we need to use the model.predict keras function, which produces an output of the same shape as the Monte Carlo truth we defined above; a direct comparison with the truth should be a good measure of how well we out ML system works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-arbitration",
   "metadata": {
    "id": "israeli-arbitration"
   },
   "outputs": [],
   "source": [
    "predict_test = model.predict(jetInfoTEST)\n",
    "print (predict_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GzGdcP3wyrAi",
   "metadata": {
    "id": "GzGdcP3wyrAi"
   },
   "source": [
    "Let's print predict_test and jetCategriesTEST. What do you expect? Can you explain it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-september",
   "metadata": {
    "id": "computational-september"
   },
   "outputs": [],
   "source": [
    "print (predict_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-affiliation",
   "metadata": {
    "id": "fixed-affiliation"
   },
   "source": [
    "To be compared with the Monte Carlo truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-kitty",
   "metadata": {
    "id": "critical-kitty"
   },
   "outputs": [],
   "source": [
    "print (jetCategoriesTEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zWodnNMPXL_f",
   "metadata": {
    "id": "zWodnNMPXL_f"
   },
   "source": [
    "You understand why one is binary and the other \"float\"? What do the float mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-disabled",
   "metadata": {
    "id": "maritime-disabled"
   },
   "source": [
    "Let's build ROC curves. We had in the network 6 outpus (isB, isBB+isGBB, isLeptonicB+isLeptonicB_C, isC+isGCC+isCC, isUD+isS, isG), but in the ROC curves we just want B, C, light. We need to consolidate categories.\n",
    "We can map\n",
    "\n",
    "old 0,1,2 --> new 0 (B)\n",
    "\n",
    "old 3 --> new 1 (C)\n",
    "\n",
    "old 4 --> new 2 (light)\n",
    "\n",
    "old 5 --> new 3 (gluon)\n",
    "\n",
    "We do it for the Monte Carlo truth and for the output of the prediction. In practice, we sum the probabilities (assuming the various possibilities at least in MC truth have sum 1!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-second",
   "metadata": {
    "id": "engaged-second"
   },
   "outputs": [],
   "source": [
    "# new roc tool\n",
    "def plot_roc_curve(summed, predict_test_summed):\n",
    "    # can you understand what this selection \">0\" means?\n",
    "    # B vs (C/L/G)\n",
    "    fpr, tpr, threshold = roc_curve(summed[:,0]>0,predict_test_summed[:,0])\n",
    "    auc1 = auc(fpr, tpr)\n",
    "\n",
    "    # B vs L/G roc curve\n",
    "\n",
    "    summed_bl=summed[summed[:,1]==0]\n",
    "    predict_test_bl=predict_test_summed[summed[:,1]==0]\n",
    "\n",
    "    print (len(summed), len(summed_bl))\n",
    "\n",
    "    fpr2, tpr2, threshold = roc_curve(summed_bl[:,0]>0,predict_test_bl[:,0])\n",
    "    auc2 = auc(fpr2, tpr2)\n",
    "\n",
    "    # B vs C roc curve\n",
    "\n",
    "    summed_bc=summed[(summed[:,2]==0)*(summed[:,3]==0)]\n",
    "    predict_test_bc=predict_test_summed[(summed[:,2]==0)*(summed[:,3]==0)]\n",
    "\n",
    "    fpr3, tpr3, threshold = roc_curve(summed_bc[:,0]>0,predict_test_bc[:,0])\n",
    "    auc3 = auc(fpr3, tpr3)\n",
    "\n",
    "# Check AUC\n",
    "\n",
    "    print (\"AUCs\", auc1, auc2, auc3)\n",
    "    return tpr,fpr,tpr2,fpr2,tpr3,fpr3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-fiber",
   "metadata": {
    "id": "selected-fiber"
   },
   "outputs": [],
   "source": [
    "summed=numpy.zeros((len(jetInfoTEST),4))\n",
    "\n",
    "# Reducing it to 4 categories -- but in the plot_roc_curve we collapse gluon and light\n",
    "\n",
    "summed[:,0]+=jetCategoriesTEST[:,0]\n",
    "summed[:,0]+=jetCategoriesTEST[:,1]\n",
    "summed[:,0]+=jetCategoriesTEST[:,2]\n",
    "summed[:,1]+=jetCategoriesTEST[:,3]\n",
    "summed[:,2]+=jetCategoriesTEST[:,4]\n",
    "summed[:,3]+=jetCategoriesTEST[:,5]\n",
    "\n",
    "predict_test_summed=numpy.zeros((len(jetInfoTEST),4))\n",
    "predict_test_summed[:,0]+=predict_test[:,0]\n",
    "predict_test_summed[:,0]+=predict_test[:,1]\n",
    "predict_test_summed[:,0]+=predict_test[:,2]\n",
    "predict_test_summed[:,1]+=predict_test[:,3]\n",
    "predict_test_summed[:,2]+=predict_test[:,4]\n",
    "predict_test_summed[:,3]+=predict_test[:,5]\n",
    "\n",
    "print (\"TEST sample truth     \",summed[0:5,0])\n",
    "print (\"TEST sample prediction\",predict_test_summed[0:5,0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f2c1e4",
   "metadata": {
    "id": "e6f2c1e4"
   },
   "source": [
    "Do you understand these two lines above? what does 0/1 mean in the first line? Do you find a correspondence in the second line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-customer",
   "metadata": {
    "id": "natural-customer"
   },
   "outputs": [],
   "source": [
    "tpr,fpr,tpr2,fpr2,tpr3,fpr3 = plot_roc_curve(summed, predict_test_summed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-imperial",
   "metadata": {
    "id": "photographic-imperial"
   },
   "outputs": [],
   "source": [
    "plt.plot(tpr,fpr,label=\"vs all\")\n",
    "plt.plot(tpr2,fpr2,label=\"vs l\")\n",
    "plt.plot(tpr3,fpr3,label=\"vs c\")\n",
    "plt.semilogy()\n",
    "plt.xlabel(\"sig. efficiency\")\n",
    "plt.ylabel(\"bkg. mistag rate\")\n",
    "plt.ylim(0.0001,1)\n",
    "plt.grid(True)\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-indiana",
   "metadata": {
    "id": "returning-indiana"
   },
   "source": [
    "**Is** it better? Probably not really. Feature #11 we used before was already quite good. Can we do better?\n",
    "you can try\n",
    "- increasing the # of training epochs\n",
    "- increasing the # of neurons in the dense layers\n",
    "\n",
    "etc.\n",
    "\n",
    "But we can also try something new: in the old training history, it seems we were reaching the plateau on the validation loss quite soon, while the training loss was still going down. This is a symptom of having found a \"local minimum on the training sample and not really improving since then\".\n",
    "\n",
    "One standard trick to control this is via dropouts between dense layers. In this way, the network has more difficulties in precisely adapting to the training sample, and generally the performance on validation and orthogonal samples are better.\n",
    "\n",
    "Going back to previous model definition, you can for example modify it including dropouts between the dense layers. [Dropout layers](https://keras.io/api/layers/regularization_layers/dropout/) have only one (important) parameter: the fraction of nodes being killed. 0.25 (25%) is a good starting point.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-henry",
   "metadata": {
    "id": "subtle-henry"
   },
   "outputs": [],
   "source": [
    "dropoutRate = 0.25\n",
    "\n",
    "\n",
    "## EDIT : copy network you defined and add dropout between dense\n",
    "# suggest a dropout rate ~ 20%\n",
    "\n",
    "## ...\n",
    "\n",
    "\n",
    "model = Model(inputs=inputLayer, outputs=outputLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-assembly",
   "metadata": {
    "id": "nutritional-assembly"
   },
   "source": [
    "Re-evaluate the training losses, what do you see now? Did you expect this? if you go from 100 to 300 epochs, the difference should be even clearer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-custom",
   "metadata": {
    "id": "spatial-custom"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "## EDIT \n",
    "#history = model.fit( .., , ... , epochs= ... , batch_size=..., verbose = 1, validation_split= ...)\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.yscale('log')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-quest",
   "metadata": {
    "id": "conventional-quest"
   },
   "source": [
    "Do you see a difference? Is that expected? \n",
    "\n",
    "Do you see that the train and validation curves are in the same order? If not, do you have an idea why?\n",
    "\n",
    "And, is the final performance better (as per the ROC curve)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xHPXNVIMt2sP",
   "metadata": {
    "id": "xHPXNVIMt2sP"
   },
   "outputs": [],
   "source": [
    "predict_test = model.predict(jetInfoTEST)\n",
    "summed=numpy.zeros((len(jetInfoTEST),4))\n",
    "summed[:,0]+=jetCategoriesTEST[:,0]\n",
    "summed[:,0]+=jetCategoriesTEST[:,1]\n",
    "summed[:,0]+=jetCategoriesTEST[:,2]\n",
    "summed[:,1]+=jetCategoriesTEST[:,3]\n",
    "summed[:,2]+=jetCategoriesTEST[:,4]\n",
    "summed[:,3]+=jetCategoriesTEST[:,5]\n",
    "\n",
    "# Reducing it to 3 categories\n",
    "\n",
    "predict_test_summed=numpy.zeros((len(jetInfoTEST),4))\n",
    "predict_test_summed[:,0]+=predict_test[:,0]\n",
    "predict_test_summed[:,0]+=predict_test[:,1]\n",
    "predict_test_summed[:,0]+=predict_test[:,2]\n",
    "predict_test_summed[:,1]+=predict_test[:,3]\n",
    "predict_test_summed[:,2]+=predict_test[:,4]\n",
    "predict_test_summed[:,3]+=predict_test[:,5]\n",
    "\n",
    "tpr,fpr,tpr2,fpr2,tpr3,fpr3 = plot_roc_curve(summed, predict_test_summed)\n",
    "\n",
    "plt.plot(tpr,fpr,label=\"vs all\")\n",
    "plt.plot(tpr2,fpr2,label=\"vs l\")\n",
    "plt.plot(tpr3,fpr3,label=\"vs c\")\n",
    "plt.semilogy()\n",
    "plt.xlabel(\"sig. efficiency\")\n",
    "plt.ylabel(\"bkg. mistag rate\")\n",
    "plt.ylim(0.0001,1)\n",
    "plt.grid(True)\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-kelly",
   "metadata": {
    "id": "complex-kelly"
   },
   "source": [
    "At this point, if you reached here, play a bit with the possible configurations (epochs, number and population of dense layers, ...).\n",
    "\n",
    "A consideration: b quark selection tools are useful in both regimes of  \n",
    "\n",
    "- high B efficiency with moderate light efficiency\n",
    "- lower B efficiency with very small light efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpreted-figure",
   "metadata": {
    "id": "interpreted-figure"
   },
   "source": [
    "Other things you may want to try:\n",
    "\n",
    "- you can increase at will the number of epochs, but at some point the gain will become minimal, so all the iterations from there on are \"wasted time\". You can put in the model.fit options to terminate the training at some point, or to try a finer approach to a minimum, for example try adding some callbacks\n",
    "\n",
    "                callbacks = [\n",
    "                EarlyStopping(monitor='val_loss', patience=10, verbose=1),\n",
    "                ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1)]\n",
    "\n",
    "ReduceLROnPlateau: Reduce learning rate when a metric has stopped improving.\n",
    "    factor: factor by which the learning rate will be reduced. new_lr = lr * factor.\n",
    "    patience: number of epochs with no improvement after which learning rate will be reduced.\n",
    "\n",
    "\n",
    "EarlyStopping: Stop training when a monitored metric has stopped improving.\n",
    "    patience\tNumber of epochs with no improvement after which training will be stopped. This has an important effect: you can increase the # of epochs, but it can have 0 effects if you have an early stop. In that case you need to increase the network complexity / reduce the stopping sensitivity in order to perform more training.\n",
    "    \n",
    "TerminateOnNaN: Stop if you find a NaN\n",
    "    \n",
    "\n",
    "### try them!\n",
    "\n",
    "you will need to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-services",
   "metadata": {
    "id": "emerging-services"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-blogger",
   "metadata": {
    "id": "ahead-blogger"
   },
   "source": [
    "By the way.... Clearly this is a small network, on a small dataset. If we want to go higher, we'd better be faster, which means use a GPU.\n",
    "If we used a VM with GPU and instantiated a container with GPU, we are probably already doing this. Let's check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-lyric",
   "metadata": {
    "id": "electoral-lyric"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-filter",
   "metadata": {
    "id": "comfortable-filter"
   },
   "source": [
    "# If you are really here ... a possible second part. Note that you are NOT expected to do all during the hackathon time; you can also have a look later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-paint",
   "metadata": {
    "id": "organic-paint"
   },
   "source": [
    "In the previous example, only \"jet level features\" were used (Pt, #of tracks, ...).\n",
    "If we have them avaiable, we could use also the characteristic of each track. But this makes a huge input (210 added features per jet), so potentially more than 4x what is already there. The dense layers will then see an exploding number of weights (it can be ok, depending on the ghardware you have)\n",
    "\n",
    "One possible solution is via Long Short Term Memories: recurrent neurons which \"remember last state\" and hence can accept asequence of inputs belonging to the same test case. They are typical in speech recognition, where a sentence is composed of a not standard number of words; as in sentences, there is a \"stop signal\" saying that the inputs are over.\n",
    "\n",
    "What we are going to try here is to complement the DNN we prepared with tracking information each track belonging to a jet.\n",
    "In order to do so, we look into an additional numpy array in the input: arr_1.\n",
    "it has a shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-pursuit",
   "metadata": {
    "id": "fitted-pursuit"
   },
   "outputs": [],
   "source": [
    "f[\"arr_1\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-figure",
   "metadata": {
    "id": "outside-figure"
   },
   "source": [
    "So it contains 20000 jets, for each of them a 2d entry. They should be read as 21 parameters per track for a max of 10 tracks per jet. Let's look into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-breakdown",
   "metadata": {
    "id": "bearing-breakdown"
   },
   "outputs": [],
   "source": [
    "f[\"arr_1\"][0,:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-fetish",
   "metadata": {
    "id": "finished-fetish"
   },
   "source": [
    "This was plottong the first (0) of the 21 features for each track of the first jet. The zeros means there were not 10 tracks in the jet, but just 4.\n",
    "\n",
    "The 21 features are \n",
    "# kinematics\n",
    "\n",
    "0. seed_pt : the track pt\n",
    "1. seed_eta : the track eta\n",
    "2. seed_phi : the track phi\n",
    "3. seed_mass : the track mass (usually a pion mass, but if some particle id happened, could be a lepton for example)\n",
    "\n",
    "# impact parameters\n",
    "\n",
    "4. seed_dz : distance of closest apprach to the beamline in z\n",
    "5. seed_dxy : 2-d distance of closest apprach to the beamline in (x,y) - the transverse plane\n",
    "6. seed_3D_ip : the track Impact Parameter (see forexample slide 4 in https://indico.cern.ch/event/322015/contributions/746885/attachments/622358/856332/B-tagging2.pdf )\n",
    "7. seed_3D_sip : the significance (value/error) of the IP\n",
    "8. seed_2D_ip : as above but only in (x,y)\n",
    "9. seed_2D_sip : as above but only in (x,y)\n",
    "10. seed_3D_signedIp\n",
    "11. seed_3D_signedSip\n",
    "12. seed_2D_signedIp\n",
    "13. seed_2D_signedSip\n",
    "\n",
    "# track probability (from IP) \n",
    "\n",
    "14. seed_3D_TrackProbability : the probability of this track to come from the primary vertex, in 3D\n",
    "15. seed_2D_TrackProbability : the probability of this track to come from the primary vertex, in 2D\n",
    "\n",
    "# quality \n",
    "\n",
    "16. seed_chi2reduced : the chi2 of the track fit\n",
    "17. seed_nPixelHits : the number of hits in the CMS pixel system\n",
    "18. seed_nHits : total number of hits in the track\n",
    "\n",
    "# jet-track distance\n",
    "\n",
    "19. seed_jetAxisDistance : the minimum distance (in 3D) between the track and the jet axis\n",
    "20. seed_jetAxisDlength : the decay length (distance between the primary vertex and the point of closest apprach between the track and the jet axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-direction",
   "metadata": {
    "id": "manufactured-direction"
   },
   "source": [
    "So they contain quantities which are well correlated with b tagging, like IP (Impact parameter) size, error and probability. This should help us in getting a better algorithm!\n",
    "\n",
    "We need to define the inputs. They are now the standard 19 features as before (or any number you decided to use) + the (10,21) bidimensional input, that we want to treat via an LSTM input. The input to the DNN will be the 19 features + the output of LSTM, for which we can define the dimensionality (64 here)\n",
    "\n",
    "Let's start as before by defining parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-vault",
   "metadata": {
    "id": "answering-vault"
   },
   "outputs": [],
   "source": [
    "dataTRACKS=f[\"arr_1\"]\n",
    "# redefine as you please\n",
    "\n",
    "batch_size = 512\n",
    "n_epochs = 100\n",
    "dropoutRate = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-movie",
   "metadata": {
    "id": "ancient-movie"
   },
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "\n",
    "## EDIT : the 19 is in case you were defining the previous network with 19 inputs ... define accordingly\n",
    "input1=[Input(shape=(19,))]\n",
    "input2=[Input(shape=(10,21))]\n",
    "\n",
    "\n",
    "Inputs = input1\n",
    "Inputs+=input2\n",
    "\n",
    "print (input2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kr0wceprdCFZ",
   "metadata": {
    "id": "kr0wceprdCFZ"
   },
   "source": [
    "Do you understand the shape of Inputs at this point? try to print the shapes and understand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-atlantic",
   "metadata": {
    "id": "fancy-atlantic"
   },
   "source": [
    "the second input, tracks, is seen by the rest of the network as a simple additional 64 inputs to the 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-computer",
   "metadata": {
    "id": "hydraulic-computer"
   },
   "outputs": [],
   "source": [
    "print (input2[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-pottery",
   "metadata": {
    "id": "weighted-pottery"
   },
   "outputs": [],
   "source": [
    "## EDIT define an LSTM with 64 outputs .... which is the input??\n",
    "# #tracks  = LSTM( ... )( ... )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-carter",
   "metadata": {
    "id": "fiscal-carter"
   },
   "source": [
    "The last flatten prepares a numpy array like structure, usable for input to the DNN. Which is the size you expect it to have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-columbia",
   "metadata": {
    "id": "assumed-columbia"
   },
   "outputs": [],
   "source": [
    "## EDIT what is the shape of \"tracks\"? Does it match your expectations?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-utilization",
   "metadata": {
    "id": "authorized-utilization"
   },
   "source": [
    "We can append these 64 features to the previous. If they were N, which is the expected shape of the input to the dense network? \n",
    "\n",
    "\n",
    "The idea is to have the 64 + the N as flat inputs, so we need to append (\"Concatenate\") them \n",
    "\n",
    "Does it make sense to you? Before building it, can you imagine how the network will look like (as per the plot_model above, for example)?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-priority",
   "metadata": {
    "id": "ignored-priority"
   },
   "outputs": [],
   "source": [
    "## EDIT what do you concatenate?\n",
    "# x = Concatenate()( [  ... , ...] )\n",
    "\n",
    "# check the shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-reader",
   "metadata": {
    "id": "crucial-reader"
   },
   "source": [
    "From this point on, we define a \"normal\" DNN (this time we can do directly with dropouts, as said before) ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99tWSFL1drTQ",
   "metadata": {
    "id": "99tWSFL1drTQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-scratch",
   "metadata": {
    "id": "million-scratch"
   },
   "outputs": [],
   "source": [
    "#once you have a flat 83 input layer, you can build the usual series of dense layers\n",
    "\n",
    "## EDIT prepare a DL network with some (O(5)) dense layers with O(100) neurons each\n",
    "# remember to put dropout layers!\n",
    "\n",
    "# ...\n",
    "\n",
    "\n",
    "## EDIT then you need an output, the usual 6 categories\n",
    "\n",
    "predictions = Dense(6, activation='softmax')(x)\n",
    "model = Model(inputs=Inputs, outputs=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-current",
   "metadata": {
    "id": "challenging-current"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()\n",
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-arnold",
   "metadata": {
    "id": "compressed-arnold"
   },
   "source": [
    "Again, can you make sense to the numbers you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-veteran",
   "metadata": {
    "id": "assisted-veteran",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## EDIT in order to fit you need to know the input, putputs as usual ... now for input you can use the same syntax as in Concatenate\n",
    "\n",
    "#history = model.fit([ ... , ... ], ... , epochs=... , batch_size=... , verbose = 1,\n",
    "                # validation_split=0.3,\n",
    "                #callbacks = [\n",
    "                #EarlyStopping(monitor='val_loss', patience=30, verbose=1),\n",
    "                #ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1),\n",
    "                #TerminateOnNaN()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-wiring",
   "metadata": {
    "id": "adapted-wiring"
   },
   "source": [
    "Did it work? Let's see the performance. First of all the usual loss wrt epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-chart",
   "metadata": {
    "id": "diverse-chart"
   },
   "outputs": [],
   "source": [
    "# plot training history\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.yscale('linear')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-command",
   "metadata": {
    "id": "persistent-command"
   },
   "source": [
    "Now we repeat the test with the ROC using the second file, all exctly as before. remember all the shapes need to match!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-messaging",
   "metadata": {
    "id": "surrounded-messaging"
   },
   "outputs": [],
   "source": [
    "testALL=test[\"arr_0\"]\n",
    "\n",
    "jetInfoTEST=testALL[:,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18]]\n",
    "print (jetInfoTEST.shape)\n",
    "jetCategoriesTEST=testALL[:,-9:-3]\n",
    "dataTrackTEST=test[\"arr_1\"]\n",
    "\n",
    "predict_test = model.predict([jetInfoTEST,dataTrackTEST] )\n",
    "print(predict_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-montreal",
   "metadata": {
    "id": "colored-montreal"
   },
   "outputs": [],
   "source": [
    "predict_test = model.predict([jetInfoTEST,dataTrackTEST] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-mailman",
   "metadata": {
    "id": "subtle-mailman"
   },
   "outputs": [],
   "source": [
    "print(predict_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-pencil",
   "metadata": {
    "id": "stretch-pencil"
   },
   "source": [
    "The next line should not be needed, but I saw that in some cases the predict_test contained a NaN (not a Number, a floating point error). The next line just wipes the NaN entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-peoples",
   "metadata": {
    "id": "alpine-peoples"
   },
   "outputs": [],
   "source": [
    "predict_test[numpy.where(numpy.isnan(predict_test))]=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-verification",
   "metadata": {
    "id": "alternate-verification"
   },
   "outputs": [],
   "source": [
    "# Reducing it to 4 categories\n",
    "\n",
    "summed=numpy.zeros((len(jetInfoTEST),4))\n",
    "summed[:,0]+=jetCategoriesTEST[:,0]\n",
    "summed[:,0]+=jetCategoriesTEST[:,1]\n",
    "summed[:,0]+=jetCategoriesTEST[:,2]\n",
    "summed[:,1]+=jetCategoriesTEST[:,3]\n",
    "summed[:,2]+=jetCategoriesTEST[:,4]\n",
    "summed[:,3]+=jetCategoriesTEST[:,5]\n",
    "\n",
    "# Reducing it to 4 categories\n",
    "\n",
    "predict_test_summed=numpy.zeros((len(jetInfoTEST),4))\n",
    "predict_test_summed[:,0]+=predict_test[:,0]\n",
    "predict_test_summed[:,0]+=predict_test[:,1]\n",
    "predict_test_summed[:,0]+=predict_test[:,2]\n",
    "predict_test_summed[:,1]+=predict_test[:,3]\n",
    "predict_test_summed[:,2]+=predict_test[:,4]\n",
    "predict_test_summed[:,3]+=predict_test[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-specific",
   "metadata": {
    "id": "traditional-specific"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-tomorrow",
   "metadata": {
    "id": "looking-tomorrow"
   },
   "outputs": [],
   "source": [
    "tpr,fpr,tpr2,fpr2,tpr3,fpr3 = plot_roc_curve(summed, predict_test_summed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-roommate",
   "metadata": {
    "id": "undefined-roommate"
   },
   "outputs": [],
   "source": [
    "# SEMILOG\n",
    "\n",
    "plt.plot(tpr,fpr,label=\"vs all\")\n",
    "plt.plot(tpr2,fpr2,label=\"vs l\")\n",
    "plt.plot(tpr3,fpr3,label=\"vs c\")\n",
    "plt.semilogy()\n",
    "plt.xlabel(\"sig. efficiency\")\n",
    "plt.ylabel(\"bkg. mistag rate\")\n",
    "plt.ylim(0.0001,1)\n",
    "plt.grid(True)\n",
    "plt.legend(loc='lower right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-stretch",
   "metadata": {
    "id": "recreational-stretch"
   },
   "source": [
    "Good work! if you reached here, you have implemented a working (altough with limited performance, due to the needs to be interactive) LSTM algorithm.\n",
    "As usual, you can try tweaking the model, layers, epochs etc to see if you can optimize it in different regions of the B efficiency spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-carry",
   "metadata": {
    "id": "committed-carry"
   },
   "source": [
    "# You are here??? Very very good .... Let's try something different then. Substituting the LSTM with a Convolutional Network 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-hundred",
   "metadata": {
    "id": "sacred-hundred"
   },
   "source": [
    "We still have the same problem: adding 10x21 additional features to out 19 standard ones, without blowing up the network size.\n",
    "In the previous use case, we used LSTM to reduce the tacking part from 210 to 64 features; here we want to use a different way: a 1d convolutional network to preprocess the 210 features and form a smaller set to be fed to the DNN.\n",
    "\n",
    "Convolutional networks read multidimensional matrices (10x21 in this case) and interpret them as \"images\". On them , standard convolutional methods are processed; being 1d, the convolutional \"window\" goes only in one direction, thus again matching the natural flow, for example, of time series or speech.\n",
    "\n",
    "We will apply a series of Conv1D filters, each one lowering the output dimesion of the previous one. In the end, we want to have a dimensionality of 4 (for example), which means that from 10 tracks we get 40 features instead of 210.\n",
    "\n",
    "We mostly reuse the numpy arrays from LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-frame",
   "metadata": {
    "id": "julian-frame"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D\n",
    "\n",
    "batch_size = 512\n",
    "n_epochs = 100\n",
    "dropoutRate = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-vacuum",
   "metadata": {
    "id": "relative-vacuum"
   },
   "source": [
    "We define 4 Convolutional1D layers, with output dimension 64, 32, 8, 4, hence reducing the size of the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-front",
   "metadata": {
    "id": "fatal-front"
   },
   "outputs": [],
   "source": [
    "tracks=Inputs[1]\n",
    "\n",
    "## EDIT define O(4) Conv1D layers, with decreasing # of neurons (for example 64, 32, 8, 4) .. the last one defines the dimensionality of the inputs to add to the standard dense network\n",
    "\n",
    " ## ...        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-alignment",
   "metadata": {
    "id": "reserved-alignment"
   },
   "source": [
    "can you guess what tracks is at the end? (which shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-california",
   "metadata": {
    "id": "unable-california"
   },
   "outputs": [],
   "source": [
    "## EDIT shape?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-discount",
   "metadata": {
    "id": "prescribed-discount"
   },
   "source": [
    "But, in order to feed a Dense like the one we used before, we need a 1D structure. \"Flatten\" can be use.\n",
    "\n",
    "If we want to use this as input to a DNN, we do not want a 10x4 matrix, but a single dimensional numpy array of size 40. We need to Flatten!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-interim",
   "metadata": {
    "id": "sixth-interim"
   },
   "outputs": [],
   "source": [
    "tracks = Flatten()(tracks)\n",
    "## EDIT what is the shape of tracks now??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-trick",
   "metadata": {
    "id": "baking-trick"
   },
   "source": [
    "Now we can add the usual DNN part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-auction",
   "metadata": {
    "id": "molecular-auction"
   },
   "outputs": [],
   "source": [
    "x = Concatenate()( [Inputs[0] , tracks] )\n",
    "\n",
    "\n",
    "### EDIT add the standard series of Dense + dropout layers\n",
    "\n",
    "# ...\n",
    "\n",
    "\n",
    "# ... and the standard output\n",
    "\n",
    "predictions = Dense(6, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=Inputs, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-silence",
   "metadata": {
    "id": "physical-silence"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()\n",
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-sudan",
   "metadata": {
    "id": "stock-sudan"
   },
   "source": [
    "Can you explain / predict the number of free parameters per layer?\n",
    "\n",
    "Let's train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-washington",
   "metadata": {
    "id": "single-washington"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## this is the same as before ...\n",
    "\n",
    "history = model.fit([jetInfo, dataTRACKS], jetCategories, epochs=n_epochs, batch_size=batch_size, verbose = 2,\n",
    "                validation_split=0.3,\n",
    "                callbacks = [\n",
    "                EarlyStopping(monitor='val_loss', patience=10, verbose=1),\n",
    "                ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1),\n",
    "                TerminateOnNaN()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-belief",
   "metadata": {
    "id": "driven-belief"
   },
   "source": [
    "Did it work? Let's see the losses as function of the epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-color",
   "metadata": {
    "id": "sustainable-color"
   },
   "outputs": [],
   "source": [
    "# plot training history\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.yscale('linear')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-evanescence",
   "metadata": {
    "id": "liked-evanescence"
   },
   "source": [
    "Try the usual ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-methodology",
   "metadata": {
    "id": "romantic-methodology"
   },
   "outputs": [],
   "source": [
    "predict_test = model.predict([jetInfoTEST,dataTrackTEST] )\n",
    "print(predict_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-donor",
   "metadata": {
    "id": "automotive-donor"
   },
   "source": [
    "usual problem with the NaNs (no comment...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-supplier",
   "metadata": {
    "id": "eligible-supplier"
   },
   "outputs": [],
   "source": [
    "predict_test[numpy.where(numpy.isnan(predict_test))]=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-semiconductor",
   "metadata": {
    "id": "outstanding-semiconductor"
   },
   "outputs": [],
   "source": [
    "# Reducing it to 4 categories\n",
    "\n",
    "summed=numpy.zeros((len(jetInfoTEST),4))\n",
    "summed[:,0]+=jetCategoriesTEST[:,0]\n",
    "summed[:,0]+=jetCategoriesTEST[:,1]\n",
    "summed[:,0]+=jetCategoriesTEST[:,2]\n",
    "summed[:,1]+=jetCategoriesTEST[:,3]\n",
    "summed[:,2]+=jetCategoriesTEST[:,4]\n",
    "summed[:,3]+=jetCategoriesTEST[:,5]\n",
    "\n",
    "# Reducing it to 4 categories\n",
    "\n",
    "predict_test_summed=numpy.zeros((len(jetInfoTEST),4))\n",
    "predict_test_summed[:,0]+=predict_test[:,0]\n",
    "predict_test_summed[:,0]+=predict_test[:,1]\n",
    "predict_test_summed[:,0]+=predict_test[:,2]\n",
    "predict_test_summed[:,1]+=predict_test[:,3]\n",
    "predict_test_summed[:,2]+=predict_test[:,4]\n",
    "predict_test_summed[:,3]+=predict_test[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-canon",
   "metadata": {
    "id": "manual-canon"
   },
   "outputs": [],
   "source": [
    "tpr,fpr,tpr2,fpr2,tpr3,fpr3 = plot_roc_curve(summed, predict_test_summed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-tobacco",
   "metadata": {
    "id": "animated-tobacco"
   },
   "outputs": [],
   "source": [
    "# SEMILOG\n",
    "\n",
    "plt.plot(tpr,fpr,label=\"vs all\")\n",
    "plt.plot(tpr2,fpr2,label=\"vs l\")\n",
    "plt.plot(tpr3,fpr3,label=\"vs c\")\n",
    "plt.semilogy()\n",
    "plt.xlabel(\"sig. efficiency\")\n",
    "plt.ylabel(\"bkg. mistag rate\")\n",
    "plt.ylim(0.0001,1)\n",
    "plt.grid(True)\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-constraint",
   "metadata": {
    "id": "unique-constraint"
   },
   "source": [
    "# Play with all the parameters, etc, and ...\n",
    "\n",
    "# Which is the absolute best result you can get?\n",
    "\n",
    "#  ....That's all folks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-brunswick",
   "metadata": {
    "id": "victorian-brunswick"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "btaggingTom_May26_nocelloutput_students.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
