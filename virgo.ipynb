{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "964a383e",
   "metadata": {},
   "source": [
    "# Virgo Exercise\n",
    "\n",
    "The aim of the exercise is to prepare a ML tool which is able to compress and clean as much as possible a gwf data \n",
    "\n",
    "In this exercise you will:\n",
    "\n",
    "- explore the content of an open dataset from IGWN containing the strain in function of time\n",
    "- try to plot quantities and understand what can be used\n",
    "- train various (?) network type in order to perform the job, and test their abilities; this can be done at various complexity levels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0db6a2",
   "metadata": {},
   "source": [
    "For an introduction on what GW and interferometers are please refer to https://confluence.infn.it/display/MLINFN/7.+Virgo+Autoencoder+tutorial\n",
    "\n",
    "For a brief vocabulary see https://labcit.ligo.caltech.edu/~ll_news/0607a_news/LIGO_Vocabulary.htm\n",
    "\n",
    "**IGWN** stands for International Gravitational-Wave Observatory Network, it is a comunity made by three collaborations: LIGO VIRGO and KAGRA \n",
    "\n",
    "**Strain** is the instrument’s ability to detect a space change within an arm in comparison to the total space (length) of the arm\n",
    "\n",
    "**GWF** stands for Gravitational Wave File and is the format IGWN used to store (not only!) interferometer strain\n",
    "\n",
    "All data we will use in this tutorial is freely accessibile and published in **GWOSC** (https://www.gw-openscience.org/about/). GWOSC, The Gravitational Wave Open Science Center, provides data from gravitational-wave observatories, along with tutorials and software tools. You could find there many information if you are interested"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172c45c5",
   "metadata": {},
   "source": [
    "To begin we will import all functions needed!\n",
    "\n",
    "To help in reading IGWN files we will used a python package called **gwdama**\n",
    "\n",
    "*gwdama* (https://pypi.org/project/gwdama/) aims at providing a unified and easy to use interface to access Gravitational Wave (GW) data and output some well organised datasets, ready to be used for Machine Learning projects or Data Analysis purposes (source properties, noise studies, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-boards",
   "metadata": {},
   "outputs": [],
   "source": [
    "#maybe needed, do it only if you have an error like \"No module named 'gwdama'\"\" below\n",
    "!pip install gwdama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e36b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gwdama.io import GwDataManager\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib\n",
    "#matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac3fe43",
   "metadata": {},
   "source": [
    "Accessible data is ordered by gps time and run type (Observing Calibration etc..). Observing run, generally called O with a number to distinguish between many of them (for example **O3**), are our scientific and acquisition runs\n",
    "\n",
    "**gps time** is a timestamp relative to an arbitrarly event, internally we use tools to convert to usual time\n",
    "\n",
    "At https://www.gw-openscience.org/events/ you could find a list of events with their gps time and a brief description of the results\n",
    "\n",
    "Let's get data between two gps 1186746568 and 1186746628 from GWOSC and call it \"**online** dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776270aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_gps = 1186746618  \n",
    "dama = GwDataManager()\n",
    "dama.read_gwdata(event_gps - 50, event_gps +10, ifo='L1',data_source=\"gwosc-online\",dts_key='online')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7ccecf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(dama)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97655e93",
   "metadata": {},
   "source": [
    "data acquisition is 21-04-21_10h50m25s.\n",
    "To acquire some confidence with data we will start with a plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b90ccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "online=dama['online']\n",
    "\n",
    "# time vs strain\n",
    "# to make plot more nice add some properties, they will be addedd to the figure\n",
    "online.attrs['t0'] = 1186746598\n",
    "online.attrs['sample_rate'] = 4096   # Unit Hz\n",
    "online.attrs['unit'] = \"\"\n",
    "online.attrs['channel'] = \"strain vs t\"\n",
    "\n",
    "online.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360a5992",
   "metadata": {},
   "source": [
    "**Note**: the timestamp is different because internally in the IGWN we use a different convention\n",
    "\n",
    "**Exercise**: read multiple files and plot them with a single for loop\n",
    "\n",
    "As a pretest to show some functions in gwdama, we will make an histogram of data and fit with a gaussian\n",
    "\n",
    "**norm** is a class of scipy to work with a normal distribution (https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html). In this case we will use the fit method to estimate meand and standard deviation of the input signal. If a **gw** is not present than we can assume for the scope of this exercise that the signal is normal distribute (which is not in real life!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadce436",
   "metadata": {},
   "outputs": [],
   "source": [
    "myplot = online.hist()\n",
    "mu, std = norm.fit(online.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eda9d9f",
   "metadata": {},
   "source": [
    "Customize the plot, adding the fitted gaussian and a legend\n",
    "\n",
    "**gca** method gets the current Axes, creating one if necessary\n",
    "\n",
    "**get_xlim** return min and max value on the x axis\n",
    "\n",
    "**linspace** return evenly spaced number in an interval\n",
    "\n",
    "**pdf** return a probability density function (see norm before)\n",
    "\n",
    "**plot** plot x,y data. 'r--' is used to control the line style (https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html) \n",
    "\n",
    "**legend** add a legend to the plot\n",
    "\n",
    "**show** is used to show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243a7db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = myplot.gca()\n",
    "xmin, xmax = ax.get_xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, mu, std)\n",
    "ax.plot(x, p, 'r--', linewidth=1, label='Gaussian fit')\n",
    "ax.legend()\n",
    "\n",
    "myplot.reshow()                    # New method to re-show closed figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234cead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "myplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4282965",
   "metadata": {},
   "source": [
    "Create a **PSD** with sample rate 4096 Hz\n",
    "\n",
    "**Note**: Power Spectral Density (PSD) is the measure of signal's power content versus frequency. A PSD is typically used to characterize broadband random signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8281d561",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 4096\n",
    "online.attrs['sample_rate'] = fs\n",
    "online.psd(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47413a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dama)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774b73ff",
   "metadata": {},
   "source": [
    "The psd is added to our dataset with name_psd\n",
    "\n",
    "Now plot the PSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11545cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "online_psd=dama['online_psd']\n",
    "fig, ax = plt.subplots()\n",
    "ax.semilogy('freq', 'PSD', data=online_psd, alpha=.7, label='online_psd')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel('Frequency [Hz]')\n",
    "ax.set_ylabel('PSD [1/Hz]')\n",
    "plt.show()\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622217d1",
   "metadata": {},
   "source": [
    "We generally work with signal normalize to its spectral density\n",
    "\n",
    "A **whitned** signal is a normalized signal to its spectral density. To compute it you should get in the freq domain, divide it by the square roots of the power spectral density and return to time domain\n",
    "\n",
    "For convenience gwdama provides a simple function that whiten the signal: output=whiten(input, input_psd, freq)\n",
    "or dama['name'].whiten() and the output will be dama['name_whiten']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1ff0a0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gwdama.preprocessing as pre\n",
    "dama['online'].whiten()\n",
    "#online_cleaned = pre.whiten(dama['online'].data, dama['online_psd'].data, 1./500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-memphis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot it\n",
    "dama['online_whiten'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f6b109",
   "metadata": {},
   "source": [
    "This data is sampled at 4096 Hz, but our interferometers acts like a pass band signal from a frequency ~ between 20-400Hz so it could be usefull to resample the data in order to work with lighter files and remove some noise\n",
    "\n",
    "**decimate_recursive** resamples an input signal keeping only 1 elem every n\n",
    "\n",
    "**resample** resamples an inpunt signal to a dest freq. We will use in this case 512 so we are keeping only 1 every 8 values being our original signal sampled at 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-leonard",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3b17cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dama['online_whiten'].resample(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-wrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dama)\n",
    "# You can delete an entry with\n",
    "#del dama[\"online_whiten_r10\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot it\n",
    "dama['online_whiten_r512'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-equipment",
   "metadata": {},
   "source": [
    "Just for fun try to resample at 128 256 1024 too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-disabled",
   "metadata": {},
   "outputs": [],
   "source": [
    "dama['online_whiten'].resample(128)\n",
    "dama['online_whiten'].resample(256)\n",
    "dama['online_whiten'].resample(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a667ce",
   "metadata": {},
   "source": [
    "if you want to get more information on data and the cleaning process here a usefull link https://github.com/losc-tutorial/Data_Guide/blob/master/Guide_Notebook.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a7f051",
   "metadata": {},
   "source": [
    "Once data is ready we can try to encode with an **autoencoder**\n",
    "\n",
    "[from wiki]: An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise”. Along with the reduction side, a reconstructing side is learned, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name\n",
    "\n",
    "some examples of autoencoders in keras at https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "\n",
    "The idea is to downsampling and resampling to original dimension searching for an optimum parameters that makes filtered signal more similar to original one. The downsampled signal will retain all needed information but with a reduction of size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a15c6cf",
   "metadata": {},
   "source": [
    "To make our statement more concise we will summary some ideas on Artificial Neural Networks and Deep Learning\n",
    "\n",
    "## Artificial Neural networks \n",
    "\n",
    "**Artificial neural networks (ANNs)** are computing systems vaguely inspired by the biological neural networks that constitute animal brains.\n",
    "\n",
    "An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron that receives a signal then processes it and can signal neurons connected to it. The \"signal\" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple time and along different paths\n",
    "\n",
    "Artificial networks could be build by many layers, each one connected to each others. There are amny types of networks. For example:\n",
    "\n",
    "A **feedforward neural network** is an artificial neural network wherein connections between nodes do not form a cycle, layers are connected starting from the top (input) to the bottom (output) and the activation flows from top to the end exclusively\n",
    "\n",
    "A **recurrent neural network (RNN)** is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs. Typically part of output of one layer is feed as input to the same layer at a different time\n",
    "\n",
    "[https://en.wikipedia.org/wiki/Artificial_neural_network]\n",
    "\n",
    "## Layers\n",
    "\n",
    "**Layers** are group of nodes that mimic some concepts. Each node inside the layer shares same input and output and the same activation function\n",
    "\n",
    "## Activation function\n",
    "\n",
    "The **activation function** of a node defines the output of that node given an input or set of inputs. Experience shows that only nonlinear activation functions allow networks to compute nontrivial problems using only small number of nodes\n",
    "\n",
    "- **rectifier or ReLU** activation function is an activation function defined as the positive part of its argument\n",
    "\n",
    "- **Sigmoid** activation function is an activation function which applies a sigmoid to the input. The assumption here is that we are interested in intermediate value of input, so we treat them in a liner way, greater values in modulo are not so important because are extreme and show fewer variation\n",
    "\n",
    "[ https://en.wikipedia.org/wiki/Activation_function ]\n",
    "\n",
    "## Loss function\n",
    "Loss is nothing but a prediction error of Neural Network. And the method to calculate the loss is called Loss Function\n",
    "\n",
    "## Optimizer\n",
    "\n",
    "The **optimizer** is used to explorer the parameters space searching which value correspond to a minimum value of the loss function\n",
    "\n",
    "## Learning \n",
    "\n",
    "**Supervised learning** is the machine learning task of learning a function that maps an input to an output based on example input-output pairs\n",
    "\n",
    "**Unsupervised learning** is the task of learning a function that maps an input to an output based on some cost function\n",
    "\n",
    "## Overfitting\n",
    "\n",
    "Overfitting is a modeling error in statistics that occurs when a function is too closely aligned to a limited set of data points. Overfitting the model generally takes the form of making an overly complex model to explain idiosyncrasies in the data under study\n",
    "\n",
    "## Regularization\n",
    "\n",
    "Regularization in machine learning is the process of regularizing the parameters that constrain, regularizes, or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, reducing the risk of Overfitting\n",
    "\n",
    "## Restatement of the task\n",
    "\n",
    "An autoencoder has an input layer, an output layer and one or more hidden layers connecting them. The output layer has the same number of units as the input layer. Its purpose is to reconstruct its own inputs. Therefore, autoencoders are unsupervised learning models. We want our autoencoder to efficient codings using unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3dd595",
   "metadata": {},
   "source": [
    "**Question**: Could an autoencoder have in total only a single layer? Two layers? Three?\n",
    "\n",
    "**Remark**:The loss function is used to optimize your model. This is the function that will get minimized by the optimizer. A metric is used to judge the performance of your model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b28bb6",
   "metadata": {},
   "source": [
    "We will use keras ( https://keras.io/ )\n",
    "\"Keras is an API designed for human beings, not machines. Keras follows best practices for reducing cognitive load: it offers consistent & simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear & actionable error messages. It also has extensive documentation and developer guides.\"\n",
    "\n",
    "Some usefull info:\n",
    "\n",
    "- https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "\n",
    "- https://keras.io/api/models/\n",
    "\n",
    "In the following we will use a slightly different approch from usual, instead of using a single command, we use sklearn as interface to call other functions. The idea is to learn to wrap Keras models for use in scikit-learn and how to use grid search. \"GridSearchCV is a library function that is a member of sklearn's model_selection package. It helps *to loop through predefined hyperparameters* and fit your estimator (model) on your training set. So, in the end, you can select the best parameters from the listed hyperparameters\" \n",
    "\n",
    "Please **Remember**: Keras is a high-level API built on Tensorflow. Scikit Learn is a general machine learning library built on top of NumPy\n",
    "\n",
    "[https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html ] \n",
    "\n",
    "[https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/ ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc3f869",
   "metadata": {},
   "source": [
    "Some layers used in keras:\n",
    "\n",
    "- **Dense** layer makes output=activation(dot(input, kernel) + bias) where kernel is the matrix of parameters and activation is the activation function. Each node is fed by the whole input! \n",
    "\n",
    "- **Dropout** layer randomly sets input units to 0 with a frequency of rate at each step during training time. Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4218374",
   "metadata": {},
   "source": [
    "## Simple example\n",
    "\n",
    "A skeleton will take input, import relevant functions, define a model, optimize it and test it\n",
    "\n",
    "Let's start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d03bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "# we will split our data in chunk of n elements and we will feeds our network with all these chunks\n",
    "n_features = 100 # is the size of a single chunk of data\n",
    "\n",
    "# add a normal dataset to dama\n",
    "dama.create_dataset('random_n', data=np.random.normal(0, 1, (10000,)))\n",
    "\n",
    "# access its data and store in variable input_data\n",
    "input_data = dama['random_n'].data\n",
    "\n",
    "# split data in chunks of 100 element each one and stack each chunk vertically\n",
    "chunks = np.stack(np.split(input_data,100))\n",
    "chunks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeae26e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.optimizers import Nadam\n",
    "\n",
    "from keras import regularizers\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense, Activation, Dropout, TimeDistributed\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685b0996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a very simple model, feedforward neural network with two layers and with two different activation functions\n",
    "def baseline_model(bias1=1e-9, bias2=1e-9, ker1=1e-9, ker2=1e-9):\n",
    "  \n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_features, activation='sigmoid', input_shape=(n_features,), bias_regularizer=regularizers.l1_l2(l1=bias1, l2=bias2), kernel_regularizer=regularizers.l1_l2(l1=ker1, l2=ker2)))\n",
    "    model.add(Dense(n_features, activation='relu', bias_regularizer=regularizers.l1_l2(l1=bias1, l2=bias2), kernel_regularizer=regularizers.l1_l2(l1=ker1, l2=ker2)))\n",
    "\n",
    "    model.compile(optimizer=Nadam(lr=5e-6), loss='mse', metrics=['mse'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70bc4e4",
   "metadata": {},
   "source": [
    "We had defined a function (with four parameters!) that build a very minimalistic model\n",
    "\n",
    "**Sequential** initialize the model\n",
    "\n",
    "**add** method is used to add layers to your model\n",
    "\n",
    "**compile** method id used to configure the model for training https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile\n",
    "\n",
    "some reference to its parameters:\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Metric\n",
    "\n",
    "You could get usefull debugging message with **model.summary()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84d2213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine learning\n",
    "# epochs is used for recurrent networks, we don't need them so we set to 1\n",
    "# batch_size is the number of input chunks\n",
    "mlp = KerasRegressor(build_fn=baseline_model, epochs=1, batch_size=n_features, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a89ed0",
   "metadata": {},
   "source": [
    "**KerasRegressor** is a wrapper to use keras from sklearn. It takes as parameters, your function model, epochs, batch size. See for example https://www.tensorflow.org/api_docs/python/tf/keras/wrappers/scikit_learn/KerasRegressor\n",
    "\n",
    "The **batch size** is a hyperparameter that defines the number of samples to work through before updating the internal model parameters\n",
    "\n",
    "The number of **epochs** is a hyperparameter that defines the number times that the learning algorithm will work through the entire training dataset. See for example https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bac79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# space of parameters used when call our model\n",
    "param_distr = dict(bias1 = [1e-9], bias2 = [1e-9], ker2 = [1e-9], ker1 = [1e-9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f999d7",
   "metadata": {},
   "source": [
    "param_distr is a dictionary of parameters to try, we will explore on all combinations\n",
    "**Query**: How many combinations will have in a general case? In this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e79ae5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build our model all together\n",
    "grid_search = GridSearchCV(estimator=mlp, param_grid=param_distr, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ee4b94",
   "metadata": {},
   "source": [
    "**GridSearchCV** Exhaustive search over specified parameter values for an estimator. So we are trying all combinations! https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n",
    "In particular: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV.fit\n",
    "\n",
    "X: array-like of shape (n_samples, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c530001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit it\n",
    "# vay all nodes based on param_distr to make output \"near\" to input. Near here mean output of a distance function 'mse' \n",
    "grid_search.fit(chunks, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25905f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some info\n",
    "print(\"Best: %f using %s\" % (grid_search.best_score_, grid_search.best_params_))\n",
    "means  = grid_search.cv_results_['mean_test_score']\n",
    "stds   = grid_search.cv_results_['std_test_score']\n",
    "params = grid_search.cv_results_['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94939105",
   "metadata": {},
   "source": [
    "**Question**: Is output data only positive? why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e4f76d",
   "metadata": {},
   "source": [
    "## Exercise 1: Alter input and model to have in output value between a different range\n",
    "\n",
    "Redo all steps but in the model function use a different activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-payment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a very simple model, feedforward neural network with two layers and with two different activation functions\n",
    "def baseline_model1(bias1=1e-9, bias2=1e-9, ker1=1e-9, ker2=1e-9):\n",
    "  \n",
    "    model1 = Sequential()\n",
    "    model1.add(Dense(n_features, activation='relu', input_shape=(n_features,), bias_regularizer=regularizers.l1_l2(l1=bias1, l2=bias2), kernel_regularizer=regularizers.l1_l2(l1=ker1, l2=ker2)))\n",
    "    model1.add(Dense(n_features, activation='sigmoid', bias_regularizer=regularizers.l1_l2(l1=bias1, l2=bias2), kernel_regularizer=regularizers.l1_l2(l1=ker1, l2=ker2)))\n",
    "\n",
    "    model1.compile(optimizer=Nadam(lr=5e-6), loss='mse', metrics=['mse'])\n",
    "\n",
    "    return model1\n",
    "\n",
    "\n",
    "mlp1 = KerasRegressor(build_fn=baseline_model1, epochs=1, batch_size=n_features, verbose=0)\n",
    "param_distr1 = dict(bias1 = [1e-9], bias2 = [1e-9], ker2 = [1e-9], ker1 = [1e-9])\n",
    "grid_search1 = GridSearchCV(estimator=mlp1, param_grid=param_distr1, cv=3)\n",
    "grid_search1.fit(chunks, chunks)\n",
    "print(\"Best: %f using %s\" % (grid_search1.best_score_, grid_search.best_params_))\n",
    "means  = grid_search1.cv_results_['mean_test_score']\n",
    "stds   = grid_search1.cv_results_['std_test_score']\n",
    "params = grid_search1.cv_results_['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3813d1",
   "metadata": {},
   "source": [
    "## Exercise 2: example with real data\n",
    "\n",
    "We need to follow exactly what we did before but using our gw data insetad of our simulated data\n",
    "\n",
    "Recall that to get data from dama you could use \"dama['random_n'].data\" \n",
    "\n",
    "In real life before start we will have to decide what will be\n",
    "- our chunk size\n",
    "- our metric\n",
    "\n",
    "using frequency and other values as starting point\n",
    "\n",
    "We can however start with an (nearly!) arbitrary value and see what will happen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-poland",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dama['online_whiten_r512'])\n",
    "input_data=np.array(dama['online_whiten_r512'].data)\n",
    "input_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-being",
   "metadata": {},
   "source": [
    "get a number of point mutiple of 100, for example we will use first 49 * 100 elems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data[0:4900].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-philippines",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chunks = np.stack(np.split(input_data[0:4900],100))\n",
    "chunks.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-sound",
   "metadata": {},
   "source": [
    "as already said, we have an array-like of shape (n_samples, n_features)\n",
    "so here we have 49 features in 100 samples\n",
    "\n",
    "We can now proceed as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-flash",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features=49\n",
    "\n",
    "# a very simple model, feedforward neural network with two layers and with two different activation functions\n",
    "def baseline_model2(bias1=1e-9, bias2=1e-9, ker1=1e-9, ker2=1e-9):\n",
    "  \n",
    "    model2 = Sequential()\n",
    "    model2.add(Dense(n_features, activation='relu', input_shape=(n_features,), bias_regularizer=regularizers.l1_l2(l1=bias1, l2=bias2), kernel_regularizer=regularizers.l1_l2(l1=ker1, l2=ker2)))\n",
    "    model2.add(Dense(n_features, activation='sigmoid', bias_regularizer=regularizers.l1_l2(l1=bias1, l2=bias2), kernel_regularizer=regularizers.l1_l2(l1=ker1, l2=ker2)))\n",
    "\n",
    "    model2.compile(optimizer=Nadam(lr=5e-6), loss='mse', metrics=['mse'])\n",
    "\n",
    "    return model2\n",
    "\n",
    "\n",
    "mlp2 = KerasRegressor(build_fn=baseline_model2, epochs=100, batch_size=10, verbose=0)\n",
    "param_distr2 = dict(bias1 = [1e-9], bias2 = [1e-9], ker2 = [1e-9], ker1 = [1e-9])\n",
    "grid_search2 = GridSearchCV(estimator=mlp2, param_grid=param_distr2, cv=3)\n",
    "grid_search2.fit(chunks, chunks)\n",
    "print(\"Best: %f using %s\" % (grid_search2.best_score_, grid_search2.best_params_))\n",
    "means  = grid_search2.cv_results_['mean_test_score']\n",
    "stds   = grid_search2.cv_results_['std_test_score']\n",
    "params = grid_search2.cv_results_['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-drilling",
   "metadata": {},
   "source": [
    "**Note**: Could you alter code to have 49 elems(samples) of 100 features each one?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ddfd6b",
   "metadata": {},
   "source": [
    "## Exercise 3: Alter model, adding a dropout layer and a new dense layer\n",
    "\n",
    "That's where we get our compression! we are asking to our model please try to be as similar as possible to the input signal with less variables. The number of variables in this case is the number of neuron in the droput layer\n",
    "\n",
    "Try to build a model that half the variable.  Suggestion pass 0.5 to droput and set size according to dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-plenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features=49\n",
    "dropout=0.1\n",
    "\n",
    "# a very simple model, feedforward neural network with two layers and with two different activation functions\n",
    "def baseline_model3(bias1=1e-9, bias2=1e-9, ker1=1e-9, ker2=1e-9):\n",
    "  \n",
    "    model3 = Sequential()\n",
    "    \n",
    "    model3.add(Dense(n_features, activation='relu', input_shape=(n_features,), bias_regularizer=regularizers.l1_l2(l1=bias1, l2=bias2), kernel_regularizer=regularizers.l1_l2(l1=ker1, l2=ker2)))\n",
    "    model3.add(Dropout(dropout))\n",
    "    model3.add(Dense(int(n_features/2), activation='relu', bias_regularizer=regularizers.l1_l2(l1=bias1, l2=bias2), kernel_regularizer=regularizers.l1_l2(l1=ker1, l2=ker2)))\n",
    "    model3.add(Dropout(dropout))\n",
    "    model3.add(Dense(int(n_features/4), activation='relu', bias_regularizer=regularizers.l1_l2(l1=bias1, l2=bias2), kernel_regularizer=regularizers.l1_l2(l1=ker1, l2=ker2)))\n",
    "    model3.add(Dropout(dropout))\n",
    "\n",
    "    model3.add(Dense(int(n_features/4), activation='sigmoid', bias_regularizer=regularizers.l1_l2(l1=bias1, l2=bias2), kernel_regularizer=regularizers.l1_l2(l1=ker1, l2=ker2)))\n",
    "    model3.add(Dense(int(n_features/2), activation='sigmoid', bias_regularizer=regularizers.l1_l2(l1=bias1, l2=bias2), kernel_regularizer=regularizers.l1_l2(l1=ker1, l2=ker2)))\n",
    "    model3.add(Dense(n_features, activation='sigmoid', bias_regularizer=regularizers.l1_l2(l1=bias1, l2=bias2), kernel_regularizer=regularizers.l1_l2(l1=ker1, l2=ker2)))\n",
    "    \n",
    "    model3.compile(optimizer=Nadam(lr=5e-6), loss='mse', metrics=['mse'])\n",
    "\n",
    "    return model3\n",
    "\n",
    "\n",
    "mlp3 = KerasRegressor(build_fn=baseline_model3, epochs=1, batch_size=100, verbose=0)\n",
    "param_distr3 = dict(bias1 = [1e-9], bias2 = [1e-9], ker2 = [1e-9], ker1 = [1e-9])\n",
    "grid_search3 = GridSearchCV(estimator=mlp3, param_grid=param_distr3, cv=3)\n",
    "grid_search3.fit(chunks, chunks)\n",
    "print(\"Best: %f using %s\" % (grid_search3.best_score_, grid_search3.best_params_))\n",
    "means  = grid_search3.cv_results_['mean_test_score']\n",
    "stds   = grid_search3.cv_results_['std_test_score']\n",
    "params = grid_search3.cv_results_['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-giving",
   "metadata": {},
   "source": [
    "We altered number of neurons, is metric better or worse compare to other exercise? was expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-framing",
   "metadata": {},
   "source": [
    "## Exercise 4: Tune parameters\n",
    "\n",
    "**Add to param_distr both Batch size and Epochs, retrain and take best values**\n",
    "\n",
    "...same model as before...\n",
    "\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "\n",
    "epochs = [10, 50, 100]\n",
    "\n",
    "param_distr = dict(bias1 = [1e-9], bias2 = [1e-9], ker2 = [1e-9], ker1 = [1e-9], batch_size=batch_size, epochs=epochs )\n",
    "\n",
    "grid_search4 = GridSearchCV(estimator=mlp4, param_grid=param_distr4, cv=3)\n",
    "\n",
    "grid_result=grid_search4.fit(chunks, chunks)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "**Same as before but optime optimizer**\n",
    "\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "\n",
    "param_grid = dict(optimizer=optimizer)\n",
    "\n",
    "**With this system you could optimize any hyperparameters...but it works only for local minimum point. No one can guarantee that a minimum for non optimal parameters exists!**\n",
    "\n",
    "https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1a44d2",
   "metadata": {},
   "source": [
    "## Exercise 4 (long): Build a complex model which takes as input signal at different freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0138ac3",
   "metadata": {},
   "source": [
    "## Exercise 5: Plot output data and compare with input signal\n",
    "\n",
    "We already have an autoencoder. From the output of GridSearchCV we got best parameters. Now how we could encode-decode a signal and compare with the original one?\n",
    "\n",
    "GridSearchCV.predict(data) return an encoded-decoded signal, we could simply make a diff or plot original signal with autoencoded one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-trace",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search2.predict(chunks).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-neutral",
   "metadata": {},
   "source": [
    "There are many ways to get the encoded signal, for example re-train the model, using as parameterets the ones found by GridSearchCV. In this case :\n",
    "\n",
    "Best: -0.280207 using **{'bias1': 1e-09, 'bias2': 1e-09, 'ker1': 1e-09, 'ker2': 1e-09}**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuid a model\n",
    "model=baseline_model3()\n",
    "model.summary()\n",
    "# fit it\n",
    "model.fit(chunks,chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-leader",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model of only encoding layers\n",
    "encoder = Sequential()\n",
    "\n",
    "for el in range(1,7):\n",
    "    encoder.add(model.layers[el])\n",
    "\n",
    "encoded = encoder.predict(chunks)\n",
    "print(encoded)\n",
    "encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-facial",
   "metadata": {},
   "source": [
    "**Note**: Compression at work! 12 Features instead of 49\n",
    "\n",
    "You can access weight using\n",
    "\n",
    "**model.get_weights()[0]** to get all values\n",
    "\n",
    "**model.layers[0].get_weights()[0]** to get values of a single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-trash",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example\n",
    "model.get_weights()[0] \n",
    "model.layers[0].get_weights()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-briefing",
   "metadata": {},
   "source": [
    "**We took only 4900 elems ... restart with a larger database, and use remain elems as test, so just try predict them and see how differ**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56e0b3b",
   "metadata": {},
   "source": [
    "## Exercise 6: Save output files and compute their size and their entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1443ad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save our model\n",
    "model.save(\"test.h5\")\n",
    "# to save our encoded file\n",
    "# don't forget to delete our useness item before saving to disk\n",
    "dama.write_gwdama('outputfile')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
