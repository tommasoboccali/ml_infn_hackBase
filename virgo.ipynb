{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "964a383e",
   "metadata": {},
   "source": [
    "<img src=\"virgo_logo.png\" alt=\"Title\" style=\"width:300px;\"/>\n",
    "\n",
    "\n",
    "# Virgo Gravitational Wave Data Exercise\n",
    "\n",
    "The aim of this exercise is to prepare a ML tool which is able to compress and clean as much as possible a dataset.\n",
    "\n",
    "**The autoencoder coded in this exercise is only used as an opportunity to acquire some insight into ML** \n",
    "\n",
    "In this exercise you will:\n",
    "\n",
    "- explore the content of a dataset from the **International Gravitational-Wave Observatory Network** containing the gravitational strain as a function of time\n",
    "- try to plot quantities and understand which features could help in separating signal from noise\n",
    "- train a network in order to perform the job, and test their abilities\n",
    "\n",
    "## **First of all, since this is a workgroup, we disable autosave in order not to mess the notebooks; the output should be 0**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5620a3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.set_autosave_interval(0)\n",
    "element.text(Jupyter.notebook.autosave_interval);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0db6a2",
   "metadata": {},
   "source": [
    "## Gravitational Wave Data\n",
    "\n",
    "For an introduction on what GW and interferometers are please refer to https://confluence.infn.it/display/MLINFN/7.+Virgo+Autoencoder+tutorial.\n",
    "\n",
    "For a brief vocabulary see https://labcit.ligo.caltech.edu/~ll_news/0607a_news/LIGO_Vocabulary.htm.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>IGWN</b> stands for International Gravitational-Wave Observatory Network, it is a comunity made by three collaborations: LIGO VIRGO and KAGRA.<br />\n",
    "    <b>Strain</b> is the effect that produces an incoming gravitational wave on our detectors, which, in the case of interferometric detectors, consists of a differential variation of the arm length. This is also the observable quantity measured by the detectors.<br />\n",
    "    <b>GWF</b> stands for Gravitational Wave File and is the format IGWN used to store (not only!) interferometer strain.\n",
    "</div>\n",
    "\n",
    "All data we will use in this tutorial is freely accessibile and published by **GWOSC** (https://www.gw-openscience.org/about/). GWOSC, The Gravitational Wave Open Science Center, provides data from gravitational-wave observatories, along with tutorials and software tools. You could find there many information if you are interested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172c45c5",
   "metadata": {},
   "source": [
    "## Data Access\n",
    "\n",
    "To ease the reading and decoding of the data file we will rely on a python package named `gwdama` that needs to be imported (and installed because not available by default on the system).\n",
    "\n",
    "[gwdama](https://pypi.org/project/gwdama/) aims at providing a unified and easy to use interface to access Gravitational Wave (GW) data and output some well organised datasets, ready to be used for Machine Learning projects or Data Analysis purposes (source properties, noise studies, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-boards",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment the following line if you have an error like \"No module named 'gwdama'\"\" below\n",
    "#!pip install gwdama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e36b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gwdama.io import GwDataManager\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac3fe43",
   "metadata": {},
   "source": [
    "Datasets made available through the `gwdama` interface are organized by time and detector. Each data entry includes the strain recorded by the detector as a function of time. Datasets can be selected by indicating a time range and the detector. For this exercise we will select events in the gps time range 118674… corresponding to signal events acquired in August 2017.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Observing runs</b>, generally called \"O\" with a number to distinguish between many of them (for example <b>O3</b>), are our scientific and acquisition runs.<br />\n",
    "    <b>gps time</b> is a timestamp relative to an arbitrarly event (6 January 1980), internally we use our tools to convert to usual epoch time. Not to be confuse with the <em>Unix time</em>\n",
    "</div>\n",
    "\n",
    "At https://www.gw-openscience.org/events/ you could find a list of events with their gps time and a brief description of the results.\n",
    "\n",
    "Firstly, let's initialize the main \"container\" class to store and pre-process data. Let's label this instance \"data\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d80df32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = GwDataManager(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40d80bb",
   "metadata": {},
   "source": [
    "At the moment, this is just an empty container, with just some metadata (attributes) as a record of when this was created. The object `dm` returned by `GwDataManager` is a smart container for various datasets. Each dataset is identified by a unique *key*. We can list the keys defined within a container simplying printing it to standard output. Let's import some data, such as that between two gps times: 1186746568 and 1186746628. We can use the `.read_gwdata` method of the `GwDataManager` class to import deta from GWOSC. Alternatively to the gps times, you could have passed a UTC time as a string in the format \"YYYY-MM-DD hh:mm(:ss.)\", such as `\"2017-08-14 12:00\"`.\n",
    "Also, ypu can specify a name (key) for the dataset you are going to import by passing the parameter `dts_key`; this will shortly be useful but for now let's keep things simple.\n",
    "\n",
    "*Behind the scenes, read_gwdata downloads data from the openscience portal, hence executing the next cell may require a few seconds.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776270aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_gps = 1186746618  \n",
    "dm = GwDataManager(\"data\")\n",
    "# Data from Virgo interferometer (ifo), labeled \"V1\". Other options are\n",
    "# L1 (LIGO Livingston) and H1 (LIGO Hanford)\n",
    "print(\"This may take some time... \",end='')\n",
    "dm.read_gwdata(event_gps - 50, event_gps +10, ifo='V1')\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae8435b",
   "metadata": {},
   "source": [
    "Now, if you print the `GwDataManager` instance you will notice that this container has a dataset labelled with the key `strain` (default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7ccecf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97655e93",
   "metadata": {},
   "source": [
    "### Plotting\n",
    "To acquire some confidence with data we can examine it with a plot. `gwdama` has a couple of useful methods for this purpose. Let's start with the most immediate one.\n",
    "\n",
    "But firstly, let's make some preparatory ooperations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b90ccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's associate the dataset to a variable, for convenience\n",
    "dts=dm['strain']\n",
    "\n",
    "# and see its attributes/metadata\n",
    "dts.show_attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2663c6d9",
   "metadata": {},
   "source": [
    "Gravitational wave strain units are dimensionless. We can however strass what this quantity represents changing the unit attribute. Also, let's specify the name of this channel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6add6fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dts.attrs['unit'] = 'strain'\n",
    "dts.attrs['channel'] = 'Virgo strain data'\n",
    "\n",
    "# and check the result\n",
    "dts.show_attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b29949",
   "metadata": {},
   "source": [
    "Now, it is time to plot this time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb1f60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dts.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a34cd5",
   "metadata": {},
   "source": [
    "It seems there is a recurring oscillation pattern in this data. We will explore this aspect in more details in a moment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360a5992",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <b>Exercise:</b> read multiple files and plot them with a single for loop. Hint: notice that you don't need to initialize multiple `GwDataManager` instances. One can contain all of your data. Just remember to label each dataset with a different key by passing the `dts_key` parameter when readiang a new set of data.\n",
    "</div>\n",
    "\n",
    "### Histograms\n",
    "\n",
    "Histograms are another way to get information about the distribution of values in a time series. `gwdama` provides the ethod `.hist`, which plots a histogram of the values in a dataset. Let's create one comparing it with a Gaussian (or Normal) distribution.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>norm</b> is a class of scipy to work with a normal distribution (https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html). In this case we will use the fit method to estimate mean and standard deviation of the input signal. If a <b>gw*</b> is not present then we can assume for the scope of this exercise that the signal is normal distribute (<em>which is not in real life!</em>)\n",
    "   </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadce436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is convenient to specify the number of bins and tell the method to plot\n",
    "# the occupancy of each of them in logarithmic scale\n",
    "myplot = dts.hist(bins=50, log=True)\n",
    "# Fit with a normal distribution\n",
    "mu, std = norm.fit(dts.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eda9d9f",
   "metadata": {},
   "source": [
    "Let's customize the standard plot:\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>gca</b> method gets the current Axes, creating one if necessary<br />\n",
    "    <b>get_xlim</b> return min and max value on the x axis<br />\n",
    "    <b>linspace</b> return evenly spaced number in an interval<br />\n",
    "    <b>pdf</b> return a probability density function (see norm before)<br />\n",
    "    <b>plot</b> plot x,y data. 'r--' is used to control the line style (https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html)<br />\n",
    "    <b>legend</b> add a legend to the plot<br />\n",
    "    <b>show</b> is used to show the plot\n",
    "   </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243a7db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = myplot.gca()                     # Get the axis related to this plot (figure)\n",
    "ax.patches[-1].set_label('Data')      # Set label for the histogram\n",
    "# Plot the fit \n",
    "xmin, xmax = ax.get_xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, mu, std)\n",
    "ax.plot(x, p, 'r--', linewidth=1, label='Gaussian fit')\n",
    "ax.legend()\n",
    "\n",
    "myplot.reshow()                       # New method to re-show closed figures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b749c1ea",
   "metadata": {},
   "source": [
    "It seems that the values of our time series are well modeled by a Gaussian distribution, with possibly some deviations on the tails.\n",
    "\n",
    "<div class=\"alert alert-danger\"><b>Exercise:</b> do you think a different distribution can be more appropriate? Try for example the Student's distribution. The syntax is similar to the one used for the Gaussian/Normal distribution. Just import <code>t</code> from the <code>scipy.stats</code> module.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4282965",
   "metadata": {},
   "source": [
    "### Spectral estimations: the Power Spectral Density\n",
    "\n",
    "When studying random time series, it is interesting to look for recurring patterns and periodicities in the values assumed by the data. This is commonly done in the *frequency domain*, looking at the intensity of the various frequency components (spectrum). The mathematical means to quantify this is the so called [Power Spectral Density (PSD)](https://en.wikipedia.org/wiki/Spectral_density). This is related to the squared modulus of the Fourier Transform of the signal, and measures the signal's power content versus frequency. A PSD is typically used to characterize broadband random signals. \n",
    "\n",
    "To estimate the PSD we must decide a duration for the Fast Fourier Transform and the overlap between consecutive segmets where thay are estimated. This are the two parameters to pass to the `.psd` method of the dataset class of `gwdama`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8281d561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 seconds long fft's, with one second of overlap\n",
    "dts_psd = dts.psd(2,1)\n",
    "\n",
    "print(dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3f66c7",
   "metadata": {},
   "source": [
    "> **Notice** The previous cell creates a new dataset within our mydama manager. If you execute the cell twice you will get an error Unable to create link (name already exists) because an object named online_psd was already defined in the container mydama.\n",
    "\n",
    "You can delete an existing key and the associated dataset with the command\n",
    "\n",
    "    del dama[key]\n",
    "\n",
    "for instance\n",
    "    \n",
    "    del dama['online_psd']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774b73ff",
   "metadata": {},
   "source": [
    "Now that a new dataset containing the values of this PSD has been added to our `GwDataManager` object with the key of the original datasat with the addition of the `_psd\"` string.\n",
    "\n",
    "<div class=\"alert alert-info\">Notice that like all methods to create new datasets you could have specified the name of the psd dataset passing to the p[revious <code>.psd</code> method the optionla argument <code>dts_key</code> with specified the name you wanted for this.<br/>\n",
    "Notice also that this new dataset is not a time series. In fact, it is saved as a <em>structured array</em>, as we will see momentarily.\n",
    "</div>\n",
    "\n",
    "Now let's plot the PSD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11545cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "# It is convenient to use \"log-log plot\"\n",
    "ax.loglog('freq', 'PSD', data=dts_psd, alpha=.7)\n",
    "\n",
    "ax.set(xlabel='Frequency [Hz]', ylabel='PSD [1/Hz]', xlim=(10,1500), ylim=(1e-47,1e-37), title=dts.attrs['channel'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6debc0ef",
   "metadata": {},
   "source": [
    "As you can see, the intensity of the various frequency components span several orders of magnitude. The vertical structures are called *sopectral lines*, and are usually associated to resonances of the apparatus, such as those from electrical origin (50 Hz mains and harmonics) or mechanical (suspensions).\n",
    "\n",
    "<div class=\"alert alert-danger\"><b>Exercise:</b> try to modify the parameters passed to the method to estimate the PSD and check how different the result looks. If you want to know more, read about the <a href=\"https://en.wikipedia.org/wiki/Welch%27s_method\">Welch's method</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622217d1",
   "metadata": {},
   "source": [
    "### Whitening\n",
    "\n",
    "It is often convenient to work with signals normalized such that to have all of their frequency components with the same intensity. In the *time domain*, this means that the signal is uncorrelated. This operation is done by a so-called *whitening* transform. There are several advantages in data analysis for doing this.\n",
    "\n",
    "For convenience, `gwdama` provides a simple function that whiten the signal. It practically computes the Fourier transform of the data, devides it by the square root of its PSD, and retrurns back in the time domain. The optional arguments of this method are indeed the same as the `psd` one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1ff0a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'strain_whiten' in dm.keys(): \n",
    "    del dm['strain_whiten']  ## Delete previously define datasets with the same name\n",
    "# del dm['strain_whiten']\n",
    "whiten = dts.whiten(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3731356",
   "metadata": {},
   "source": [
    "If you print `dm` you will notice that a new dataset has been added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-memphis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot it\n",
    "whiten.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e862d2aa",
   "metadata": {},
   "source": [
    "A **whitned** signal is (usually) normalized such that the distribution of its values is described by a standard Normal distribution with zero mean and unit variance. SDOmetime, the normalization is fixed in the frequency domain, and the one of the time series is scaled by the number of samples.\n",
    "\n",
    "<div class=\"alert alert-danger\"><b>Exercise:</b> test the previous claim making a histogram of the whitened dataset.\n",
    "</div>\n",
    "\n",
    "As a quick check (not valid as an answer to the previous question!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792b780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean:     {:.3f}\\nStd.dev.: {:.3f}\".format(*norm.fit(whiten.data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f6b109",
   "metadata": {},
   "source": [
    "### Resampling\n",
    "\n",
    "This data is sampled at 4096 Hz, but our interferometers acts like a band pass filter for the gravitational wave signazl. Indeed, looking at the plot of the PSD you can notice that the \"bucket\" where they achieve their best sensitivity is limited to a band between 30 and about 1000 Hz. For this reason, it could be usefull to resample the data in order to work with smaller files and remove some high frequency noise.\n",
    "\n",
    "**decimate_recursive** resamples an input signal keeping only 1 elem every n\n",
    "\n",
    "**resample** resamples an inpunt signal to a dest freq. We will use in this case 512 so we are keeping only 1 every 8 values being our original signal sampled at 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3b17cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'strain_whiten_r512' in dm: \n",
    "    del dama['strain_whiten_r512']\n",
    "wres = whiten.resample(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-wrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot it\n",
    "dm['strain_whiten_r512'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-equipment",
   "metadata": {},
   "source": [
    "Just for fun try to resample at 128, 256, 1024, too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-disabled",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in [k for k in dm.keys() if 'strain_whiten_r' in k]: \n",
    "    del dm[key]  ## Delete all resamplings\n",
    "dm['strain_whiten'].resample(128)\n",
    "dm['strain_whiten'].resample(256)\n",
    "dm['strain_whiten'].resample(512)\n",
    "dm['strain_whiten'].resample(1024)\n",
    "print(dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca5499b",
   "metadata": {},
   "source": [
    "If you prefer to work directly with the usual list/array of values, simply get them with:\n",
    "values= dama[name_record].data \n",
    "times = dama[name_record].times\n",
    "\n",
    "**Remark**: Through all exercises, we will assume a constant `sample_rate` with fixed lentgh of chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a667ce",
   "metadata": {},
   "source": [
    "if you want to get more information on data and the cleaning process here a usefull link https://github.com/losc-tutorial/Data_Guide/blob/master/Guide_Notebook.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a7f051",
   "metadata": {},
   "source": [
    "Once data is ready we can try to encode with an **autoencoder**\n",
    "\n",
    "> [from wiki]: An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise”. Along with the reduction side, a reconstructing side is learned, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name\n",
    "\n",
    "some examples of autoencoders in keras can be found at https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "\n",
    "The idea is to encoding and decoding to original dimension searching for an optimum set of parameters that makes filtered signal more similar to original one. The encoded signal will retain all needed information but with a reduction of size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a15c6cf",
   "metadata": {},
   "source": [
    "To make our statement more concise we will summarize some ideas on Artificial Neural Networks and Deep Learning\n",
    "\n",
    "## Artificial Neural networks \n",
    "\n",
    "**Artificial neural networks (ANNs)** are computing systems vaguely inspired by the biological neural networks that constitute animal brains.\n",
    "\n",
    "An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron that receives a signal then processes it and can signal neurons connected to it. The \"signal\" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple time and along different paths\n",
    "\n",
    "Artificial networks could be build by many layers, each one connected to each others. There are amny types of networks. For example:\n",
    "\n",
    "A **feedforward neural network** is an artificial neural network wherein connections between nodes do not form a cycle, layers are connected starting from the top (input) to the bottom (output) and the activation flows from top to the end exclusively\n",
    "\n",
    "A **recurrent neural network (RNN)** is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs. Typically part of output of one layer is feed as input to the same layer at a different time\n",
    "\n",
    "[https://en.wikipedia.org/wiki/Artificial_neural_network]\n",
    "\n",
    "## Layers\n",
    "\n",
    "**Layers** are group of nodes that mimic some concepts. Each node inside the layer shares same input and output and the same activation function\n",
    "\n",
    "## Activation function\n",
    "\n",
    "The **activation function** of a node defines the output of that node given an input or set of inputs. Experience shows that only nonlinear activation functions allow networks to compute nontrivial problems using only small number of nodes\n",
    "\n",
    "- **rectifier or ReLU** activation function is an activation function defined as the positive part of its argument\n",
    "\n",
    "- **Sigmoid** activation function is an activation function which applies a sigmoid to the input. The assumption here is that we are interested in intermediate value of input, so we treat them in a liner way, greater values in modulo are not so important because are extreme and show fewer variation\n",
    "\n",
    "[ https://en.wikipedia.org/wiki/Activation_function ]\n",
    "\n",
    "## Loss function\n",
    "Loss is nothing but a prediction error of Neural Network. And the method to calculate the loss is called Loss Function. The output of keras contains two similar terms, val_loss and loss. *val_loss is the value of cost function for your cross-validation data and loss is the value of cost function for your training data.*\n",
    "\n",
    "## Optimizer\n",
    "\n",
    "The **optimizer** is used to explorer the parameters space searching which value correspond to a minimum value of the loss function\n",
    "\n",
    "## Learning \n",
    "\n",
    "**Supervised learning** is the machine learning task of learning a function that maps an input to an output based on example input-output pairs\n",
    "\n",
    "**Unsupervised learning** is the task of learning a function that maps an input to an output based on some cost function\n",
    "\n",
    "## Overfitting\n",
    "\n",
    "Overfitting is a modeling error in statistics that occurs when a function is too closely aligned to a limited set of data points. Overfitting the model generally takes the form of making an overly complex model to explain idiosyncrasies in the data under study\n",
    "\n",
    "## Regularization\n",
    "\n",
    "Regularization in machine learning is the process of regularizing the parameters that constrain, regularizes, or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, reducing the risk of Overfitting\n",
    "\n",
    "## Restatement of the task\n",
    "\n",
    "An autoencoder has an input layer, an output layer and one or more hidden layers connecting them. The output layer has the same number of units as the input layer. Its purpose is to reconstruct its own inputs. Therefore, autoencoders are unsupervised learning models. We want our autoencoder to efficient encodes using unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3dd595",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><b>Question:</b> Could an autoencoder have in total only a single layer? Two layers? Three?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\"><b>Remark:</b> The loss function is used to optimize your model. This is the function that will get minimized by the optimizer. A metric is used to judge the performance of your model!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b28bb6",
   "metadata": {},
   "source": [
    "We will use keras ( https://keras.io/ )\n",
    "\"Keras is an API designed for human beings, not machines. Keras follows best practices for reducing cognitive load: it offers consistent & simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear & actionable error messages. It also has extensive documentation and developer guides.\"\n",
    "\n",
    "Some usefull info:\n",
    "\n",
    "- https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "\n",
    "- https://keras.io/api/models/\n",
    "\n",
    "In the following we will use a slightly different approch from usual, instead of using a single command, we use sklearn as interface to call other functions. The idea is to learn to wrap Keras models for use in scikit-learn and how to use grid search. \"GridSearchCV is a library function that is a member of sklearn's model_selection package. It helps *to loop through predefined hyperparameters* and fit your estimator (model) on your training set. So, in the end, you can select the best parameters from the listed hyperparameters\" \n",
    "\n",
    "Please **Remember**: Keras is a high-level API built on Tensorflow. Scikit Learn is a general machine learning library built on top of NumPy\n",
    "\n",
    "[https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html ] \n",
    "\n",
    "[https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/ ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc3f869",
   "metadata": {},
   "source": [
    "Some layers used in keras:\n",
    "\n",
    "- **Dense** layer makes output=activation(dot(input, kernel) + bias) where kernel is the matrix of parameters and activation is the activation function. Each node is fed by the whole input! \n",
    "\n",
    "- **Dropout** layer randomly sets input units to 0 with a frequency of rate at each step during training time. Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed011d26",
   "metadata": {},
   "source": [
    "*Please remember, before process data with machine learning tecniques it is really important to normalize and resample at a fixed rate all the data. That's why we explained in the introduction how to do it*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286fd063",
   "metadata": {},
   "source": [
    "## Starter Example\n",
    "Write an autoencoder with keras only that takes as input a 1D signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69458be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "# This is the size of our encoded representations\n",
    "original_dim = 128\n",
    "encoding_dim = 32  # 32 floats -> compression of factor 4.0, assuming the input is 128 floats\n",
    "\n",
    "# This is our input image\n",
    "input_signal = keras.Input(shape=(original_dim,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = layers.Dense(encoding_dim, activation='relu')(input_signal)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = layers.Dense(original_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "# This model maps an input to its reconstruction\n",
    "autoencoder = keras.Model(input_signal, decoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.summary()\n",
    "\n",
    "# for fun, let's create a random input signal() mean=0 std=3) of 128*1000 elements. Why it is multiple of 128?\n",
    "noise = np.random.normal(0, 3, original_dim*1000)\n",
    "\n",
    "# a 2d-array of shape x=original_dim, y=100\n",
    "x_signal = np.stack(np.split(noise,1000))\n",
    "x_signal.shape\n",
    "\n",
    "# first 90 rows to use as train\n",
    "x_train=x_signal[0:900,:]\n",
    "#last 10 rows to use as test\n",
    "x_test = x_signal[900:,:]\n",
    "\n",
    "# encode with a batch of 50 rows (how many iterations?) for 15 times...\n",
    "autoencoder.fit(x_train, x_train, epochs=15, batch_size=30, shuffle=True, validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56154109",
   "metadata": {},
   "source": [
    "## Starter example II\n",
    "Add some layers for fun and acquire some confidence with keras.\n",
    "\n",
    "The workflow will be the following:\n",
    "```\n",
    "original -> encoded size 32 -> convolution -> encoded size 16 -> decoded size 32 -> decoded size original\n",
    "```\n",
    "It is convenient to play a bit with layers that expect a different number of dimension in this case\n",
    "\n",
    "Conv1D expects a 3d signal, for this reason we will add a singleton dimension doing\n",
    "\n",
    "*shape=(original_dim,1,)*\n",
    "\n",
    "please note the \"1\" added to the list of the dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d8ddcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "# This is the size of our encoded representations\n",
    "original_dim = 128\n",
    "encoding_dim = 32  # 32 floats -> compression of factor 4.0, assuming the input is 128 floats\n",
    "encoding_dim_1 = 16  # 16 floats -> compression of factor 8.0, assuming the input is 128 floats\n",
    "\n",
    "# This is our input image\n",
    "input_signal = keras.Input(shape=(1,original_dim))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "convolved = layers.Conv1D(filters=64, kernel_size=3, strides=1, padding=\"causal\", activation='relu', input_shape=(1,original_dim))(input_signal)\n",
    "encoded = layers.Dense(encoding_dim, activation='relu')(convolved)\n",
    "encoded_1 = layers.Dense(encoding_dim_1, activation='relu')(encoded)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded_1 = layers.Dense(encoding_dim, activation='sigmoid')(encoded_1)\n",
    "decoded = layers.Dense(original_dim, activation='sigmoid')(decoded_1)\n",
    "\n",
    "# This model maps an input to its reconstruction\n",
    "autoencoder = keras.Model(input_signal, decoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.summary()\n",
    "\n",
    "# for fun, let's create a random input signal() mean=0 std=3) of 128*1000 elements. Why it is multiple of 128?\n",
    "noise = np.random.normal(0, 3, original_dim*1000)\n",
    "\n",
    "# reshape to fit data\n",
    "# process the data to fit in a keras CNN properly\n",
    "# input data needs to be (N, C, X, Y) - shaped where\n",
    "# N - number of samples\n",
    "# C - singleton dimension\n",
    "# (X, Y) - sample size\n",
    "\n",
    "X_signal = noise.reshape((1000, 1, original_dim, 1))\n",
    "x_train = X_signal[0:900,:,:,:]\n",
    "x_test = X_signal[900:,:,:,:]\n",
    "# encode with a batch of 50 rows (how many iterations?) for 15 times...\n",
    "autoencoder.fit(x_train, x_train, epochs=15, batch_size=30, shuffle=True, validation_data=(x_test, x_test))\n",
    "X_signal.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa86d26a",
   "metadata": {},
   "source": [
    "Sometime you need to see what an intermediate layer is doing,\n",
    "\n",
    "here a simple skeleton, get a list of all layers used, create a keras function with two input parameters [input, learning_phase()] and [single_layer]...then call it with data\n",
    "\n",
    "\n",
    "inp = autoencoder.input                                           # input placeholder\n",
    "outputs = [layer.output for layer in autoencoder.layers]          # all layer outputs\n",
    "functors = [keras.function([inp, keras.learning_phase()], [out]) for out in outputs]    # evaluation functions\n",
    "\n",
    "Testing\n",
    "test = np.random.random(input_shape)[np.newaxis,...]\n",
    "layer_outs = [func([test, 1.]) for func in functors]\n",
    "print layer_outs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aca496d",
   "metadata": {},
   "source": [
    "## Starter example III\n",
    "\n",
    "Now we will try do denoise a simple sin wave using an autoencoder, the idea is that noise is small (and higher freqs) compared to signal. \n",
    "\n",
    "Autoencoding remove some data, maybe it could remove part of noise in this very simple case...\n",
    "\n",
    "Here you could see what's happen if! Feel free to change parameters like offset signal, num_array, interval, epochs and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e407439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_time(tmin=0,tmax=np.pi*10,step=0.05):\n",
    "    time = np.arange(tmin, tmax, step)\n",
    "    return time, len(time)\n",
    "\n",
    "def gen_signal(time, amp=1):\n",
    "    # Amplitude of the sine wave is sine of a variable like time.\n",
    "    amplitude = amp* np.sin(time)\n",
    "    return amplitude\n",
    "\n",
    "def gen_noise(npoints=1000, sigma=3, mean=0, amplitude=0.1):\n",
    "    noise = amplitude * np.random.normal(mean, sigma, npoints)\n",
    "    return noise\n",
    "\n",
    "def gen_signal_array(time, amp_signal, amp_noise, sigma, mean, nelems=1000, offset=0):\n",
    "    npoints = len(time)\n",
    "    arr = np.array([])\n",
    "    for el in range(0,nelems):\n",
    "        signal = gen_signal(time, amp_signal) + offset\n",
    "        noise = gen_noise(npoints, sigma, mean, amp_noise)\n",
    "        wave = signal + noise\n",
    "        arr = np.concatenate((arr, wave))\n",
    "    return arr\n",
    "\n",
    "time, npoints = gen_time()\n",
    "\n",
    "# This is the size of our encoded representations\n",
    "original_dim = npoints\n",
    "encoding_dim = npoints/2  \n",
    "encoding_dim_1 = npoints/20\n",
    "num_array = 10\n",
    "\n",
    "arr = gen_signal_array(time, amp_signal=0.5, amp_noise=0.05, sigma=3, mean=0, nelems=num_array, offset=0.5)\n",
    "\n",
    "# reshape to fit data\n",
    "# process the data to fit in a keras CNN properly\n",
    "# input data needs to be (N, C, X, Y) - shaped where\n",
    "# N - number of samples\n",
    "# C - singleton dimension\n",
    "# (X, Y) - sample size\n",
    "\n",
    "X_signal = arr.reshape((num_array, 1, original_dim, 1))\n",
    "x_train = X_signal[0:round(0.9*num_array),:,:,:]\n",
    "if round(0.9*num_array) == num_array:\n",
    "    x_test = x_train\n",
    "else:\n",
    "    x_test = X_signal[round(0.9*num_array):,:,:,:]\n",
    "\n",
    "def autoencoder_func(x_train, x_test, epochs=150):\n",
    "    # This is our input image\n",
    "    input_signal = keras.Input(shape=(1,original_dim))\n",
    "    # \"encoded\" is the encoded representation of the input\n",
    "    convolved = layers.Conv1D(filters=314, kernel_size=3, strides=1, padding=\"causal\", activation='relu', input_shape=(1,original_dim))(input_signal)\n",
    "    encoded = layers.Dense(encoding_dim, activation='relu')(convolved)\n",
    "    encoded_1 = layers.Dense(encoding_dim_1, activation='relu')(encoded)\n",
    "    # \"decoded\" is the lossy reconstruction of the input\n",
    "    decoded_1 = layers.Dense(encoding_dim, activation='sigmoid')(encoded_1)\n",
    "    decoded = layers.Dense(original_dim, activation='sigmoid')(decoded_1)\n",
    "\n",
    "    # This model maps an input to its reconstruction\n",
    "    autoencoder = keras.Model(input_signal, decoded)\n",
    "\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    autoencoder.summary()\n",
    "    history = autoencoder.fit(x_train, x_train, epochs=epochs, batch_size=10, shuffle=True, validation_data=(x_test, x_test))\n",
    "    decoded = autoencoder.predict(x_test)\n",
    "    return history, decoded, autoencoder\n",
    "\n",
    "history, decoded, model = autoencoder_func(x_train, x_test, epochs=20)\n",
    "\n",
    "# plot results\n",
    "plt.plot(time, x_train[0,:,:].reshape(len(time),1))\n",
    "plt.plot(time, decoded[0,:,:].reshape(len(time),1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af730a3",
   "metadata": {},
   "source": [
    "**Query**: Sometimes the decoded signal get clamped, are you able to understand why?\n",
    "\n",
    "**Query2**: Do you remember the difference between loss and val_loss?\n",
    "\n",
    "**Task**: Are you able to make a plot of loss vs epochs?\n",
    "\n",
    "**Query3**: The training time increase/decrease when changing the batch_size? and how about the loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7553e249",
   "metadata": {},
   "source": [
    "draw the model!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522f3dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3df7b6",
   "metadata": {},
   "source": [
    "**Remark** There are many parameters, you have to chose them to obtain the minimum of val_loss/loss\n",
    "\n",
    "It is a matter of chosing the right parameters, and you have to explore them, that's why we introduce a grid search\n",
    "\n",
    "That's function is in sklearn, let's jump straight to the next example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4218374",
   "metadata": {},
   "source": [
    "## Simple example\n",
    "In this example we will use different api from the one used before, it is easy to change between one format and the other\n",
    "\n",
    "A skeleton autoencoder will take input, import relevant functions, define a model, optimize it and test it\n",
    "\n",
    "Let's start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d03bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "# we will split our data in chunk of n elements and we will feeds our network with all these chunks\n",
    "n_features = 100 # is the size of a single chunk of data\n",
    "n_encoded = n_features\n",
    "\n",
    "# add a normal dataset to dama\n",
    "if 'random_n' in dm.keys(): \n",
    "    del dm['random_n']\n",
    "dm.create_dataset('random_n', data=np.random.normal(0, 1, (10000,)))\n",
    "\n",
    "# access its data and store in variable input_data\n",
    "input_data = dm['random_n'].data\n",
    "\n",
    "# split data in chunks of 100 element each one and stack each chunk vertically\n",
    "chunks = np.stack(np.split(input_data,100))\n",
    "chunks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeae26e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.optimizers import Nadam\n",
    "\n",
    "from keras import regularizers\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense, Activation, Dropout, TimeDistributed\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import joblib\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685b0996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a very simple model, feedforward neural network with two layers and with two different activation functions\n",
    "def baseline_model(learning_rate=5e-6, activation='sigmoid', bias1=1e-9, bias2=1e-9, ker1=1e-9, ker2=1e-9):\n",
    "  \n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_encoded, activation=activation, input_shape=(n_features,), bias_regularizer=regularizers.l1_l2(l1=bias1, l2=bias2), kernel_regularizer=regularizers.l1_l2(l1=ker1, l2=ker2)))\n",
    "    model.add(Dense(n_features, activation='relu', bias_regularizer=regularizers.l1_l2(l1=bias1, l2=bias2), kernel_regularizer=regularizers.l1_l2(l1=ker1, l2=ker2)))\n",
    "    \n",
    "    model.compile(optimizer=Nadam(lr=learning_rate), loss='mse', metrics=['mse'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70bc4e4",
   "metadata": {},
   "source": [
    "We had defined a function (with six \"hyperparameters\"!) that build a very minimalistic model\n",
    "\n",
    "**Sequential** initialize the model\n",
    "\n",
    "**add** method is used to add layers to your model\n",
    "\n",
    "**compile** method id used to configure the model for training https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile\n",
    "\n",
    "some reference to its parameters:\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Metric\n",
    "\n",
    "You could get useful debugging message with **model.summary()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84d2213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine learning\n",
    "# epochs is used for recurrent networks, we don't need them so we set to 1\n",
    "# batch_size is the number of input chunks\n",
    "#mlp = KerasRegressor(build_fn=baseline_model, epochs=1, batch_size=n_features, verbose=0)\n",
    "mlp = KerasRegressor(build_fn=baseline_model, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a89ed0",
   "metadata": {},
   "source": [
    "**KerasRegressor** is a wrapper to use keras from sklearn. It takes as parameters, your function model, epochs, batch size. See for example https://www.tensorflow.org/api_docs/python/tf/keras/wrappers/scikit_learn/KerasRegressor\n",
    "\n",
    "The **batch size** is a hyperparameter that defines the number of samples to work through before updating the internal model parameters\n",
    "\n",
    "The number of **epochs** is a hyperparameter that defines the number times that the learning algorithm will work through the entire training dataset. See for example https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bac79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# space of parameters used when call our model, only one value for each....\n",
    "# in the next example we will use a dictionary like the following, trying to search on parameters that regularise the network\n",
    "# param_distr = dict(bias1 = [1e-9], bias2 = [1e-9], ker2 = [1e-9], ker1 = [1e-9])\n",
    "param_distr = dict(learning_rate=[1e-6, 5e-6, 10e-6], activation=['relu', 'sigmoid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f999d7",
   "metadata": {},
   "source": [
    "param_distr is a dictionary of parameters to try, we will explore on all combinations\n",
    "**Query**: How many combinations will have in a general case? In this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e79ae5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build our model all together\n",
    "grid_search = GridSearchCV(estimator=mlp, param_grid=param_distr, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ee4b94",
   "metadata": {},
   "source": [
    "**GridSearchCV** Exhaustive search over specified parameter values for an estimator. So we are trying all combinations! https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n",
    "In particular: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV.fit\n",
    "\n",
    "X: array-like of shape (n_samples, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c530001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit it\n",
    "# tuning all nodes based on param_distr to make output \"near\" to input. Near here mean output of a distance function 'mse' \n",
    "grid_search.fit(chunks, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25905f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some info\n",
    "print(\"Best: %f using %s\" % (grid_search.best_score_, grid_search.best_params_))\n",
    "means  = grid_search.cv_results_['mean_test_score']\n",
    "stds   = grid_search.cv_results_['std_test_score']\n",
    "params = grid_search.cv_results_['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94939105",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><b>Question:</b> Is output data only positive? why?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e4f76d",
   "metadata": {},
   "source": [
    "## Exercise 1: Alter model changing activation functions\n",
    "\n",
    "Redo all steps but in the model function use a different activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f73e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a very simple model, feedforward neural network with two layers and with two different activation functions\n",
    "def baseline_model1(learning_rate=5e-6, activation='sigmoid', bias1=1e-9, bias2=1e-9, ker1=1e-9, ker2=1e-9):\n",
    "  \n",
    "    model1 = Sequential()\n",
    "    model1.add(Dense(n_features, activation=activation, input_shape=(n_features,), bias_regularizer=regularizers.l1_l2(l1=bias1, l2=bias2), kernel_regularizer=regularizers.l1_l2(l1=ker1, l2=ker2)))\n",
    "    model1.add(Dense(n_encoded, activation='sigmoid', bias_regularizer=regularizers.l1_l2(l1=bias1, l2=bias2), kernel_regularizer=regularizers.l1_l2(l1=ker1, l2=ker2)))\n",
    "    \n",
    "    model1.compile(optimizer=Nadam(lr=learning_rate), loss='mse', metrics=['mse'])\n",
    "\n",
    "    return model1\n",
    "\n",
    "\n",
    "mlp1 = KerasRegressor(build_fn=baseline_model1, epochs=1, batch_size=n_features, verbose=0)\n",
    "param_distr1 = dict(learning_rate=[1e-5], activation=['sigmoid'], bias1 = [1e-9], bias2 = [1e-9], ker2 = [1e-9], ker1 = [1e-9])\n",
    "grid_search1 = GridSearchCV(estimator=mlp1, param_grid=param_distr1, cv=3)\n",
    "grid_search1.fit(chunks, chunks)\n",
    "print(\"Best: %f using %s\" % (grid_search1.best_score_, grid_search.best_params_))\n",
    "means  = grid_search1.cv_results_['mean_test_score']\n",
    "stds   = grid_search1.cv_results_['std_test_score']\n",
    "params = grid_search1.cv_results_['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3813d1",
   "metadata": {},
   "source": [
    "## Exercise 2: example with real data\n",
    "\n",
    "We need to follow exactly what we did before but using our gw data insetad of our simulated data\n",
    "\n",
    "Recall that to get data from dama you could use \"dama['random_n'].data\" \n",
    "\n",
    "In real life before start we will have to decide what will be\n",
    "- our chunk size\n",
    "- our metric\n",
    "\n",
    "using frequency and other values as starting point\n",
    "\n",
    "We can however start with an (nearly!) arbitrary value and see what will happen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-poland",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dm['strain_whiten_r512'])\n",
    "input_data=np.array(dm['strain_whiten_r512'].data)\n",
    "input_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-being",
   "metadata": {},
   "source": [
    "get a number of point multiple of 100, for example we will use first 49 * 100 elems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data[0:4900].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-philippines",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chunks = np.stack(np.split(input_data[0:4900],100))\n",
    "chunks.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-sound",
   "metadata": {},
   "source": [
    "as already said, we have an array-like of shape (n_samples, n_features)\n",
    "so here we have 49 features in 100 samples\n",
    "\n",
    "To make it easier to tweak the model, we define the number of features and the number of encoded features as hyperparameters in the `baseline_model` function.\n",
    "\n",
    "We can now proceed as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-flash",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a very simple model, feedforward neural network with two layers and with two different activation functions\n",
    "def baseline_model2(n_features=49, n_encoded=100, learning_rate=5e-6, activation='sigmoid', bias1=1e-9, bias2=1e-9, ker1=1e-9, ker2=1e-9):\n",
    "  \n",
    "    model2 = Sequential()\n",
    "    model2.add(Dense(n_encoded, activation=activation, input_shape=(n_features,), bias_regularizer=regularizers.l1_l2(l1=bias1, l2=bias2), kernel_regularizer=regularizers.l1_l2(l1=ker1, l2=ker2)))\n",
    "    model2.add(Dense(n_features, activation='sigmoid', bias_regularizer=regularizers.l1_l2(l1=bias1, l2=bias2), kernel_regularizer=regularizers.l1_l2(l1=ker1, l2=ker2)))\n",
    "    \n",
    "    model2.compile(optimizer=Nadam(lr=learning_rate), loss='mse', metrics=['mse'])\n",
    "    return model2\n",
    "\n",
    "\n",
    "mlp2 = KerasRegressor(build_fn=baseline_model2, epochs=100, batch_size=10, verbose=0)\n",
    "param_distr2 = dict(learning_rate=[1e-5], activation=['sigmoid'], bias1 = [1e-9], bias2 = [1e-9], ker2 = [1e-9], ker1 = [1e-9])\n",
    "grid_search2 = GridSearchCV(estimator=mlp2, param_grid=param_distr2, cv=3)\n",
    "grid_search2.fit(chunks, chunks)\n",
    "print(\"Best: %f using %s\" % (grid_search2.best_score_, grid_search2.best_params_))\n",
    "means  = grid_search2.cv_results_['mean_test_score']\n",
    "stds   = grid_search2.cv_results_['std_test_score']\n",
    "params = grid_search2.cv_results_['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-drilling",
   "metadata": {},
   "source": [
    "**Note**: Could you alter code to have 49 elems(samples) of 100 features each one?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ddfd6b",
   "metadata": {},
   "source": [
    "## Exercise 3: Alter model, adding a dropout layer and a new dense layer\n",
    "\n",
    "Let's make a complex example, adding a new dense layer(s), a droput layer and asking for a compression factor of 4 times\n",
    "\n",
    "Dropout in this case help in reduce the number of neurons, in order to avoid the problem of \"sleeping neurons\", you could add a convolution layer for fun if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-plenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features=49\n",
    "\n",
    "# a very simple model, feedforward neural network with two layers and with two different activation functions\n",
    "def baseline_model3(dropout=0.1, bias1=1e-9, bias2=1e-9, ker1=1e-9, ker2=1e-9):\n",
    "  \n",
    "    model3 = Sequential()\n",
    "    \n",
    "    model3.add(Dense(n_features, activation='relu', input_shape=(n_features,), bias_regularizer=regularizers.l1_l2(l1=bias1, l2=bias2), kernel_regularizer=regularizers.l1_l2(l1=ker1, l2=ker2)))\n",
    "    model3.add(Dropout(dropout))\n",
    "    model3.add(Dense(int(n_features/2), activation='relu', bias_regularizer=regularizers.l1_l2(l1=bias1, l2=bias2), kernel_regularizer=regularizers.l1_l2(l1=ker1, l2=ker2)))\n",
    "    model3.add(Dropout(dropout))\n",
    "    model3.add(Dense(int(n_features/4), activation='relu', bias_regularizer=regularizers.l1_l2(l1=bias1, l2=bias2), kernel_regularizer=regularizers.l1_l2(l1=ker1, l2=ker2)))\n",
    "    model3.add(Dropout(dropout))\n",
    "\n",
    "    model3.add(Dense(int(n_features/4), activation='sigmoid', bias_regularizer=regularizers.l1_l2(l1=bias1, l2=bias2), kernel_regularizer=regularizers.l1_l2(l1=ker1, l2=ker2)))\n",
    "    model3.add(Dense(int(n_features/2), activation='sigmoid', bias_regularizer=regularizers.l1_l2(l1=bias1, l2=bias2), kernel_regularizer=regularizers.l1_l2(l1=ker1, l2=ker2)))\n",
    "    model3.add(Dense(n_features, activation='sigmoid', bias_regularizer=regularizers.l1_l2(l1=bias1, l2=bias2), kernel_regularizer=regularizers.l1_l2(l1=ker1, l2=ker2)))\n",
    "    \n",
    "    model3.compile(optimizer=Nadam(lr=5e-6), loss='mse', metrics=['mse'])\n",
    "\n",
    "    return model3\n",
    "\n",
    "\n",
    "mlp3 = KerasRegressor(build_fn=baseline_model3, epochs=1, batch_size=100, verbose=0)\n",
    "param_distr3 = dict(dropout=[0.1, 0.5, 0.9], bias1 = [1e-9], bias2 = [1e-9], ker2 = [1e-9], ker1 = [1e-9])\n",
    "grid_search3 = GridSearchCV(estimator=mlp3, param_grid=param_distr3, cv=3)\n",
    "grid_search3.fit(chunks, chunks)\n",
    "print(\"Best: %f using %s\" % (grid_search3.best_score_, grid_search3.best_params_))\n",
    "means  = grid_search3.cv_results_['mean_test_score']\n",
    "stds   = grid_search3.cv_results_['std_test_score']\n",
    "params = grid_search3.cv_results_['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-giving",
   "metadata": {},
   "source": [
    "We altered number of neurons, is metric better or worse compare to other exercise? was expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-framing",
   "metadata": {},
   "source": [
    "## Exercise 4: Tune parameters\n",
    "\n",
    "**Add to param_distr both Batch size and Epochs, retrain and take best values**\n",
    "\n",
    "...same model as before...\n",
    "\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "\n",
    "epochs = [10, 50, 100]\n",
    "\n",
    "param_distr = dict(bias1 = [1e-9], bias2 = [1e-9], ker2 = [1e-9], ker1 = [1e-9], batch_size=batch_size, epochs=epochs )\n",
    "\n",
    "grid_search4 = GridSearchCV(estimator=mlp4, param_grid=param_distr4, cv=3)\n",
    "\n",
    "grid_result=grid_search4.fit(chunks, chunks)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "**Same as before but optime optimizer**\n",
    "\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "\n",
    "param_grid = dict(optimizer=optimizer)\n",
    "\n",
    "**With this system you could optimize any hyperparameters...but it works only for local minimum point. No one can guarantee that a minimum for non optimal parameters exists!**\n",
    "\n",
    "https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0138ac3",
   "metadata": {},
   "source": [
    "## Exercise 5: Plot output data and compare with input signal\n",
    "\n",
    "Let's put all together and try to visualize the effect of the compression.\n",
    "\n",
    "Define the best autoencoder that you can, possibly using the indications obtained from the GridSearchCV.\n",
    "Then, try to increase the number of nodes of the internal hidden layer until you manage to reproduce exactly the input.\n",
    "In this case you have no compression and no loss in terms of quality.\n",
    "You can then try to decrease the number of encoded features and superpose the input and the predicted waveforms. The less the number of nodes of the hidden layer, the more the two waveforms are different, but the higher compression rate you are reaching.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-trace",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search2.predict(chunks).shape\n",
    "def baseline_model4(hidden_layers, n_features=49):\n",
    "    model3 = Sequential()\n",
    "    model3.add(Dense(hidden_layers, activation='tanh', input_shape=(n_features,), kernel_initializer='he_normal' ))\n",
    "    model3.add(Dense(n_features, activation='linear', kernel_initializer='he_normal'))\n",
    "    model3.compile(optimizer=Nadam(lr=3e-3), loss='mse', metrics=['mse'])\n",
    "    return model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f1b5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the data\n",
    "n_features = 49\n",
    "print(dm['strain_whiten_r512'])\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "input_data=np.array(dm['strain_whiten_r512'].data)\n",
    "q = StandardScaler ()\n",
    "chunks = np.reshape(input_data[:(len(input_data)//n_features)*n_features], (-1, n_features))\n",
    "chunks = q.fit_transform (chunks)\n",
    "print (chunks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-neutral",
   "metadata": {},
   "source": [
    "There are many ways to get the encoded signal, for example re-train the model, using as parameterets the ones found by GridSearchCV. In this case :\n",
    "\n",
    "Best: -0.280207 using **{'bias1': 1e-09, 'bias2': 1e-09, 'ker1': 1e-09, 'ker2': 1e-09}**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuid a model\n",
    "model=baseline_model4(hidden_layers=49, n_features=49)\n",
    "model.summary()\n",
    "# fit it\n",
    "history = model.fit(chunks,chunks, epochs=100, validation_split=0.1, verbose=True)\n",
    "plt.plot (history.history['loss'], label = \"loss\")\n",
    "plt.plot (history.history['val_loss'], label = \"val_loss\")\n",
    "plt.legend()\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7e0a8d",
   "metadata": {},
   "source": [
    "In this case there is no compression and we expect a rather good agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-leader",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "predicted_chunks = model.predict(chunks)\n",
    "for original, compressed in zip (chunks, predicted_chunks[:5]):\n",
    "    plt.plot(q.inverse_transform([original])[0], alpha=0.5, label = \"Original\")\n",
    "    plt.plot(q.inverse_transform([compressed])[0], label = \"Compressed\")\n",
    "    plt.legend()\n",
    "    plt.ylabel (\"Normalized strain\")\n",
    "    plt.xlabel (\"Sample\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27454b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuid a model\n",
    "model_25=baseline_model4(hidden_layers=25, n_features=49)\n",
    "model_25.summary()\n",
    "# fit it\n",
    "history = model_25.fit(chunks,chunks, epochs=100, validation_split=0.1, verbose=True)\n",
    "plt.plot (history.history['loss'], label = \"loss\")\n",
    "plt.plot (history.history['val_loss'], label = \"val_loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8de1e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "predicted_chunks = model_25.predict(chunks)\n",
    "for original, compressed in zip (chunks, predicted_chunks[:5]):\n",
    "    plt.plot(q.inverse_transform([original])[0], alpha=0.5, label = \"Original\")\n",
    "    plt.plot(q.inverse_transform([compressed])[0], label = \"Compressed\")\n",
    "    plt.legend()\n",
    "    plt.ylabel (\"Normalized strain\")\n",
    "    plt.xlabel (\"Sample\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-facial",
   "metadata": {},
   "source": [
    "**Note**: Compression at work! 25 Features instead of 49\n",
    "\n",
    "You can access weight using\n",
    "\n",
    "**model.get_weights()[0]** to get all values\n",
    "\n",
    "**model.layers[0].get_weights()[0]** to get values of a single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-trash",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example\n",
    "# build a model of only encoding layers\\n\",\n",
    "encoder = Sequential()\n",
    "\n",
    "encoder.add(model_25.layers[0])\n",
    "\n",
    "encoded = encoder.predict(chunks)\n",
    "print(encoded)\n",
    "encoded.shape\n",
    "\n",
    "# for example\n",
    "model.get_weights()[0] \n",
    "model.layers[0].get_weights()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-briefing",
   "metadata": {},
   "source": [
    "**We took only 4900 elems ... restart with a larger database, and use remain elems as test, so just try predict them and see how differ**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56e0b3b",
   "metadata": {},
   "source": [
    "## Exercise 6: Save output files and compute their size and their entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1443ad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save our model\n",
    "model.save(\"test.h5\")\n",
    "# to save our encoded file\n",
    "# don't forget to delete our useness item before saving to disk\n",
    "dm.write_gwdama('outputfile')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
