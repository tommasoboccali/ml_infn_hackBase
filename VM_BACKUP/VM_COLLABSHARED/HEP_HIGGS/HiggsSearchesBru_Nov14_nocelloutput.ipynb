{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HiggsSearchesBru_Nov14_nocelloutput.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CwfBRRX5Els"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tommasoboccali/ml_infn_hackBase/blob/main/HiggsSearchesBru_Nov14_nocelloutput.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "<img src=\"https://cms-docdb.cern.ch/cgi-bin/PublicDocDB/RetrieveFile?docid=3045&filename=CMSlogo_color_label_1024_May2014.png&version=3\" style=\"display: center; margin: auto;\" width=\"50\" align=\"right\"  /><br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSAVrGjI5m2U"
      },
      "source": [
        "<img src=\"https://agenda.infn.it/event/25855/logo-3765382068.png\" style=\"display: center; margin: auto;\" width=\"950\" align=\"center\"  /><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjFJW_zF7JlE"
      },
      "source": [
        "\n",
        "# <font color=\"#002A48\">Signal/background discrimination for the VBF Higgs four lepton decay channel with the CMS experiment : introduction to the exercise</font>\n",
        "\n",
        "\n",
        "**<font color=\"#002A48\">Authors:**</font> Brunella D'Anzi<sup>1</sup>,Nicola De Filippis<sup>2</sup>, Domenico Diacono<sup>1</sup>, Walaa Elmetenawee<sup>1</sup>, Giorgia Miniello<sup>1</sup>, Andre Sznajder<sup>3</sup>\n",
        "\n",
        "\n",
        "<sup>1</sup> University of Bari, INFN Bari Section\n",
        "\n",
        "\n",
        "<sup>2</sup> Politecnico of Bari, INFN Bari Section\n",
        "\n",
        "\n",
        "<sup>3</sup> University of Rio de Janeiro \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0x7ODVzcaKA"
      },
      "source": [
        "In this exercise, you will perform a binary classification task using 2018 CMS Monte Carlo  (MC) simulated samples representing the Vector Boson Fusion (VBF) Higgs boson production in the four-charged-lepton final state signal and its main background processes. For this purpose, you will implement a Machine Learning (ML) algorithm consisting in an **Artificial Neural Network (ANN)**.\n",
        "\n",
        "### <font color=\"#002A48\">Learning Goals of the exercise</font>\n",
        "* You will learn how a *Multivariate Analysis algorithm* works and how a Machine Learning model must be implemented;\n",
        "\n",
        "* you will acquire basic knowledge about the *Higgs boson physics* as it is described by the Standard Model. During the exercise, you will be invited to plot some physical observables in order to understand what is the underlying Particle Physics problem;\n",
        "\n",
        "*  you will be invited to *change hyper-parameters* of the ANN algorithm in order to better understand which can be the consequences in terms of the  model performance;\n",
        "\n",
        "*   you will understand that the choice of the *input variables* is key to ensure the goodness of the algorithm itself, since an optimal choice allows achieving the best possible performance;\n",
        "\n",
        "*   moreover, you will have the possibility of changing the background data-sets, the decay channels, and see how the performance of the ML algorithm changes consequently. \n",
        "\n",
        "To find a long version of this exercise (including a Random Forest algorithm implementation), see the <font color=\"#002A48\">ML_INFN knowledge basis</font> ([here](https://confluence.infn.it/pages/viewpage.action?pageId=53906361))\n",
        "\n",
        "We left for you statistical and particle physics concepts concerning the exercise so that you do not have to go back and forth from the slides (available [here](https://docs.google.com/presentation/d/1sf428xC2Y1HLrWkTrIRKNAQGcoU4zPIg15SLvxpWosk/edit?usp=sharing)) to this Jupyter Notebook! You can skip those parts at any moment (just click on the cell and press the &#8595; symbol on your keyboard or use the summary icon on the left hand side they are labelled as THEORY sections) and go through with the exercise! \n",
        "\n",
        "Moreover, note that you are NOT expected to do all during the hackathon time (in particular the challenge at the end of the exercise); you can also have a look later. Have fun!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_VF5HCifzrD"
      },
      "source": [
        "\n",
        "\n",
        "### <font color=\"#002A48\">Multivariate Analysis and Machine learning algorithms: basic THEORY concepts</font>\n",
        "Multivariate Analysis algorithms receive as input a set of discriminating variables. Each single variable does not allow to reach an optimal discrimination power between two categories (signal and background). Therefore the algorithms compute an output that combines the input variables. \n",
        "\n",
        "This is what every **Multivariate Analysis (MVA) discriminator** does. The discriminant output, also called *discriminator, score , or classifier*, is used as a test statistic and is then adopted to perform the signal selection. It could be used as a variable on which a cut can be applied under a particular hypothesis test.\n",
        "\n",
        "In particular, Machine Learning tools are models which have enough capability to define their own internal representation of data to accomplish two main tasks : *learning from data* and make predictions without being explicitly programmed to do so. \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/bdanzi/Higgs_exercise/fbb053597adf9e950d7c7ccef4b72797d0a22f9d/Hypothesis_testing.png\" style=\"display: center; margin: auto;\" width=\"500\" align=\"right\"  /><br>\n",
        "In the case of binary classification, firstly the algorithm is *trained* with two data-sets: \n",
        "* one that contains events distributed according to **the null** (in our case **signal** - there exist other conventions in actual physics analyses) **hypothesis $H_{0}$** ;\n",
        "* another one according to the **alternative** (in our case **background**) **hypothesis $H_{1}$** .\n",
        "\n",
        "Then the algorithm must learn how to classify new data-sets (the test data-set in our case). \n",
        "<br><br>\n",
        "This means that you have the same set of features (random variables) with their own distribution on the $H_{0}$ and $H_{1}$ hypotheses.\n",
        "<br>\n",
        "In order to obtain a good ML classifier with high discriminating power, you will follow the steps: \n",
        "\n",
        "* **Training (learning)**: a discriminator is built by using all the input variables. Then, the parameters are iteratively modified by comparing the discriminant output to the true label of the data-set (*supervised machine learning algorithms*). This phase is crucial: one should tune the input variables and the parameters of the algorithm!\n",
        "  * As an alternative, algorithms that group and find patterns in the data according to the observed distribution of the input data are called *unsupervised learning*. It is basically used when the training set is unlabeled.\n",
        "  * A good habit is to train multiple models with various hyperparameters on a “reduced” training set ( i.e. the full training set minus the so-called **validation set**), and then select the model that performs best on the validation set.\n",
        "  * Once, the validation process is over, you can re-train the best\n",
        "model on the full training set (including the validation set), and this\n",
        "gives you the final model.\n",
        "\n",
        "* **Test**: once the training has been performed, the discriminator score is computed in a *separated, independent data-set*  for both $H_{0}$ and $H_{1}$.\n",
        "* A comparison is made between test and training classifier and their performances (in terms of ROC curves) are evaluated.\n",
        " * If the test fails and the performance of the test and training are different, this could be a symptom of **overtraining** and our model can be considered not good!\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0p89g-7jAG0"
      },
      "source": [
        "## <font color=\"#002A48\">Uploading Files</font>\n",
        "First of all, you need to access the data. We provide files in [ROOT](https://root.cern) format; the commented samples can be used later as optional exercise (istructions at the end of the notebook). In order to get the sample files, you can use Linux Shell commands (\"**cp**\", \"**wget**\", ...) to put the input files in a given place. Here, we put the files in a local directory, from a public web server. In this exercise you will use a Software as Services (SAS) from [Cloud@ReCaS-Bari](https://www.recas-bari.it/index.php/it/recas-bari-i-servizi-it/recas-bari-i-servizi/cloud-recas-software-as-a-service).\n",
        "\n",
        "In order not to repeat the download N times, we check if the files are alredy available.\n",
        "\n",
        "The \"! command\" syntax in Jupyter executes a shell command, so we can use it to download the files and check later if they are ok.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ExFRCkLfMpo"
      },
      "source": [
        "\n",
        "#This is selecting the full data-set, it will take a while (2-3 minutes). \n",
        "#Comment afterwards, no need to re-run this box!\n",
        "\n",
        "!pip install uproot\n",
        "\n",
        "files = { #time for uploading = almost 3 minutes\n",
        "    \"VBF_HToZZTo4mu.root\" : \"PCH8ZgVPVwgtqXF\", #signal events| 4-muons channel\n",
        "    \"GluGluHToZZTo4mu.root\": \"64ngyPZosPKsp3Q\", #1st background events| 4-muons ch\n",
        "    \"ZZTo4mu.root\"       : \"JUzjDb5tjy6tZpC\", # 2nd background| 4-muons\n",
        "    #\"ttH_HToZZ_4mu.root\"   :\"RzS4CXKExWijjfQ\", 3nd background | 4-muons\n",
        "    #\"VBF_HToZZTo4e.root\": \"dBdaKwA13bygU8c\", # alternative signal| 4e-channel\n",
        "    #\"GluGluHToZZTo4e.root\": \"2DxFLqLtjkbWFdM\", #alternative background| 4e-ch\n",
        "    #\"ZZTo4e.root\"     : \"NhAutemXTrVunU0\", #alternative background| 4e-ch\n",
        "     #\"ttH_HToZZ_4e.root\" :   \"DRDeVWSSZzJNduw\" #alternative background| 4e-ch\n",
        "    }\n",
        "    \n",
        "#If the size of the root files is less than the expected one write \n",
        "#use the remove command \"!rm -f name_file_you_want_to_remove.root\n",
        "\n",
        "import os \n",
        "for file in files.items():\n",
        "  if not os.path.exists(file[0]):\n",
        "    b = os.system ( \"wget -O %s --no-check-certificate 'https://recascloud.ba.infn.it/index.php/s/%s/download'\" % file )\n",
        "    if b: raise IOError ( \"Error in downloading the file %s : (%s)\" % file )\n",
        "\n",
        "\n",
        "# If you have trouble running the previous commands in this cell, you can import the same files from INFN Pandora\n",
        "\n",
        "\n",
        "# !pip install uproot\n",
        "\n",
        "#files = { #time for uploading = almost 3 minutes\n",
        "#    \"VBF_HToZZTo4mu.root\" : \"a00d03/dl/VBF_HToZZTo4L.root\", #signal events| 4-muons channel\n",
        "#    \"GluGluHToZZTo4mu.root\": \"a00d03/dl/GluGluHToZZTo4L.root\", #1st background events| 4-muons ch\n",
        "#    \"ZZTo4mu.root\"       : \"a00d03/dl/ZZTo4L.root\", # 2nd background| 4-muons\n",
        "    #\"ttH_HToZZ_4mu.root\"   :\"a00d03/dl/ttH_HToZZ_4L.root\", 3nd background | 4-muons\n",
        "    #\"VBF_HToZZTo4e.root\": \"7a81f2/dl/VBF_HToZZTo4L.root\", # alternative signal| 4e-channel\n",
        "    #\"GluGluHToZZTo4e.root\": \"7a81f2/dl/GluGluHToZZTo4L.root\", #alternative background| 4e-ch\n",
        "    #\"ZZTo4e.root\"     : \"7a81f2/dl/ZZTo4L.root\", #alternative background| 4e-ch\n",
        "     #\"ttH_HToZZ_4e.root\" :   \"7a81f2/dl/ttH_HToZZ_4L.root\" #alternative background| 4e-ch\n",
        "    #}\n",
        "    \n",
        "#If the size of the root files is less than the expected one write \n",
        "#use the remove command \"!rm -f name_file_you_want_to_remove.root\n",
        "\n",
        "#import os \n",
        "#for file in files.items():\n",
        "#  if not os.path.exists(file[0]):\n",
        "#    b = os.system ( \"wget -O %s --no-check-certificate 'https://pandora.infn.it/public/%s'\" % file )\n",
        "#    if b: raise IOError ( \"Error in downloading the file %s : (%s)\" % file )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb4pUePobWFV"
      },
      "source": [
        "# <font color=\"#002A48\">Introduction to the physics problem</font>\n",
        "In this section you will find the following subsections:\n",
        "* **Particle Physics basic concepts: the Standard Model and the Higgs boson**<br>\n",
        "you may skip it if you already have basic knowledge about Particle Physics (cross-section,decay channels,Standard Model definitions, etc.).\n",
        "* **Data exploration:** <br>it is important that you pay attention to this section in order to understand all the next steps of the exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY51FM8XjpbR"
      },
      "source": [
        "\n",
        "## <font color=\"#002A48\">Particle Physics basic THEORY concepts: the Standard Model and the Higgs boson</font>\n",
        "\n",
        "\n",
        "If everything went well until now, let's have a look at the physics we are interested in!\n",
        "\n",
        "\n",
        "<br>\n",
        "<img src=\"https://raw.githubusercontent.com/bdanzi/Higgs_exercise/main/SM.png\" width=\"400\" align=\"left\">\n",
        "\n",
        " The **Standard Model** of elementary particles represents our knowledge of the microscopic world. It describes the matter constituents (quarks and leptons)  and their interactions (mediated by bosons), which are the electromagnetic, the weak, and the strong interactions.\n",
        "\n",
        "\n",
        " Among all these particles, the **Higgs boson** still represents a very peculiar case. It is the second heaviest known elementary particle (mass of 125 GeV) after the top quark (175 GeV).\n",
        "\n",
        " The ideal instrument for measuring the Higgs boson properties is a particle collider. The **Large Hadron Collider** (LHC), situated nearby Geneva, between France and Switzerland, is the largest proton-proton collider ever built on Earth. It consists of a 27 km circumference ring, where proton beams are smashed at a center-of-mass energy of 13 TeV (99.999999% of the speed of light). At the LHC, 40 Million collisions / second occurs, providing an enormous amount of data. Thanks to these data, **ATLAS** and **CMS** experiments discovered the missing piece of the Standard Model, the Higgs boson, in 2012.\n",
        "\n",
        " During a collision, the energy is so high that protons are \"broken\" into their fundamental components, i.e. **quarks** and **gluons**, which can interact together, producing particles that we don't observe in our everyday life, such as the top quark. The production of a top quark is, by the way, a relatively \"rare\" phenomenon, since there are other physical processes that occur more often, such as those initiated by strong interaction, producing lighter quarks (such as up, down, strange quarks). In high-energy physics, we speak about the **cross-section** of a process. We say that the top quark production has a smaller cross-section than one of the productions of light quarks.\n",
        "\n",
        " The experimental consequence is that distinguishing the decay products of a top quark from a light quark can be extremely difficult, due to the quite larger probability to occur of the latter phenomenon.\n",
        "### <font color=\"#002A48\">Experimental signature of Higgs boson in a particle detector</font>\n",
        "\n",
        "Let's first understand what are the experimental signatures and how the detectors work at the LHC experiment. As an example, this is a sketch of the Compact Muon Solenoid (CMS) detector.\n",
        "\n",
        "<br>\n",
        "<img src=\"https://raw.githubusercontent.com/bdanzi/Higgs_exercise/main/EPS_CMS_Slice.png\" width=\"800\" >\n",
        "\n",
        "A collider detector is organized in layers: each layer is able to distinguish and measure different particles and their properties. For example, the silicon tracker detects each particle that is charged. The electromagnetic calorimeter detects photons and electrons. The hadronic calorimeter detects hadrons (such as protons and neutrons). The muon chambers detect muons (that have a long lifetime and travel through the inner layers).\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/bdanzi/Higgs_exercise/main/Schermata%202021-04-09%20alle%2023.42.06.png\" width=\"200\" height=\"200\" style=\"display:center; margin: auto;\" align=\"left\">\n",
        "\n",
        "Our physics problem consists in detecting the so-called **“golden decay channel”  $H \\to ZZ^{*} \\to l^{+}l^{-}l'^{+}l'^{-}$**  which is one of the possible Higgs boson's decays: its name is due to the fact that it has the clearest and cleanest signature of all the possible Higgs boson's decay modes. The decay chain is sketched here: the Higgs boson decays into Z boson pairs, which in turn decay into a lepton pair (in the picture, muon-antimuon or electron-positron pairs). In this exercise, you will use only data-sets concerning the **$4\\mu$ decay channel** and the data-sets about the **4e channel** are given to you to be analyzed as an optional exercise. At the LHC experiments, the decay channel **2e2mu** is also widely analyzed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mncOajo7pTcz"
      },
      "source": [
        "## <font color=\"#002A48\">Data exploration</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft0tVpGHj6cX"
      },
      "source": [
        "Now, let's see what you have in your directory now running the following command line: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlfeFoJfj4lC"
      },
      "source": [
        "!ls -l --block-size=MB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEXFpeJ3GMdz"
      },
      "source": [
        "In this exercise, we are mainly interested in the following ROOT files (you may look at [ROOT manual](https://root.cern/manual/storing_root_objects/) if you prefer to learn more about which kind of objects you can store in them):\n",
        "\n",
        "\n",
        "*   **VBF_HToZZTo4mu.root** \n",
        "*   **GluGlueHtoZZTo4mu.root** \n",
        "*   **ZZto4mu.root**.\n",
        "\n",
        "They should be ~ 1.3 GB (respectively 191 MB, 63 MB and 1025 MB). If it is less than that, you should remove them and retry the download from the second cell(\"**!rm -f name_of_the_root_file.root**\").\n",
        "\n",
        "The VBF ROOT file contains the Higgs boson production (mass of 125 GeV) via the Vector Boson Fusion (VBF) mechanism $q\\bar q' \\to H q\\bar q'\\to ZZ^{(*)} q\\bar q'\\to 4\\mu q\\bar q' $ - our **signal events** - that we want to discriminate from the so-called Gluon Gluon Fusion $gg \\to H \\to ZZ^{(*)} \\to 4\\mu $ Higgs production events and the QCD process $q\\bar q' \\to ZZ\\to 4\\mu$ which are both **irreducible backgrounds** (you can see an example of an irreducible background in the Feynmann diagram at the leading order (LO) in the picture below and the cross-sections expected for the Higgs boson production processes and the branching ratios for its decay channels ). <br><img src=\"https://raw.githubusercontent.com/bdanzi/Higgs_exercise/main/Schermata%202021-04-10%20alle%2009.52.41.png\" style=\"display: block; margin: auto;\" width=\"200\" align=\"right\"  /><br>\n",
        "The processes are characterized by the same final-state particles but we can use the value of multiple variables,such as kinematic properties of the particles, for classifying data into the two categories,signal and background.\n",
        "\n",
        "The first one is the statistically less probable process that results in producing the Higgs boson at the Large Hadron Collider (LHC) experiments and it is still understudies by the CMS collaboration.\n",
        "<br>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/bdanzi/Higgs_exercise/main/Schermata%202021-04-20%20alle%2017.09.35.png\" style=\"display: block; margin: auto;\" width=\"950\" /><br>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/bdanzi/Higgs_exercise/main/Schermata%202021-04-20%20alle%2017.17.01.png\" style=\"display: block; margin: auto;\" width=\"950\" /><br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W294q6snAGzf"
      },
      "source": [
        "In order to train our Machine Learning algorithms, we will look at the decay products of our physics problem. In our case we going to deal with:\n",
        "\n",
        "\n",
        "*   electrically-charged leptons (electrons or muons, denoted with $l$)\n",
        "*   particle jets (collimated streams of particles originating from quarks or gluons, denoted with $j$).\n",
        "\n",
        "For each object, several kinetic variables are measured:\n",
        "\n",
        "* the momentum transverse to the beam direction ($pt$)\n",
        "* two angles $\\theta$ (polar) and $\\phi$ (azimuthal) - see picture below for the CMS reference frame used.\n",
        "* for convenience, at hadron colliders, the pseudorapidity $\\eta$, defined as\n",
        " $\\eta=-ln(tan(\\theta/2))$ is used instead of the polar angle $\\theta$.\n",
        "\n",
        "We will use some of them for training our Machine Learning algorithms.\n",
        "<img src=\"https://raw.githubusercontent.com/bdanzi/Higgs_exercise/main/Schermata%202021-04-10%20alle%2011.05.00.png\" style=\"display: block; margin: auto;\" width=\"800\" /><br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YCc4lCpOv9B"
      },
      "source": [
        "# <font color=\"#002A48\">Load data using PANDAS data frames</font>\n",
        "\n",
        "Now you can start using your data and load three different NumPy arrays! One corresponding to the VBF $q\\bar q' \\to H q\\bar q'\\to ZZ^{(*)} q\\bar q'\\to 4\\mu q\\bar q' $ signal and the other two corresponding to the production of the Higgs boson via the strong interaction (in jargon, QCD) background processes  $gg\\to H\\to ZZ^{(*)}\\to4\\mu$ and $q\\bar q'\\to H\\to ZZ^{*}\\to4\\mu$ that will be used as a merged background. <br> \n",
        "\n",
        "Moreover, you will look at the physical observables that you can use to train the ML algorithms. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4v5f0ZD1u-C"
      },
      "source": [
        "#import libraries \n",
        "\n",
        "import uproot\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import h5py\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import SGD, Adam, RMSprop, Adagrad, Adadelta\n",
        "from tensorflow.keras.layers import Input, Activation, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow import random as tf_random\n",
        "#from keras.utils import plot_model\n",
        "import random as python_random\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb3U6F_Off-R"
      },
      "source": [
        "# Fix random seed for reproducibility\n",
        "\n",
        "# The below is necessary for starting Numpy generated random numbers\n",
        "# in a well-defined initial state.\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "# The below is necessary for starting core Python generated random numbers\n",
        "# in a well-defined state.\n",
        "python_random.seed(seed)\n",
        "\n",
        "# The below set_seed() will make random number generation\n",
        "# in the TensorFlow backend have a well-defined initial state.\n",
        "# For further details, see: https://www.tensorflow.org/api_docs/python/tf/random/set_seed\n",
        "tf_random.set_seed(seed)\n",
        "\n",
        "treename = 'HZZ4LeptonsAnalysisReduced'\n",
        "filename = {}\n",
        "upfile = {}\n",
        "params = {}\n",
        "df = {}\n",
        "\n",
        "# Define what are the ROOT files we are interested in (for the two categories,\n",
        "# signal and background)\n",
        "\n",
        "filename['sig'] = 'VBF_HToZZTo4mu.root'\n",
        "filename['bkg_ggHtoZZto4mu'] = 'GluGluHToZZTo4mu.root'\n",
        "filename['bkg_ZZto4mu'] = 'ZZTo4mu.root'\n",
        "#filename['bkg_ttH_HToZZ_4mu.root']= 'ttH_HToZZ_4mu.root'\n",
        "#filename['sig'] = 'VBF_HToZZTo4e.root'\n",
        "#filename['bkg_ggHtoZZto4e'] = 'GluGluHToZZTo4e.root'\n",
        "#filename['bkg_ZZto4e'] = 'ZZTo4e.root'\n",
        "\n",
        "# Variables from Root Tree that must be copyed to PANDA dataframe (df)\n",
        "VARS = [ 'f_run', 'f_event', 'f_weight', \\\n",
        "        'f_massjj', 'f_deltajj', 'f_mass4l', 'f_Z1mass' , 'f_Z2mass', \\\n",
        "        'f_lept1_pt','f_lept1_eta','f_lept1_phi', \\\n",
        "        'f_lept2_pt','f_lept2_eta','f_lept2_phi', \\\n",
        "        'f_lept3_pt','f_lept3_eta','f_lept3_phi', \\\n",
        "        'f_lept4_pt','f_lept4_eta','f_lept4_phi', \\\n",
        "        'f_jet1_pt','f_jet1_eta','f_jet1_phi', \\\n",
        "        'f_jet2_pt','f_jet2_eta','f_jet2_phi' ]\n",
        "\n",
        "#checking the dimensions of the df , 26 variables\n",
        "NDIM = len(VARS)\n",
        "\n",
        "print(\"Number of kinematic variables imported from the ROOT files = %d\"% NDIM)\n",
        "\n",
        "upfile['sig'] = uproot.open(filename['sig'])\n",
        "upfile['bkg_ggHtoZZto4mu'] = uproot.open(filename['bkg_ggHtoZZto4mu'])\n",
        "upfile['bkg_ZZto4mu'] = uproot.open(filename['bkg_ZZto4mu'])\n",
        "#upfile['bkg_ttH_HToZZ_4mu.root'] = uproot.open(filename['bkg_ttH_HToZZ_4mu'])\n",
        "#upfile['sig'] = uproot.open(filename['sig'])]\n",
        "#upfile['bkg_ggHtoZZto4e'] = uproot.open(filename['bkg_ggHtoZZto4e'])\n",
        "#upfile['bkg_ZZto4e'] = uproot.open(filename['bkg_ZZto4e'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlHcr_Yf0K7t"
      },
      "source": [
        "Let's see what you have uploaded in your Colab notebook!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9KDNH2AgKD0"
      },
      "source": [
        "# Look at the signal and bkg events before applying physical requirement\n",
        "\n",
        "df['sig'] = pd.DataFrame(upfile['sig'][treename].arrays(VARS, library=\"np\"),columns=VARS)\n",
        "print(df['sig'].shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQjGWHti6MCE"
      },
      "source": [
        "**Comment**: We have 24867 rows, i.e. 24867 different events, and 26 columns (whose meaning will be explained later).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O82ZppYI6Cr3"
      },
      "source": [
        "#### <font color=\"#4196B4\">**TASK:**</font> print out the first rows of the signal data-set to have a look at them!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTzxpHXE1KdQ"
      },
      "source": [
        "# *****************************************************\n",
        "# * EDIT: Print the first rows of the signal data-set *\n",
        "# *****************************************************\n",
        "\n",
        "df['sig'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSlG8Rvc6WGh"
      },
      "source": [
        "* The first 2 columns contain information that is provided by experiments at the LHC that will not be used in the training of our Machine Learning algorithms, therefore we skip our explanation to the next columns.\n",
        "\n",
        "* The next variable is the `f_weights`. This corresponds to the probability of having that particular kind of physical process on the whole experiment. Indeed, it is a product of Branching Ratio (BR), geometrical acceptance and kinematic phase-space (generator level). It is very important for the training phase and you will use it later.\n",
        "\n",
        "* The variables `f_massjj`,`f_deltajj`,`f_mass4l`,`f_Z1mass`, and `f_Z2mass` are named **high-level features (event features)** since they contain overall information about the final-state particles (the mass of the two jets, their separation in space, the invariant mass of the four leptons, the masses of the two Z bosons). Note that the $m_{Z_{2}}$ mass is lighter w.r.t. the $m_{Z_{1}}$ one. Why is that? In the Higgs boson production (hypothesis of mass = 125 GeV) only one of the Z bosons is an **actual particle** that has the nominal mass of 91.18 GeV. The other one is a virtual (off-mass shell) particle. \n",
        "\n",
        "* The other columns represent the **low-level features (object kinematics observables)**, the basic measurements which are made by the detectors for the individual final state objects (in our case four charged leptons and jets) such as `f_lept1(2,3,4)_pt(phi,eta)` corresponding to their transverse momentum $pt$ and the spatial distribution of their tracks ($\\eta, \\phi $)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KI3p5GUI1Xzo"
      },
      "source": [
        "The same comments hold for the background data-sets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7_-kbsVjSNU"
      },
      "source": [
        "# Part of the code in \"#\" can be used in the second part of the exercise\n",
        "# for trying to use alternative data-sets for the training of our ML algorithms\n",
        "\n",
        "df['bkg_ggHtoZZto4mu'] = pd.DataFrame(upfile['bkg_ggHtoZZto4mu'][treename].arrays(VARS, library=\"np\"),columns=VARS)\n",
        "df['bkg_ggHtoZZto4mu'].head()\n",
        "#df['bkg_ggHtoZZto4e'] = pd.DataFrame(upfile['bkg_ggHtoZZto4e'][treename].arrays(VARS, library=\"np\"),columns=VARS)\n",
        "#df['bkg_ggHtoZZto4e'].head()\n",
        "#df['bkg_ZZto4e'] = pd.DataFrame(upfile['bkg_ZZto4e'][treename].arrays(VARS, library=\"np\"),columns=VARS)\n",
        "#df['bkg_ZZto4e'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMt4FZ8lV2ig"
      },
      "source": [
        "df['bkg_ZZto4mu'] = pd.DataFrame(upfile['bkg_ZZto4mu'][treename].arrays(VARS, library=\"np\"),columns=VARS)\n",
        "df['bkg_ZZto4mu'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0L75xdz_kSQI"
      },
      "source": [
        "# Let's merge our background processes together!\n",
        "df['bkg'] = pd.concat([df['bkg_ZZto4mu'],df['bkg_ggHtoZZto4mu']])\n",
        "# Let's shuffle them!\n",
        "df['bkg']= shuffle(df['bkg'])\n",
        "# Let's see its shape!\n",
        "print(df['bkg'].shape)\n",
        "#print(len(df['bkg_ZZto4mu']))\n",
        "#print(len(df['bkg_ggHtoZZto4mu']))\n",
        "#print(len(df['bkg_ggHtoZZto4e']))\n",
        "#print(len(df['bkg_ZZto4e']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5te1sKbI1-fk"
      },
      "source": [
        "Note that the background data-sets seem to have a very large number of events! Is that true? \n",
        "\n",
        "Let's make physical selection requirements!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4mNSqF8jbUM"
      },
      "source": [
        "# Remove undefined variable entries VARS[i] <= -999\n",
        "\n",
        "for i in range(NDIM):\n",
        "    df['sig'] = df['sig'][(df['sig'][VARS[i]] > -999)]\n",
        "    df['bkg']= df['bkg'][(df['bkg'][VARS[i]] > -999)]\n",
        "\n",
        "# Add the columnisSignal to the dataframe containing the truth information\n",
        "# i.e. it tells if that particular event is signal (isSignal=1) or background (isSignal=0)\n",
        "\n",
        "df['sig']['isSignal'] = np.ones(len(df['sig'])) \n",
        "df['bkg']['isSignal'] = np.zeros(len(df['bkg'])) \n",
        "print(\"Number of Signal events = %d \" %len(df['sig']['isSignal']))\n",
        "print(\"Number of Background events = %d \" %len(df['bkg']['isSignal']))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-yTxQ6qxigT"
      },
      "source": [
        "#### <font color=\"#4196B4\">**TASK**</font>: show that the variable `isSignal` is correctly assigned to the VBF signal events and the background ones!</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uc8xS-Ofk8oh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a929811-5d51-4706-aa3e-6c60ba0837d3"
      },
      "source": [
        "# ********************************************************************************************\n",
        "# * EDIT: show that the variable 'isSignal' is correctly assigned to the VBF signal events   *\n",
        "# ********************************************************************************************\n",
        "print(df['sig']['isSignal'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        1.0\n",
            "1        1.0\n",
            "2        1.0\n",
            "3        1.0\n",
            "4        1.0\n",
            "        ... \n",
            "24858    1.0\n",
            "24859    1.0\n",
            "24860    1.0\n",
            "24861    1.0\n",
            "24862    1.0\n",
            "Name: isSignal, Length: 14260, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXSeOb9Al_lO"
      },
      "source": [
        "# Showing that the variable isSignal is correctly assigned for bkg events\n",
        "# Some events are missing because of the selection. So we do not have in total 134682 \n",
        "# background events anymore!\n",
        "print(df['bkg']['isSignal'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJLljCP_3eCn"
      },
      "source": [
        "Let's see in which way we have to use the `f_weight`: variable!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOtm71Nr1_xE"
      },
      "source": [
        "# Renormalizes the events weights to give unit sum in the signal and background dataframes\n",
        "# This is necessary for the ML algorithms to learn signal and background \n",
        "# in the same proportion,independently of number of events \n",
        "# and absolute weights of events in each sample of events!\n",
        "# The relative contributions of each background process is retained - so the classifier\n",
        "# learns to focus more on the importance backgrounds, and the background matches the data\n",
        "# shape - but overall signal and background have equal importance (the classifier\n",
        "# learns to identify signal and background equally well).\n",
        "# In the pandas technical vocabolary axis=0 stands for columns, axis=1 for rows.\n",
        "\n",
        "df['sig']['f_weight']=df['sig']['f_weight']/df['sig']['f_weight'].sum(axis=0)\n",
        "df['bkg']['f_weight']=df['bkg']['f_weight']/df['bkg']['f_weight'].sum(axis=0)\n",
        "\n",
        "# Note: Number of events remain unchanged after this \"normalization procedure\"\n",
        "print(\"Number SIG events=\", len(df['sig']['f_weight']))\n",
        "print(\"Number BKG events=\", len(df['bkg']['f_weight']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfImX2nQGjJa"
      },
      "source": [
        "Let's merge our signal and background events!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtUVRp1VDw_z"
      },
      "source": [
        "# Concatenate the signal and background dfs in a single data frame \n",
        "df_all = pd.concat([df['sig'],df['bkg']])\n",
        "\n",
        "# Random shuffles the data-set to mix signal and background events \n",
        "# before the splitting between train and test data-sets\n",
        "df_all = shuffle(df_all)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdWAMFRZ4mUe"
      },
      "source": [
        "# <font color=\"#002A48\">Preparing input features for the ML algorithms</font>\n",
        "\n",
        "We have our data-sets ready to train our ML algorithm! Before doing that, we have to decide which input variables have to be passed to the algorithm to let the model distinguish between signal and background events.\n",
        "\n",
        "We can use:\n",
        "\n",
        "1.   The *five high-level input variables*\n",
        "`f_massjj`,`f_deltajj`,`f_mass4l`,`f_Z1mass`, and `f_Z2mass` . \n",
        "2.   The 18 kinematic variables characterize the four-lepton + two jest final states objects. \n",
        "\n",
        "To make the best choice, we can look at the two sets of correlation plots - the so-called **scatter plots** using the *seaborn library* - among the features available and see which set captures better the differences between signal and background events.\n",
        "\n",
        "**Note:** this operation is quite long for both the sets since we are dealing with quite a lots of events. Skip the following two code cells and trust us in using the high level features for building your ML models! Indeed, we will obtain better  discriminators' performance using high-level features. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anE42oYCJkay"
      },
      "source": [
        "# It will take a while (5 minutes), you can skip it as said before.\n",
        "# We leave you the output of this code cell using a .png format\n",
        "\n",
        " #VAR = [ 'f_massjj', 'f_deltajj', 'f_mass4l', 'f_Z1mass' , 'f_Z2mass', 'isSignal']\n",
        " #sns.pairplot( data=df_all.filter(VAR), hue='isSignal' , kind='scatter', diag_kind='auto' );\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDHeUXL4MY8L"
      },
      "source": [
        "\n",
        "<img src=\"https://raw.githubusercontent.com/bdanzi/Higgs_exercise/main/Schermata%202021-04-20%20alle%2009.30.43.png\" style=\"display: block; margin: auto;\" width=\"800\" /><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XUL6q-ZzRCz"
      },
      "source": [
        "#### <font color=\"#4196B4\">**TASK:**</font> return to this part of the exercise and try to produce scatter-plots by using some low level features. Which features would be useful to discriminate signal vs background?\n",
        "\n",
        "#### **HINT**: avoid MC truth! Is it clear why?\n",
        "#### <font color=\"#4196B4\">**ANSWER:**</font> The ϕ distributions are just flat ones and I would expect no distiction between signal and background.In general try and use as many as possible: even if you do not really see the use of them, there can be hidden correlations ML can see!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jzpv8JYkKIfI"
      },
      "source": [
        "# ******************************************************************\n",
        "# * EDIT:It will take a while (1 hour). Skip it!                   *\n",
        "# * We leave you the output of this code cell using a .png format. *\n",
        "# ******************************************************************\n",
        "\n",
        "\n",
        "# NN_VARS_scatter = ['f_lept1_pt','f_lept1_eta','f_lept1_phi', \\\n",
        "#           'f_lept2_pt','f_lept2_eta','f_lept2_phi', \\\n",
        "#           'f_lept3_pt','f_lept3_eta','f_lept3_phi', \\\n",
        "#           'f_lept4_pt','f_lept4_eta','f_lept4_phi', \\\n",
        "#           'f_jet1_pt','f_jet1_eta','f_jet1_phi', \\\n",
        "#           'f_jet2_pt','f_jet2_eta','f_jet2_phi', 'isSignal']\n",
        "# sns.pairplot( data=df_all.filter(NN_VARS_scatter), hue='isSignal' , kind='scatter', diag_kind='auto' );\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtPXccgCPqrC"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/bdanzi/Higgs_exercise/main/Schermata%202021-04-20%20alle%2010.35.01.png\" style=\"display: block; margin: auto;\" width=\"800\" /><br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsUoT7t7MZdy"
      },
      "source": [
        "# Filter dataframe leaving just the Neural Network input variables  \n",
        "\n",
        "NN_VARS= ['f_massjj', 'f_deltajj', 'f_mass4l', 'f_Z1mass' , 'f_Z2mass']\n",
        "# NN_VARS = [ 'f_lept1_pt','f_lept1_eta','f_lept1_phi', \\\n",
        "#          'f_lept2_pt','f_lept2_eta','f_lept2_phi', \\\n",
        "#          'f_lept3_pt','f_lept3_eta','f_lept3_phi', \\\n",
        "#          'f_lept4_pt','f_lept4_eta','f_lept4_phi', \\\n",
        "#          'f_jet1_pt','f_jet1_eta','f_jet1_phi', \\\n",
        "#          'f_jet2_pt','f_jet2_eta','f_jet2_phi']\n",
        "         \n",
        "df_input  = df_all.filter(NN_VARS)\n",
        "df_target = df_all.filter(['isSignal']) # flag\n",
        "df_weights = df_all.filter(['f_weight']) \n",
        "# the weights are also important to be given as input to the training\n",
        "\n",
        "# Transform dataframes to numpy arrays of float32 \n",
        "# (X->NN input , Y->NN target output , W-> event weights)\n",
        "\n",
        "NINPUT=len(NN_VARS)\n",
        "print(\"Number NN input variables=\",NINPUT)\n",
        "print(\"NN input variables=\",NN_VARS)\n",
        "X  = np.asarray( df_input.values ).astype(np.float32)\n",
        "Y  = np.asarray( df_target.values ).astype(np.float32)\n",
        "W  = np.asarray( df_weights.values ).astype(np.float32)\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "print(W.shape)\n",
        "print('\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba0rCyp2aG_v"
      },
      "source": [
        "# <font color=\"#002A48\">Dividing the data into testing and training data-set</font>\n",
        "\n",
        "You can split now the data-sets into two parts (one for the training and validation steps and one for testing phase). \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeFsncoBd80P"
      },
      "source": [
        "#### <font color=\"#4196B4\">**TASK**:</font> are you able to do that by using the [scikit-learn](https://scikit-learn.org) library?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmCQlr9m3too"
      },
      "source": [
        "# ******************************************************************\n",
        "# * EDIT: Classical way to proceed, using a scikit-learn algorithm:*\n",
        "# ******************************************************************\n",
        "\n",
        "# X_train_val, X_test, Y_train_val , Y_test , W_train_val , W_test = \n",
        "# train_test_split(X, Y, W , test_size=0.2,shuffle=None,stratify=None )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmKBE326_C3w"
      },
      "source": [
        "# Alternative way, the one that we chose in order to study the model's performance \n",
        "# with ease (with an analogous procedure used by TMVA in the ROOT framework)\n",
        "# to keep information about the flag isSignal in both training and test steps. \n",
        "\n",
        "size= int(len(X[:,0]))\n",
        "test_size = int(0.2*len(X[:,0]))\n",
        "print('X (features) before splitting')\n",
        "print('\\n')\n",
        "print(X.shape)\n",
        "print('X (features) splitting between test and training')\n",
        "X_test= X[0:test_size+1,:]\n",
        "\n",
        "print('Test:')\n",
        "print(X_test.shape)\n",
        "X_train_val= X[test_size+1:len(X[:,0]),:]\n",
        "print('Training:')\n",
        "print(X_train_val.shape)\n",
        "print('\\n')\n",
        "\n",
        "print('Y (target) before splitting')\n",
        "print('\\n')\n",
        "print(Y.shape)\n",
        "print('Y (target) splitting between test and training ')\n",
        "Y_test= Y[0:test_size+1,:]\n",
        "print('Test:')\n",
        "print(Y_test.shape)\n",
        "Y_train_val= Y[test_size+1:len(Y[:,0]),:]\n",
        "print('Training:')\n",
        "print(Y_train_val.shape)\n",
        "print('\\n')\n",
        "\n",
        "print('W (weights) before splitting')\n",
        "print('\\n')\n",
        "print(W.shape)\n",
        "print('W (weights) splitting between test and training ')\n",
        "W_test= W[0:test_size+1,:]\n",
        "print('Test:')\n",
        "print(W_test.shape)\n",
        "W_train_val= W[test_size+1:len(W[:,0]),:]\n",
        "print('Training:')\n",
        "print(W_train_val.shape)\n",
        "print('\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ovQCvjM3z4y"
      },
      "source": [
        "#### <font color=\"#4196B4\">**TASK**:</font> have a look to the parameter setting *test_size*. Why did we choose that fraction of events to be used for the testing phase? \n",
        "#### <font color=\"#4196B4\">**ANSWER**:</font> Generally speaking, with more training data, the model will learn the underlying distribution of the real data better. Since a larger training set in your case improves the performance of on training and test set, you should get more data if you can. The performance on the test set might not be that reliable if your test set is small. In other words, your performance might be different if you change to another test set. That is one of the reasons that you perform cross validation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZCRq9TnbQfX"
      },
      "source": [
        "# <font color=\"#002A48\">Description of the Artificial Neural Network (ANN) model and KERAS API </font>\n",
        "In this section you will find the following subsections:\n",
        "* **Introduction to the Neural Network algorithm**<br>\n",
        "If you have the knowledge about ANN you may skip it.\n",
        "* **Usage of Keras API: basic concepts**<br>\n",
        "Here you find concepts that are useful for the ANN implementation using KERAS API (callfunctions, metrics etc.).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp1ZKtMW7UI2"
      },
      "source": [
        "There are three ways to create Keras models:\n",
        "\n",
        "* **The Sequential model**, which is very straightforward (a simple list of \n",
        "layers), but is limited to single-input, single-output stacks of layers (as the name gives away).\n",
        "* **The Functional API**, which is an easy-to-use, fully-featured API that supports arbitrary model architectures. For most people and most use cases, this is what you should be using. This is the Keras \"industry strength\" model. We will use it.\n",
        "* **Model subclassing**,where you implement everything from scratch on your own. You have to use this one, if you have complex, out-of-the-box research use cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46H-vibwQ-oI"
      },
      "source": [
        "\n",
        "## <font color=\"#002A48\">Introduction to the Neural Network algorithm : basic THEORY concepts</font>\n",
        "\n",
        "\n",
        "\n",
        "A Neural Network (NN) is a biology-inspired analytical model, but not a bio-mimetic one. It is formed by a network of basic elements called *neurons* or *perceptrons* (see the picture below), which receive input, change their state according to the input and produce an output.\n",
        "\n",
        "\n",
        "### <font color=\"#002A48\">The neuron/perceptron concept</font>\n",
        "<img src=\"\n",
        "https://raw.githubusercontent.com/bdanzi/Higgs_exercise/main/Schermata%202021-04-10%20alle%2013.19.41.png\" style=\"display: block; margin: auto;\" width=\"300\" height=\"200\" align=\"right\" /><br>\n",
        "The perceptron, while it has a simple structure, has\n",
        "the ability to learn and solve very complex\n",
        "problems.\n",
        "* It takes the inputs which feed into the\n",
        "perceptrons, multiplies them by their\n",
        "weights, and computes the sum. In the first iteration the weights are set randomly.\n",
        "* It adds the number one, multiplied by a “bias\n",
        "weight”.\n",
        "* It feeds the sum through the activation\n",
        "function in a simple perceptron system, the\n",
        "activation function is a step function.\n",
        "* The result of the step function is the neuron output.\n",
        "\n",
        "### <font color=\"#002A48\">Neural Network Topologies</font>\n",
        "\n",
        "A Neural Networks (NN) can be classified according to the type of\n",
        "neuron interconnections and the flow of information.\n",
        "<br>\n",
        "#### <font color=\"#002A48\">Feed Forward Networks</font>\n",
        "<img src=\"https://raw.githubusercontent.com/bdanzi/Higgs_exercise/main/Schermata%202021-04-10%20alle%2015.22.25.png\" style=\"display: block; margin: auto;\" width=\"150\" align=\"right\"/><br>\n",
        "A feedforward NN is a neural network where connections\n",
        "between the nodes do not form a cycle. In a feed-forward\n",
        "network information always moves one direction, from input to\n",
        "output, and it never goes backward. Feedforward NN can be\n",
        "viewed as mathematical models of a function $f: R^{N} \\to R^{M}$.\n",
        "<br>\n",
        "#### <font color=\"#002A48\">Recurrent Neural Network</font>\n",
        "<img src=\"https://raw.githubusercontent.com/bdanzi/Higgs_exercise/main/Schermata%202021-04-10%20alle%2015.23.19.png\" style=\"display: block; margin: auto;\" width=\"150\" align=\"left\"/><br>\n",
        "A Recurrent Neural Network (RNN) is the one that allows connections between nodes in the same layer, among each other or with previous layers.\n",
        "\n",
        "Unlike feedforward neural networks, RNNs can use their internal state (memory) to process sequential input data.\n",
        "<br>\n",
        "\n",
        "### <font color=\"#002A48\">Dense Layer</font>\n",
        "\n",
        "A Neural Network layer is called a dense layer to indicate that it’s fully\n",
        "connected.\n",
        "\n",
        "Information about the Neural Network architectures can be found here: [https://www.asimovinstitute.org/neural-network-zoo/](https://www.asimovinstitute.org/neural-network-zoo/)\n",
        "\n",
        "### <font color=\"#002A48\">Artificial Neural Network</font>\n",
        "\n",
        "The discriminant output is computed by combining the response of multiple nodes, each representing a single neuron cell. Nodes are arranged into layers.\n",
        "\n",
        "In an ANN the input variable values $ x_{1} ; x_{2} ;…; x_{p}$ are passed to a first input layer, whose output is passed as input to the next layer, and so on.\n",
        "\n",
        "\n",
        "The last output layer usually consists of a single node that provides the discriminant output. Intermediate layers between the input and the output\n",
        "layers are called hidden layers. Usually, if a Neural Network has more than one hidden layer is called **Deep Neural Network** and theoretically it is able to do the feature extraction by itself (it becomes a Deep Learning algorithm).\n",
        "\n",
        "Such a structure is also called **Feedforward Multilayer Perceptron** (MLP, see the picture).\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/bdanzi/Higgs_exercise/main/Schermata%202021-04-10%20alle%2014.27.57.png\" style=\"display: block; margin: auto;\" width=\"400\" align=\"left\"/><br>\n",
        "\n",
        "The output of the $k_{th}$ node of the $nth$ layers is\n",
        "computed as the weighted average of the input variables,\n",
        "with weights that are subject to optimization via\n",
        "training.\n",
        "\n",
        "The activation layer filters out the output , using an\n",
        "activation function. It converts the output of a given\n",
        "layer before passing on the information to\n",
        "consecutive layers. It can be a sigmoid, arctangent,\n",
        "step function (new functions as ReLu,SeLu) because\n",
        "they mimic a learning curve.\n",
        "\n",
        "Then a bias or threshold parameter $w_{0}$ is applied.\n",
        "This bias accounts for the random noise, in the sense\n",
        "that it measures how well the model fits the training\n",
        "set (i.e. how much the model is able to correctly predict\n",
        "the known outputs of the training examples.)\n",
        "The output of a given node is: $y^{(n)}_{k}(\\vec{x})=\\phi (w^{n}_{0}\\sum_{j=1}^{p^{(n)}}w^{(n)}_{kj}x_{j})$.\n",
        "\n",
        "### <font color=\"#002A48\">Supervised Learning: the loss function</font>\n",
        "In order to train the neural network, a further function is introduced in the model, the **loss (cost) function** that quantifies the error between the NN output $y(\\vec{x})$and the desired target output.The choice of the loss function is directly related to the activation function used in\n",
        "the output layer !\n",
        "\n",
        "If we have binary targets $t \\in\\{0,1\\}$ we use the **Cross Entropy Loss**: $L = -tlog[y(\\vec{x})] - (1-t)log(1-y(\\vec{x}))$.\n",
        "\n",
        "During training we optimize the loss function, i.e. reduce\n",
        "the error between actual and predicted values.\n",
        "Since we deal with a binary classification problem, the\n",
        "$y_{true}$ can take on just two values, $y_{true} =0$ (for hypothesis\n",
        "$H_{0}$) and $y_{true} = 1$ (for hypothesis $H_{1}$).\n",
        "\n",
        "A popular algorithm to optimize the weights consists of iteratively modifying the weights after each training observation or after a bunch of training observations by doing a minimization of the loss function.\n",
        "\n",
        "The minimization usually proceeds via the so-called **Stochastic Gradient Descent** (SGD) which modifies weight at each iteration according to the following formula: $w^{(n)}_{ij} \\to w^{(n)}_{ij} - \\eta \\frac{\\partial L(w)}{\\partial w^{(n)}_{ij}}$ .\n",
        "\n",
        "Other more complex optimization algorithms are available in KERAS API. \n",
        "\n",
        "More info: [https://keras.io/api/optimizers/](https://keras.io/api/optimizers/).\n",
        "\n",
        "### <font color=\"#002A48\">Metrics</font>\n",
        "A metric is a function that is used to judge the performance of your model.\n",
        "\n",
        "Metric functions are similar to loss functions, except that the results from evaluating a metric are not used during the training of the model. Note that you may use any loss function as a metric.\n",
        "\n",
        "### <font color=\"#002A48\">Other parameters of a Neural Network</font>\n",
        "\n",
        "Hyperparameters are the variables that determine the network structure\n",
        "and how the network is trained.\n",
        "Hyperparameters are set before training. A list of the main parameters is below:\n",
        "* `Number of Hidden Layers and units`: the hidden layers are the layers between the input layer and the output layer. Many hidden units within a layer can increase accuracy. A smaller number of units may cause underfitting.\n",
        "* `Network Weight Initialization`: ideally, it may be better to use different weight initialization schemes according to the activation\n",
        "function used on each layer. Mostly uniform distribution is used.\n",
        "* `Activation functions`: they are used to introduce nonlinearity to models, which allows deep learning models to learn nonlinear prediction boundaries.\n",
        "* `Learning Rate`: it defines how quickly a network updates its parameters. A low learning rate slows down the learning process but converges smoothly. A larger learning rate speeds up the learning but may not converge. Usually a decaying Learning rate is preferred.\n",
        "* `Number of epochs`: in terms of artificial neural networks, an epoch refers to one cycle through the full training data-set. Usually, training a neural network takes more than a few epochs. An epoch is often mixed up with an iteration. Iterations is the number of batches or steps through partitioned packets of the training data, needed to complete one epoch. You must increase the number of epochs until the validation accuracy starts decreasing even when the training accuracy is increasing in order to avoid overfitting.\n",
        "* `Batch size`: a number of subsamples (events) given to the network after the update of the parameters. A good default for batch size might be 32. Also try 32, 64, 128, 256, and so on.\n",
        "* `Dropout`: regularization technique to avoid overfitting thus increasing the generalizing power. Generally, use a small dropout value of 10%-50% of neurons.Considering a too low value has minimal effect, while a too high one could result in a network under-learning.\n",
        "\n",
        "\n",
        "### <font color=\"#002A48\">Applications in High Energy Physics</font>\n",
        "Nowadays ANNs are used on a variety of tasks: image and speech recognition, translation,filtering, game playing, medical diagnosis, autonomous vehicles.\n",
        "There are also many applications in High Energy Physics: classification of signal and background events, particle tagging, simulation of event reconstruction...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFHqBLBD_iJv"
      },
      "source": [
        "## <font color=\"#002A48\">Usage of Keras API: basic THEORY concepts</font>\n",
        "\n",
        "### <font color=\"#002A48\">Keras layers API</font>\n",
        "Layers are the basic building blocks of neural networks in Keras. A layer consists of a tensor-in tensor-out computation function (the layer's call method) and some state, held in TensorFlow variables (the layer's weights).\n",
        "\n",
        "### <font color=\"#002A48\">Callbacks API</font>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/bdanzi/Higgs_exercise/main/Schermata%202021-04-10%20alle%2015.46.18.png\" style=\"display: block; margin: auto;\" width=\"400\" align=\"right\"/><br>\n",
        "\n",
        "A callback is an object that can perform actions at various stages of training (e.g. at the start or end of an epoch, before or after a single batch, etc).\n",
        "\n",
        "You can use callbacks in order to:\n",
        "\n",
        "* Write TensorBoard logs after every batch of training to monitor your metrics\n",
        "* Periodically save your model to disk\n",
        "* Do early stopping\n",
        "* Get a view on internal states and statistics of a model during training\n",
        "\n",
        "More info and examples about the most used that we will use: [ModelCheckPoint](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint), [EarlyStopping](https://keras.io/api/callbacks/early_stopping/), [LearningRateScheduler](https://keras.io/api/callbacks/learning_rate_scheduler/), [ReduceLROnPlateau](https://keras.io/api/callbacks/reduce_lr_on_plateau/).\n",
        "\n",
        "\n",
        "### <font color=\"#002A48\">Regularization layers : the dropout layer</font>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/bdanzi/Higgs_exercise/main/Schermata%202021-04-10%20alle%2015.46.29.png\" style=\"display: block; margin: auto;\" width=\"400\" align=\"right\"/><br>\n",
        "\n",
        "\n",
        "\n",
        "The Dropout layer randomly sets input units to 0 with a frequency of `rate` at each step during training time, which helps prevent **overtraining**. Inputs not set to 0 are scaled up by `1/(1-rate)` such that the sum over all inputs is unchanged.\n",
        "\n",
        "Note that the Dropout layer only applies when training is set to `True` such that no values are dropped during inference. When using `model.fit`, training will be appropriately set to `True` automatically, and in other contexts, you can set the flag explicitly to `True` when calling the layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcKx4V06bJob"
      },
      "source": [
        "# <font color=\"#002A48\">Artificial Neural Network implementation</font>\n",
        "\n",
        "We can now start to define the first architecture. The most simple approach is using fully connected layers (**`Dense`** layers in Keras/Tensorflow), with **`selu`** activation function and a **`sigmoid`** final layer, since we are affording a binary classification problem.\n",
        "\n",
        "We are using the `binary_crossentropy` loss function during training, a standard loss function for binary classification problems. \n",
        "We will optimize the model with the RMSprop algorithm and we will collect `accuracy` metrics while the model is trained.\n",
        "\n",
        "In order to avoid overfitting we use also Dropout layers and some callback functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zz82hpSROOqC"
      },
      "source": [
        "# Define Neural Network with 3 hidden layers ( #h1=10*NINPUT , #h2=2*NINPUT , #h3=NINPUT ) & Dropout layers\n",
        "\n",
        "input_layer  = Input(shape=(NINPUT,), name = 'input') \n",
        "hidden = Dense(NINPUT*10, name = 'hidden1', kernel_initializer='normal', activation='selu')(input_layer)\n",
        "hidden = Dropout(rate=0.1)(hidden)\n",
        "hidden = Dense(NINPUT*2 , name = 'hidden2', kernel_initializer='normal', activation='selu')(hidden)\n",
        "hidden = Dropout(rate=0.1)(hidden)\n",
        "hidden = Dense(NINPUT, name = 'hidden3', kernel_initializer='normal', activation='selu')(hidden)\n",
        "hidden = Dropout(rate=0.1)(hidden)\n",
        "output  = Dense(1 , name = 'output', kernel_initializer='normal', activation='sigmoid')(hidden)\n",
        "\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=input_layer, outputs=output)\n",
        "# Define the optimizer ( minimization algorithm )\n",
        "\n",
        "#optim = SGD(learning_rate=0.01,decay=1e-6)\n",
        "#optim = Adam(learning_rate=0.0001)\n",
        "#optim = Adagrad(learning_rate=0.0001 )\n",
        "#optim = Adadelta(learning_rate=0.0001 )\n",
        "optim = RMSprop(learning_rate = 1e-4) #default lr= 1e-3\n",
        "\n",
        "# Compile the model\n",
        "\n",
        "#model.compile(optimizer=optim, loss='mean_squared_error', metrics=['accuracy'], weighted_metrics=['accuracy'])\n",
        "model.compile( optimizer=optim, loss='binary_crossentropy', metrics=['accuracy'], weighted_metrics=['accuracy'])\n",
        "# NOTE: the accuracy (defined as the number of good matches between the predictions and the class labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDUhRSrP_8Ul"
      },
      "source": [
        "#### <font color=\"#4196B4\">**TASK**:</font> print the summary of the model's properties! Are you able to tell us which is the expression to retrieve the number of parameter per each DNN layer?\n",
        "#### <font color=\"#4196B4\">**ANSWER**:</font> For each DNN layer: $n_{param} = n_{input} \\times $ $n_{neurons}$ + $n_{neurons}$. The addition to $n_{neurons}$ is added due to the bias parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYidvCpUAAmi"
      },
      "source": [
        "# ****************************************************************\n",
        "# * EDIT: print the model summary by using a standard keras tool *\n",
        "# * which creates a table with layers and parameters.            *\n",
        "# ****************************************************************\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEbtYhRhoQHH"
      },
      "source": [
        "# Alternative way of plotting the model properties via graphical visualization\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "plot_model(model, show_shapes=True, show_layer_names=True, show_dtype=True,show_layer_activations=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVHdkx1URJKC"
      },
      "source": [
        "## Call functions implementation to monitor the chosen metrics\n",
        "\n",
        "#Stop training when a monitored metric has stopped improving\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor = 'val_loss',\n",
        "                                           mode='min',# quantity that has to be monitored(to be minimized in this case)\n",
        "                              patience = 50, # Number of epochs with no improvement after which training will be stopped.\n",
        "                              min_delta = 1e-7,\n",
        "                              restore_best_weights = True) # update the model with the best-seen weights\n",
        "\n",
        "#Reduce learning rate when a metric has stopped improving\n",
        "reduce_LR = keras.callbacks.ReduceLROnPlateau( monitor = 'val_loss',\n",
        "                                              mode='min',# quantity that has to be monitored\n",
        "                                              min_delta=1e-7,\n",
        "                                              factor = 0.1, # factor by which LR has to be reduced...\n",
        "                                              patience = 10, #...after waiting this number of epochs with no improvements \n",
        "                                              #on monitored quantity\n",
        "                                              min_lr= 0.00001 ) \n",
        "\n",
        "\n",
        "callback_list = [reduce_LR, early_stop]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi3WF3qhJGk9"
      },
      "source": [
        "You need to define the hyper-parameters of your ANN for the algorithm's training phase. Please test more!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1IQ3bysILPa"
      },
      "source": [
        "# Number of training epochs\n",
        "nepochs=200\n",
        "# Batch size\n",
        "batch=250\n",
        "# Train classifier (2 minutes more or less)\n",
        "history = model.fit(X_train_val[:,0:NINPUT], \n",
        "                    Y_train_val,\n",
        "                    epochs=nepochs, \n",
        "                    sample_weight=W_train_val,\n",
        "                    batch_size=batch,\n",
        "                    callbacks = callback_list, \n",
        "                    verbose=1, # switch to 1 for more verbosity \n",
        "                    validation_split=0.3 ) # fix the validation data-set size\n",
        "\n",
        "# You can save the best ANN model in:\n",
        "model_file = 'ANN_model.h5'\n",
        "model.save(model_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aza8j4-p1LJr"
      },
      "source": [
        "# <font color=\"#002A48\">Performance evaluation: THEORY concepts</font>\n",
        "\n",
        "In this section you will find the following subsections:\n",
        "* **ROC curve and Rates definitions**<br>\n",
        "* **Overfitting and test evaluation of an MVA model**<br>\n",
        "If you have the knowledge about these theoretical concepts you may skip it.\n",
        "* **Artificial Neural Network performance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63Gsrh2fLPww"
      },
      "source": [
        "## <font color=\"#002A48\"> ROC curve and rates definitions</font>\n",
        "\n",
        "There are many ways to evaluate the quality of the prediction of a model. In the ANN implementation, we evaluated the accuracy metric and the loss of the training and validation samples.\n",
        "\n",
        "Another largely used evaluation metric for binary classification tasks is the *Receiver Operating Characteristic* curve or **ROC curve**.\n",
        "\n",
        "First, we introduce the terms `positive` and `negative` referring to the classifier’s prediction, and the terms `true` and `false` referring to whether the network prediction corresponds to the observation (the \"truth\" level). In our Higgs boson binary classification exercise, we can think the `negative` outcome as the one labeling background (that, in the last sigmoid layer of our network, would mean a number close to 0), and the `positive` outcome as the one labeling signal (that, in the last sigmoid layer of our network, would mean a number close to 1 ). \n",
        "\n",
        "* **TP (true positive)**: the event is signal, the prediction is signal (*correct result*)\n",
        "* **FP (false positive)**: the event is background, but the prediction is signal (*unexpected result*)\n",
        "* **TN (true negative)**: the event is background, the prediction is background (*correct absence of signal*)\n",
        "* **FN (false negative)**: the event is signal, the prediction is background (*missing a true signal event*)\n",
        "\n",
        "Some additional definitions:\n",
        "\n",
        "* **TPR (true positive rate)**: how often the network predicts a positive outcome (*signal*), when the input is positive (*signal*):  $TPR = \\frac{TP}{TP+FN}$\n",
        "* **FPR (false positive rate)**: how often the network predicts a positive outcome (*signal*), when the input is negative (*background*) : $FPR = \\frac{FP}{FP+TN}$\n",
        "\n",
        "A good classifier should give a high TPR and a small FPR.\n",
        "\n",
        "Quoting wikipedia:\n",
        "\n",
        "\"A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.\n",
        "\n",
        "The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The true-positive rate is also known as sensitivity, probability of detection, or signal efficiency in high energy physics. The false-positive rate is also known as the probability of false alarm or fake rate in high energy physics.\"\n",
        "\n",
        "The ROC curve requires the true binary value (0 or 1, background or signal) and the probability estimates of the positive (signal) class.\n",
        "\n",
        "The **`roc_auc_score`** function computes the area under the receiver operating characteristic (ROC) curve, which is also denoted by AUC. By computing the area under the roc curve, the curve information is summarized in one number. \n",
        "\n",
        "For more information see: [https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve).\n",
        "\n",
        "\n",
        "The AUC is the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one. The higher the AUC, the better the performance of the classifier. If the AUC is 0.5, the classifier is uninformative, i.e., it will rank equally a positive or a negative observation.\n",
        "\n",
        "## <font color=\"#002A48\">Other metrics</font>\n",
        "\n",
        "\n",
        "The **precision/purity** is the ratio $\\frac{TP}{TP + FP} $where TP is the number of true positives and FP the number of false positives. \n",
        "The precision is intuitively the ability of the classifier not to label as positive a sample\n",
        "that is negative.\n",
        "\n",
        "\n",
        "The **recall/sensitivity/TPR/signal efficiency** is the ratio $\\frac{TP}{TP + FN}$ where TP is the number of\n",
        "true positives and FN the number of false negatives. The recall is\n",
        "intuitively the ability of the classifier to find all the positive samples.\n",
        "\n",
        "\n",
        "**Accuracy** is defined as the number of good matches between the predictions and the true labels:\n",
        "\n",
        "$\\text{Accuracy} =\\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}$.\n",
        "\n",
        "<p>For binary classification, accuracy can also be calculated in terms of positives and negatives as follows:</p>\n",
        "\n",
        "<div class=\"left-align\">\n",
        "$$\\text{Accuracy} = \\frac{TP+TN}{TP+TN+FP+FN}$$\n",
        "</div>\n",
        "\n",
        "\n",
        "You can always achieve high accuracy on skewed/unbalanced data-sets by predicting the most the same output (the most common one) for every input. Thus the another metric, **F1** can be used when there are more positive examples than negative examples. It is defined in terms of the precision and recall as **(2 * precision * recall) / (precision + recall)**. In our case, we will use a simplification of this metric that is the product **signal*efficiency**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mO73SyERzGd"
      },
      "source": [
        "#Let's import all the metrics that we need later on!\n",
        "from sklearn.metrics import ConfusionMatrixDisplay,confusion_matrix,accuracy_score , precision_score , recall_score , precision_recall_curve , roc_curve, auc , roc_auc_score \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUbiuU3q30rB"
      },
      "source": [
        "### <font color=\"#002A48\">Overfitting and test evaluation of an MVA model</font>\n",
        "<img src=\"https://raw.githubusercontent.com/bdanzi/Higgs_exercise/main/Schermata%202021-04-10%20alle%2015.51.23.png\" style=\"display: block; margin: auto;\" width=\"400\" align=\"right\"/><br><br><br>\n",
        "The loss function and the accuracy metrics give us a measure of the **overtraining (overfitting)** of the ML algorithm. Over-fitting happens when an ML algorithm learns to recognize a pattern that is primarily based on the training (validation) sample and that is nonexistent when looking at the testing (training) set (see the plot on the right side to understand what we would expect when overfitting happens).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bv7I2NCJRisS"
      },
      "source": [
        "## <font color=\"#002A48\">Artificial Neural Network performance</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0j2gni9O4AsQ"
      },
      "source": [
        "Let's see how our ANN model training went by making some plots! In order to do so, look into the [history](https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/) object. Plot them as a function of the epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjEOLLIvRhrg"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# plot the loss fuction vs epoch during the training phase\n",
        "# the plot of the loss function on the validation set is also computed and plotted\n",
        "plt.rcParams['figure.figsize'] = (13,6)\n",
        "plt.plot(history.history['loss'], label='loss train',color='green')\n",
        "plt.plot(history.history['val_loss'], label='loss validation',color='magenta')\n",
        "plt.title(\"Loss\", fontsize=12,fontweight='bold', color='r')\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR1hcbPh8K8A"
      },
      "source": [
        "#### <font color=\"#4196B4\">**TASK**:</font> Why does the validation loss decrease more than the training loss? \n",
        "\n",
        "#### <font color=\"#4196B4\">**ANSWER**:</font> we used several call functions to train our ANN which helped us to get better results on the validation data-set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TWaCCR1R4l-"
      },
      "source": [
        "# Plot accuracy metrics vs epoch during the training\n",
        "# for the proper training data-set and the validation one\n",
        "plt.rcParams['figure.figsize'] = (13,6)\n",
        "plt.plot(history.history['accuracy'], label='accuracy train',color='green')\n",
        "plt.plot(history.history['val_accuracy'], label='accuracy validation',color='magenta')\n",
        "plt.title(\"Accuracy\",fontsize=12,fontweight='bold', color='r')\n",
        "plt.ylim([0, 1.0])\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw5l8vrV4XqY"
      },
      "source": [
        "Now let's use our **test data-set** in order to see which are the performance of our model on a never-seen-before data-set and make comparison with what we obtained with the **training data-set**!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LHwIgRZSJZK"
      },
      "source": [
        "# Get ANN model label predictions and performance metrics curves, after having trained the model\n",
        "y_true = Y_test[:,0]\n",
        "y_true_train = Y_train_val[:,0]\n",
        "w_test = W_test[:,0]\n",
        "w_train = W_train_val[:,0]\n",
        "Y_prediction = model.predict(X_test[:,0:NINPUT])\n",
        "\n",
        "# Get precision, recall, \n",
        "p, r, t = precision_recall_curve( y_true= Y_test, probas_pred= Y_prediction , \n",
        "                                 sample_weight=w_test )\n",
        "# Get False Positive Rate (FPR) True Positive Rate (TPR) , Thresholds/Cut on the ANN's score\n",
        "fpr, tpr, thresholds = roc_curve( y_true= Y_test,  y_score= Y_prediction, sample_weight=w_test )\n",
        "\n",
        "\n",
        "Y_prediction_train = model.predict(X_train_val[:,0:NINPUT])\n",
        "p_train, r_train, t_train = precision_recall_curve( Y_train_val, Y_prediction_train ,\n",
        "                                                   sample_weight=w_train )\n",
        "fpr_train, tpr_train, thresholds_train = roc_curve(Y_train_val, Y_prediction_train,\n",
        "                                                   sample_weight=w_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8Mh3CiaNw9O"
      },
      "source": [
        "#### <font color=\"#4196B4\">**TASK**:</font> plot the ANN ROC curve on the test and training data-sets to check the ANN performance!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZlFT6gkTKcl"
      },
      "source": [
        "# *********************************************************************\n",
        "# EDIT: Plotting the ANN ROC curve on the test and training data-sets *\n",
        "# *********************************************************************\n",
        "\n",
        "roc_auc = auc(fpr, tpr)\n",
        "roc_auc_train = auc(fpr_train,tpr_train)\n",
        "plt.rcParams['figure.figsize'] = (10,5)\n",
        "plt.plot(fpr_train, tpr_train,  color='green', label='NN AUC_train = %.4f' % (roc_auc_train))\n",
        "plt.plot(fpr, tpr, color='magenta', label='NN AUC_test = %.4f' % (roc_auc))\n",
        "# Comparison with the random chance curve\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='k', label='random chance')\n",
        "plt.xlim([0, 1.0]) #fpr\n",
        "plt.ylim([0, 1.0]) #tpr\n",
        "plt.xlabel('False Positive Rate(FPR)')\n",
        "plt.ylabel('True Positive Rate(TPR)')\n",
        "plt.title('Receiver Operating Characteristic (ROC)',fontsize=12,fontweight='bold', color='r')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LouX6W-4UPfs"
      },
      "source": [
        "# Plot of the metrics Efficiency x Purity -- ANN \n",
        "# Looking at this curve we will choose a threshold on the ANN score\n",
        "# for distinguishing between signal and background events\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (10,5)\n",
        "plt.plot(t,p[:-1]*r[:-1],label='purity*efficiency_test')\n",
        "plt.plot(t_train,p_train[:-1]*r_train[:-1],label='purity*efficiency_train')\n",
        "plt.xlabel('Threshold/cut on the ANN score')\n",
        "plt.ylabel('Purity*efficiency')\n",
        "plt.title('Purity*efficiency vs Threshold on the ANN score',fontsize=12,fontweight='bold', color='r')\n",
        "#plt.tick_params(width=2, grid_alpha=0.5)\n",
        "plt.legend(markerscale=50)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rvs_ANmfQFl"
      },
      "source": [
        "\n",
        "# Print metrics imposing a threshold for the test sample. In this way the student\n",
        "# can use later the model's score to discriminate signal and bkg events for a fixing\n",
        "# score\n",
        "\n",
        "cut_dnn=0.6\n",
        "\n",
        "# Transform predictions into a array of entries 0,1 depending if prediction is beyond the\n",
        "# chosen threshold\n",
        "y_pred = Y_prediction[:,0] \n",
        "y_pred[y_pred >= cut_dnn]= 1 #classify them as signal\n",
        "y_pred[y_pred < cut_dnn]= 0 #classify them as background\n",
        "y_pred_train = Y_prediction_train[:,0]\n",
        "y_pred_train[y_pred_train>=cut_dnn]=1\n",
        "y_pred_train[y_pred_train<cut_dnn]=0\n",
        "\n",
        "print(\"y_true.shape\",y_true.shape)\n",
        "print(\"y_pred.shape\",y_pred.shape)\n",
        "print(\"w_test.shape\",w_test.shape)\n",
        "print(\"Y_prediction\",Y_prediction)\n",
        "print(\"y_pred\",y_pred)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgwuQ6J0SsYD"
      },
      "source": [
        "# Other Metrics values for the ANN algorithm having fixed an ANN score threshold\n",
        "accuracy  = accuracy_score(y_true, y_pred, sample_weight=w_test)\n",
        "precision = precision_score(y_true, y_pred, sample_weight=w_test)\n",
        "recall    = recall_score(y_true, y_pred, sample_weight=w_test)\n",
        "f1 = 2*precision*recall/(precision+recall)\n",
        "cm = confusion_matrix( y_true, y_pred, sample_weight=w_test)\n",
        "print('Cut/Threshold on the ANN output : %.4f' % cut_dnn)\n",
        "print('ANN Test Accuracy: %.4f' % accuracy)\n",
        "print('ANN Test Precision/Purity: %.4f' % precision)\n",
        "print('ANN Test Sensitivity/Recall/TPR/Signal Efficiency: %.4f' % recall)\n",
        "print('ANN Test F1: %.4f' %f1)\n",
        "print('')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tk1gtIqWeQz"
      },
      "source": [
        "The information from the evaluation metrics can be summarised in a so-called **confusion matrix** whose elements, from the top-left side, represent TN, FP, FN and TP rates. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRG-dk_rWamW"
      },
      "source": [
        "print('Cut/Threshold on the ANN output : %.4f \\n' % cut_dnn )\n",
        "print('Confusion matrix ANN\\n')\n",
        "\n",
        "plt.style.use('default') # It's ugly otherwise\n",
        "plt.figure(figsize=(10,10) )\n",
        "\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "mat_train = confusion_matrix(y_true_train, y_pred_train,sample_weight=w_train,normalize='all')\n",
        "sns.heatmap(mat_train.T, square=True, annot=True, fmt='.4f', cbar=True,linewidths=1,linecolor='black' )\n",
        "plt.xlabel('True label')\n",
        "plt.ylabel('Predicted label');\n",
        "plt.title('Normalized Confusion Matrix for the Train data-set - Artificial Neural Network ')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "mat_test = confusion_matrix(y_true, y_pred ,sample_weight=w_test,normalize='all' )\n",
        "sns.heatmap(mat_test.T, square=True, annot=True, fmt='.4f', cbar=True,linewidths=1,linecolor='black')\n",
        "plt.xlabel('True label')\n",
        "plt.ylabel('Predicted label');\n",
        "plt.title('Normalized Confusion Matrix for the Test data-set - Artificial Neural Network ')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzsDvvE7XYRY"
      },
      "source": [
        "An alternative way to check overfitting, and choosing correctly a threshold for selecting signal events, is plotting signal and background ANN predictions for the training and test data-sets. If the distributions are quite similar it means that the algorithm learned how to generalize! <br> For measuring quantitatively the overfitting one can perform a [**Kolmogorov-Smirnov**](https://en.wikipedia.org/wiki/Kolmogorov–Smirnov_test) test that we will not implement here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg3V-bobT5pk"
      },
      "source": [
        "# Let's get signal and background events for both test and training data-set!\n",
        "\n",
        "df_sig  = df['sig'].filter(NN_VARS)\n",
        "df_bkg  = df['bkg'].filter(NN_VARS)\n",
        "\n",
        "X_sig  = np.asarray( df_sig.values ).astype(np.float32)\n",
        "X_bkg  = np.asarray( df_bkg.values ).astype(np.float32)\n",
        "\n",
        "df_test = df_all.iloc[0:test_size+1]\n",
        "df_train = df_all.iloc[test_size+1:size]\n",
        "\n",
        "df_test_sig = df_test[(df_test['isSignal']>=1)].filter(NN_VARS)\n",
        "df_test_bkg = df_test[(df_test['isSignal']<1)].filter(NN_VARS)\n",
        "\n",
        "df_train_sig = df_train[(df_train['isSignal']>=1)].filter(NN_VARS)\n",
        "df_train_bkg = df_train[(df_train['isSignal']<1)].filter(NN_VARS)\n",
        "\n",
        "X_test_sig  = np.asarray( df_test_sig.values ).astype(np.float32)\n",
        "X_test_bkg  = np.asarray( df_test_bkg.values ).astype(np.float32)\n",
        "X_train_sig  = np.asarray( df_train_sig.values ).astype(np.float32)\n",
        "X_train_bkg  = np.asarray( df_train_bkg.values ).astype(np.float32)\n",
        "\n",
        "print('Test data-set shape:')\n",
        "print(df_test.shape)\n",
        "print('Test data-set signal shape:')\n",
        "print(df_test_sig.shape)\n",
        "print('Test data-set background shape:')\n",
        "print(df_test_bkg.shape)\n",
        "print('Training data-set shape' )\n",
        "print(df_train.shape)\n",
        "print('Training signal data-set shape' )\n",
        "print(df_train_sig.shape)\n",
        "print('Training background data-set shape' )\n",
        "print(df_train_bkg.shape)\n",
        "\n",
        "Y_test_sig = model.predict(X_test_sig) #flag predicted on all signal events\n",
        "Y_test_bkg = model.predict(X_test_bkg) #flag predicted on all background events\n",
        "Y_train_sig = model.predict(X_train_sig)\n",
        "Y_train_bkg = model.predict(X_train_bkg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1_LBRJyW-y9"
      },
      "source": [
        "df_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRBleKHTSVEJ"
      },
      "source": [
        "df_all.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhVXAb-1OxIQ"
      },
      "source": [
        "#### <font color=\"#4196B4\">**TASK**:</font> Plot the normalized distribution of the ANN score for the whole data-set!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Km-C02SGT7Hw"
      },
      "source": [
        "# *************************************************************************\n",
        "# * EDIT: Normalized Distribution of the ANN score for the whole data-set *\n",
        "# *************************************************************************\n",
        "X = np.linspace(0.0, 1.0, 100) #100 numbers between 0 and 1\n",
        "plt.rcParams['figure.figsize'] = (10,5)\n",
        "hist_test_sig = plt.hist(Y_test_sig, bins=X, label='test_sig',histtype='step',log=True,density=1)\n",
        "hist_test_bkg = plt.hist(Y_test_bkg, bins=X, label='test_bkg',histtype='step',log=True,density=1)\n",
        "hist_train_sig = plt.hist(Y_train_sig, bins=X, label='train_sig',histtype='step',log=True,density=1)\n",
        "hist_train_bkg = plt.hist(Y_train_bkg, bins=X, label='train_bkg',histtype='step',log=True,density=1)\n",
        "plt.xlabel('ANN score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend( loc='upper right',prop={'size': 8} )\n",
        "plt.title('ANN score normalized distribution on the whole data-set',fontsize=12,fontweight='bold', color='r')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrCfh4r6Q2MC"
      },
      "source": [
        "# <font color=\"#002A48\">Plot physics observables</font>\n",
        "\n",
        "We can easily plot the quantities (e.g. $m_{jj}$, $\\eta_{j}$, $m_{4l}$, $m_{Z_{1}}$,$m_{Z_{2}}$) for those events in the data-sets which have the ANN output scores greater than the chosen decision threshold in order to show that the ML discriminators did learn from physics observables!\n",
        "<br>\n",
        "The subsections of this notebook part are:\n",
        "* **Artificial Neural Network rates fixing an ANN score threshold from data frame**\n",
        "* **Plot some physical quantities after that the event selection is applied**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PFp6y0wXDq1"
      },
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define a data frame for low level features\n",
        "data = df_all.filter(NN_VARS)\n",
        "X_all   = np.asarray( data.values ).astype(np.float32)\n",
        "#Use it for evaluating the NN output score for the entire data-set\n",
        "Y_all = model.predict(X_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwI0rQUYXLfF"
      },
      "source": [
        "## <font color=\"#002A48\">Artificial Neural Network rates fixing an ANN score threshold from data frame</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLccfT-KX_1N"
      },
      "source": [
        "Let's fix a cut (looking at the performance of our models in terms of the previous **purity*efficiency** metrics plot) on our test statistic (ANN score) to select mostly VBF Higgs production signal events! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjKid9InXu-P"
      },
      "source": [
        "# Add the ANN prediction array 'NNoutput'column to the complete dataframe in order \n",
        "# keep the information about the ML algorithm prediction for every event in the whole data-set\n",
        "df_all['NNoutput'] = Y_all\n",
        "# Selects events with NNoutput > cut\n",
        "cut_dnn = 0.6\n",
        "df_sel   = df_all[(df_all['NNoutput'] >= cut_dnn)]\n",
        "df_TP    = df_all[(df_all['NNoutput'] >= cut_dnn) & (df_all['isSignal'] == 1)]\n",
        "df_unsel = df_all[(df_all['NNoutput'] < cut_dnn)]\n",
        "df_TN    = df_all[(df_all['NNoutput'] < cut_dnn) & (df_all['isSignal'] == 0)]\n",
        "\n",
        "TP = len(df_TP)\n",
        "FP = len(df_sel) - TP\n",
        "TN = len(df_TN)\n",
        "FN = len(df_unsel) - TN\n",
        "\n",
        "truepositiverate = float(TP)/(TP+FN)\n",
        "fakepositiverate = float(FP)/(FP+FN)\n",
        "print('ANN score cut chosen:%.4f' % cut_dnn)\n",
        "print(\"TP rate = %.4f\"%truepositiverate)\n",
        "print(\"FP rate = %.4f\"%fakepositiverate)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT287gN2V3e1"
      },
      "source": [
        "## <font color=\"#002A48\">Plot some physical quantities after that the event selection is applied</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A164JqJX911E"
      },
      "source": [
        "Note that we have not used the *low level features* in the training phase of our models,they behaved as **spectator variables**. We will plot the distribution of events considering their **actual label** (in the legend **signal** and **background**) and the distributions for the two classes that our classifiers have built after having fixed a threshold on their output scores.\n",
        "\n",
        "#### <font color=\"#4196B4\">**TASK:**</font> look at the observable plots and comment on them. Make an **importance list** of the high-level features used by the NN algorithm during its training phase. Taking into account the physics processes involved, did you expect these distributions?\n",
        "\n",
        "#### <font color=\"#4196B4\">**HINT:**</font>  The data-sets are simulated events in which the Higgs boson is produced with a mass of 125 GeV. Therefore, we expect to see one **on-mass-shell** Z boson and another **off-mass-shell** Z boson."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAVXz99TZQQ6"
      },
      "source": [
        "# Plot high level variables for signal, background and ANN selected events \n",
        "\n",
        "plt.xlabel('$\\Delta \\eta $  between $j_1$ and $j_2$')\n",
        "X = np.linspace(0.0,10.,100)\n",
        "plt.rcParams['figure.figsize'] = (10,5)\n",
        "# Plot bkg events\n",
        "df_all['f_deltajj'][(df_all['isSignal'] == 0)].plot.hist( bins=X, label='bkg',histtype='step', density=1 )\n",
        "# Plot signal events\n",
        "df_all['f_deltajj'][(df_all['isSignal'] == 1)].plot.hist(bins=X, label='signal',histtype='step', density=1)\n",
        "# Plot selected events by the ANN\n",
        "df_sel['f_deltajj'].plot.hist(bins=X, label='NN',histtype='step', density=1)\n",
        "plt.legend(loc='best')\n",
        "plt.title('$\\Delta \\eta $ between $j_1$ and $j_2$ normalized distribution',fontsize=12,fontweight='bold', color='r')\n",
        "plt.xlim(0,10)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaqLMubpQf6k"
      },
      "source": [
        "#### <font color=\"#4196B4\">**TASK:**</font> Plot other high-level features like the dijets mass for signal, background and the NN classification for those events. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0k54G8TZZ5k"
      },
      "source": [
        "# *********************************************************************************\n",
        "# * EDIT: Plot dijets mass for signal, background and NN selected events          *\n",
        "# *********************************************************************************\n",
        "\n",
        "plt.xlabel('$mass_{jj}$ (GeV)')\n",
        "X = np.linspace(0.0,1000.,100)\n",
        "plt.rcParams['figure.figsize'] = (10,5)\n",
        "df_all['f_massjj'][(df_all['isSignal'] == 0)].plot.hist(bins=X, label='bkg',histtype='step', density=1 )\n",
        "df_all['f_massjj'][(df_all['isSignal'] == 1)].plot.hist(bins=X, label='signal',histtype='step', density=1)\n",
        "df_sel['f_massjj'].plot.hist(bins=X, label='NN',histtype='step', density=1)\n",
        "plt.title('$m_{jj}$ normalized distribution',fontsize=12,fontweight='bold', color='r')\n",
        "plt.legend(loc='upper right')\n",
        "plt.xlim(0,1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qChekLzIZfPt"
      },
      "source": [
        "\n",
        "# Plot  dijets mass for signal, background and ANN selected events \n",
        "plt.xlabel('f_mass4l (GeV)')\n",
        "X = np.linspace(50, 400, 100)\n",
        "plt.rcParams['figure.figsize'] = (10,5)\n",
        "df_all['f_mass4l'][(df_all['isSignal'] == 0)].plot.hist(bins=X, label='bkg',histtype='step',log=True, density=1)\n",
        "df_all['f_mass4l'][(df_all['isSignal'] == 1)].plot.hist(bins=X, label='signal',histtype='step',log=True, density=1)\n",
        "df_sel['f_mass4l'].plot.hist(bins=X, label='NN',histtype='step', log=True, density=1)\n",
        "plt.title('$mass(4\\mu)$ normalized distribution',fontsize=12,fontweight='bold', color='r')\n",
        "plt.legend(loc='upper right')\n",
        "plt.xlim(50,400)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_fGRHZ5Zht9"
      },
      "source": [
        "\n",
        "plt.xlabel('$mass(Z_{1})$ (GeV)')\n",
        "X = np.linspace(20, 150, 100)\n",
        "plt.rcParams['figure.figsize'] = (10,5)\n",
        "df_all['f_Z1mass'][(df_all['isSignal'] == 0)].plot.hist(bins=X, label='bkg',histtype='step',log=True ,density=1)\n",
        "df_all['f_Z1mass'][(df_all['isSignal'] == 1)].plot.hist(bins=X, label='signal',histtype='step',log=True, density=1)\n",
        "df_sel['f_Z1mass'].plot.hist(bins=X, label='NN',histtype='step', log=True,density=1)\n",
        "plt.title('$mass(Z_{1})$ normalized distribution',fontsize=12,fontweight='bold', color='r')\n",
        "plt.legend(loc='upper right')\n",
        "plt.xlim(20,150)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMmpbANgZo_M"
      },
      "source": [
        "plt.xlabel('$mass(Z_{2})$ (GeV)')\n",
        "X = np.linspace(0., 150, 100)\n",
        "plt.rcParams['figure.figsize'] = (10,5)\n",
        "df_all['f_Z2mass'][(df_all['isSignal'] == 0)].plot.hist(bins=X, label='bkg',histtype='step', density=1)\n",
        "df_all['f_Z2mass'][(df_all['isSignal'] == 1)].plot.hist(bins=X, label='signal',histtype='step', density=1)\n",
        "df_sel['f_Z2mass'].plot.hist(bins=X, label='NN',histtype='step', density=1)\n",
        "plt.title('$mass(Z_{2})$ normalized distribution',fontsize=12,fontweight='bold', color='r')\n",
        "plt.legend(loc='upper right')\n",
        "plt.xlim(0.,150)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIZ-48Zn_EvW"
      },
      "source": [
        "#### <font color=\"#4196B4\">**TASK:**</font> let's do the same for some variables which we have not used during the **training phase**. What can you say about them?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WD37Uj5gZsH3"
      },
      "source": [
        "# Plot Jet1 eta for signal, background and NN selected events \n",
        "plt.xlabel('$\\eta$(Jet1)')\n",
        "X = np.linspace(-5.,5.,100)\n",
        "plt.rcParams['figure.figsize'] = (10,5)\n",
        "df_all['f_jet1_eta'][(df_all['isSignal'] == 0)].plot.hist(bins=X, label='bkg',histtype='step', density=1)\n",
        "df_all['f_jet1_eta'][(df_all['isSignal'] == 1)].plot.hist(bins=X, label='signal',histtype='step', density=1)\n",
        "df_sel['f_jet1_eta'].plot.hist(bins=X, label='NN',histtype='step', density=1)\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('$jet1(\\eta)$ normalized distribution',fontsize=12,fontweight='bold', color='r')\n",
        "plt.xlim(-5,5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2o7dAPWey7_"
      },
      "source": [
        "# Plot Jet2 eta for signal, background and NN selected events \n",
        "plt.xlabel('$\\eta$(Jet2)')\n",
        "X = np.linspace(-5.,5.,100)\n",
        "plt.rcParams['figure.figsize'] = (10,5)\n",
        "df_all['f_jet2_eta'][(df_all['isSignal'] == 0)].plot.hist(bins=X, label='bkg',histtype='step', density=1)\n",
        "df_all['f_jet2_eta'][(df_all['isSignal'] == 1)].plot.hist(bins=X, label='signal',histtype='step', density=1)\n",
        "df_sel['f_jet2_eta'].plot.hist(bins=X, label='NN',histtype='step', density=1)\n",
        "plt.title('$jet2(\\eta)$ normalized distribution',fontsize=12,fontweight='bold', color='r')\n",
        "plt.legend(loc='upper right')\n",
        "plt.xlim(-5,5)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsfqCKP7pkcv"
      },
      "source": [
        "# <font color=\"#002A48\">Optional Exercise 1 - Change the decay channel</font>\n",
        "\n",
        "**Question to students:** What happens if you switch to the $4e$ decay channel? You can submit your model (see the ML challenge below) for this physical process as well!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioptbmBm2_85"
      },
      "source": [
        "# <font color=\"#002A48\">Optional Exercise 2 - Merge the backgrounds</font>\n",
        "\n",
        "**Question to students:** Merge the backgrounds used up to now for the training of our ML algorithms together with the ROOT File named **ttH_HToZZ_4L.root**. In this case you will use also the QCD $gg\\to t\\bar tH \\to t\\bar t ZZ \\to 4\\mu (e) + X$ *irreducible* background. Uncomment the correct lines of code to proceed!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHpI_MUppgXz"
      },
      "source": [
        "# <font color=\"#002A48\">You are really here, very very good...Try your hand at the Machine Learning challenge!</font>\n",
        "\n",
        "Once you manage to improve the network performances, you can submit your results and participate in our ML challenge. The challenge samples are available in this workspace, but the true labels (`isSignal`) are removed so that you can't compute the AUC.\n",
        "\n",
        "* You can participate as a single participant or as a team\n",
        "* The winner is the one scoring the best AUC in the challenge samples!\n",
        "* In the next box, you will find some lines of code for preparing an output csv file, containing your y_predic for this new data-set!\n",
        "* Choose a meaningful name for your result csv file (i.e. your name, or your team name, the model used for the training phase, and the decay channel - 4$\\mu$ or 4$e$ - but avoid to submit `results.csv`)\n",
        "* Download the csv file and upload it here: https://recascloud.ba.infn.it/index.php/s/CnoZuNrlr3x7uPI\n",
        "* You can submit multiple results, paying attention to name them accordingly (add the version number, such as `v1`, `v34`, etc.)\n",
        "* You can use this exercise as a starting point (train over constituents)\n",
        "* We will consider your best result for the final score.\n",
        "* The winner will be asked to present the ML architecture!\n",
        "\n",
        "**<font color=\"#002A48\">Have fun!</font>**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBTeNwm__vn1"
      },
      "source": [
        "### Evaluate performance on an independent sample\n",
        "# DO NOT CHANGE BELOW!\n",
        "\n",
        "\n",
        "\n",
        "files_challenge = {\n",
        "    \"input_hl.csv\":\"dBHt9vsvKDUkJNt\" #high level features\n",
        "    }\n",
        "\n",
        "import os \n",
        "for file in files_challenge.items():\n",
        "  if not os.path.exists(file[0]):\n",
        "    b = os.system ( \"wget -O %s --no-check-certificate 'https://recascloud.ba.infn.it/index.php/s/%s/download'\" % file )\n",
        "    if b: raise IOError ( \"Error in downloading the file %s : (%s)\" % file )\n",
        "\n",
        "# If you have trouble running the previous commands in this cell, you can import the same files from INFN Pandora\n",
        "\n",
        "#files_challenge = {\n",
        "#    \"input_hl.csv\":\"93dcd6/dl/input_HL-2.csv\" #high level features\n",
        "#    }\n",
        "#import os \n",
        "#for file in files_challenge.items():\n",
        "#  if not os.path.exists(file[0]):\n",
        "#    b = os.system ( \"wget -O %s --no-check-certificate 'https://pandora.infn.it/public/%s'\" % file )\n",
        "#    if b: raise IOError ( \"Error in downloading the file %s : (%s)\" % file )\n",
        "\n",
        "\n",
        "filename = {}\n",
        "\n",
        "df_challenge = {}\n",
        "#Open the file with dat aset without y_true (only features used for the training of the previous NN model)\n",
        "filename['input'] = 'input_hl.csv'\n",
        "df_challenge['input']  = pd.read_csv(filename['input'])\n",
        "print(df_challenge['input'].shape)\n",
        "df_challenge['input'].columns= NN_VARS \n",
        "X_challenge  = np.asarray( df_challenge['input'].values ).astype(np.float32)\n",
        "ret = model.predict(X_challenge[:,0:NDIM])\n",
        "print(ret.shape)\n",
        "print(ret)\n",
        "#Convert the y_pred in a dataframe \n",
        "df_answer= pd.DataFrame(ret)\n",
        "df_answer.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8y3faPpF_Pw"
      },
      "source": [
        "#Check of the input data-set without y_true\n",
        "df_challenge['input'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLOwI94pBX92"
      },
      "source": [
        "Now, let us convert the data-frame into a csv file!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGkRUOwA4YXr"
      },
      "source": [
        "# As a first step,insert your group name (be creative)!\n",
        "group_name = input('Enter your group name:')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVPtFd36B2TO"
      },
      "source": [
        "Now you are ready to create your predicted labels to send us!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKvU1nJ9Bw6L"
      },
      "source": [
        "output_filename = group_name + '_answer.csv'\n",
        "df_answer.to_csv(output_filename)\n",
        "print('Your y_pred has been created! Download it from your Drive directory!\\n')\n",
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHSuimZElkhM"
      },
      "source": [
        " Upload your results here:\n",
        " \n",
        " https://recascloud.ba.infn.it/index.php/s/CnoZuNrlr3x7uPI\n"
      ]
    }
  ]
}