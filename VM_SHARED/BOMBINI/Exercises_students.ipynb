{
 "cells": [
 {
   "cell_type": "markdown",
   "id": "26f37f1a",
   "metadata": {},
   "source": [
    "# Exercises: Hands on DNN and 1DCNN\n",
    "\n",
    "> <font size=+1>Instructions: this notebook contains the whole definition of a training and test workflow. It is intended for you to **modify the code of the existing cells** to complete the challenges proposed as exercises.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6187d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading some useful libraries discussed in previous sessions of the hackathon\n",
    "\n",
    "# OS is the built-in python library for exploring the file system\n",
    "## from Standard Python Library\n",
    "import os \n",
    "import math\n",
    "\n",
    "# Numpy, Pandas and matplotlib are must have libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#  Image processing libraries\n",
    "\n",
    "from PIL import Image, ImageOps \n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "# Machine learning libraries \n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"-1\"\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "# GPU usage; see https://www.tensorflow.org/guide/gpu\n",
    "print('Check on not using GPU through TensorFlow\\n')\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "assert len(gpus) == 0, \"Using GPU is disabled by default.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3d9f7b",
   "metadata": {},
   "source": [
    "## Exercise 0: Data preprocessing using numpy\n",
    "\n",
    "In the lecture we skipped a crucial step, which precedes the training process: the data preprocessing. \n",
    "\n",
    "It is a fundamental step to standardize and uniformize data, which ultimately leads to a better training process. \n",
    "\n",
    "<img src=\"./Assets/Images/data_preprocessing.png\" alt=\"Drawing\" style=\"width: 65%\"/>\n",
    "\n",
    "In particular, here we should normalize the data (so that the count per histogram per pixel are normalized to lie in the $[0,1)$ interval). \n",
    "\n",
    "This exercise is quite introductory and it doesn't need any knowledge on Machine Learning; it is an introductory exercise where people may test their fluency in the use of numpy.\n",
    "\n",
    "**EXERCISE**\n",
    "\n",
    "Import the data and standardize them, following the steps:\n",
    "1. Compute the number of counts per pixel (i.e. for each row of the dataset);\n",
    "2. Compute the mean and standard deviation for each pixel\n",
    "3. Create a new, preprocessed X_train (X_test) where the new array values have zero mean, uniform standard deviation and they sum to 1. \n",
    "\n",
    "If you feel comfortable about it, you may define a custom ah-hoc python class for this pre-processing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d85ff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset: pick a path to .csv file\n",
    "path = 'Training_dataset/dataset.csv'\n",
    "\n",
    "if not os.path.isfile(path):\n",
    "    try:\n",
    "        # wget\n",
    "        !wget -O {path} https://pandora.infn.it/public/b0a561/dl/dataset.csv\n",
    "    except:\n",
    "        !wget  https://pandora.infn.it/public/b0a561/dl/dataset.csv\n",
    "        path = path.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6b0b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to import csv\n",
    "df_train_dataset = pd.read_csv(path, index_col=0)\n",
    "\n",
    "# Transform the strings representing the histograms into lists\n",
    "df_train_dataset['X'] = df_train_dataset['X'].str[1:-1].str.split(\" \")\n",
    "df_train_dataset['y'] = df_train_dataset['y'].str[1:-1].str.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc26b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfrom the lists into arrays, cleaning empty entries \n",
    "hists = []\n",
    "rgbs  = []\n",
    "from tqdm import tqdm \n",
    "for idx_, (X, y) in tqdm(df_train_dataset[['X', 'y']].iterrows(), total=len(df_train_dataset), desc=\"Loading...\"):\n",
    "    hists.append (np.array([float(v) for v in X if v not in [\"\\n\", \"\", \" \"]]))\n",
    "    rgbs.append (np.array([float(v) for v in y if v not in [\"\\n\", \"\", \" \"]]))   \n",
    "    \n",
    "hists = np.array(hists)\n",
    "rgbs = np.array(rgbs)\n",
    "del df_train_dataset\n",
    "np.savez (\"pixel_dataset.npz\", hists=hists.astype(np.float32), rgbs=rgbs.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa2131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://pandora.infn.it/public/ca79b6/dl/pixel_dataset.npz -O \"dataset.npz\"\n",
    "!wget -q https://pandora.infn.it/public/141790/dl/Putto_Area1_fiori.npz -O \"Rawdata+RGB/Putto_Area1_fiori.npz\"\n",
    "\n",
    "dataset = np.load('dataset.npz')\n",
    "hists = dataset['hists']\n",
    "rgbs = dataset['rgbs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3a3791",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lrt Rawdata+RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c168ca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_dataset(X, y, percentage = 0.8):\n",
    "    # Shuffle the dataframe\n",
    "    assert len(X)==len(y), \"Inconsistent number of rows in X and y\"\n",
    "    \n",
    "    N = len(X)\n",
    "    indices = np.random.permutation (N)\n",
    "    \n",
    "    n = int(N*percentage)\n",
    "    train = indices[:n]\n",
    "    test = indices[n:]\n",
    "    \n",
    "    return X[train], y[train], X[test], y[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35919d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = prepare_training_dataset(hists, rgbs, percentage = 0.8)\n",
    "\n",
    "print('Train elements: ', np.shape(X_train) )\n",
    "print('Test Elements: ', np.shape(X_test) )\n",
    "\n",
    "print('Train/test features: ', len(X_train[0]) )\n",
    "print(f'Shape X_train: {(np.shape(X_train)[0], np.shape(X_train)[1]) }')\n",
    "print(f'Shape y_train: {(np.shape(y_train)[0], np.shape(y_train)[1]) }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890c532b",
   "metadata": {},
   "source": [
    "## Exercise 1: Counteract Overfitting - DropOut layers\n",
    "\n",
    "<img src=\"./Assets/Images/overfitting_1.png\" alt=\"Drawing\" style=\"float: center; margin-top: 15px\"/>\n",
    "\n",
    "In statistics, overfitting is:\n",
    "\n",
    "*\"the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably.\"*\n",
    "\n",
    "<img src=\"./Assets/Images/overfitting-comics.jpg\" alt=\"Drawing\" style=\"float: center; margin-top: 15px\"/>\n",
    "\n",
    "In machine learning models (which are statistical models), overfitting appears when there is an unbalance between the number of trainable parameters and the size of the training dataset; it becomes manifest during the training process when the model performs poorly on the test/validation dataset, w.r.t. the training dataset, i.e. when the validation scores starts increasing in epochs, while training scores still decreases\n",
    "\n",
    "<img src=\"./Assets/Images/overfitting_2.png\" alt=\"Drawing\" style=\"float: center; margin-top: 15px; width: 400px\"/>\n",
    "\n",
    "A way to counteract this event in Deep Neural Networks is to add **Dropout layers** in the DNN architecture. [2], [3], [4]\n",
    "\n",
    "Dropout is a regularization method that approximates training a large number of neural networks with different architectures in parallel. During training, some of the layer outputs are randomly ignored or “dropped out”. Dropout has the effect of making the training process noisy, forcing nodes within a layer to probabilistically take on more or less responsibility on the outputs.\n",
    "\n",
    "<img src=\"./Assets/Images/DropLayer.gif\" alt=\"Drawing\" style=\"float: center; margin-top: 15px;\"/>\n",
    "\n",
    "In Keras, dropout layers are easily implemented by the *Dropout* class [5]\n",
    "\n",
    "tf.keras.layers.Dropout(rate, noise_shape=None, seed=None, **kwargs)\n",
    "\n",
    "that:\n",
    "\n",
    "*The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged.*\n",
    "\n",
    "**EXERCISE**\n",
    "\n",
    "Build a Deep Dense Neural Network with (at least) 5 layers, and apply it to our model. (Keep an eye on the number of parameters, and on how long does it take to train an epoch).\n",
    "\n",
    "See if it shows overfitting; if not, deepen it, until it shows it. \n",
    "\n",
    "Then, create a new network with *Dropout* layers, and see if it is now free of overfitting. \n",
    "\n",
    "Training a very deep neural network may become may become very expensive. Here we try to create networks large and deep enough to reach overfitting almost immediately. \n",
    "\n",
    "\n",
    "-------\n",
    "[1] https://www.lexico.com/definition/overfitting\n",
    "\n",
    "[2] https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/\n",
    "\n",
    "[3] https://machinelearningmastery.com/how-to-reduce-overfitting-with-dropout-regularization-in-keras/\n",
    "\n",
    "[4] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov, *Dropout: A Simple Way to Prevent Neural Networks from Overfitting* https://jmlr.org/papers/v15/srivastava14a.html\n",
    "\n",
    "[5] https://keras.io/api/layers/regularization_layers/dropout/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c11875",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "# Import relevant Keras classes\n",
    "\n",
    "# Input layer \n",
    "from keras.layers import InputLayer, Input\n",
    "# The Dense Layer\n",
    "from keras.layers import Dense\n",
    "# Utilities\n",
    "from keras.layers import Flatten, MaxPooling1D, Dropout, AveragePooling1D, GlobalAveragePooling1D, Reshape\n",
    "\n",
    "# Sequential class for instantiating the Model\n",
    "from keras.models import Sequential\n",
    "from keras import Model # If we want to use the Keras API\n",
    "\n",
    "# To get the Activation functions\n",
    "from keras.layers import Activation\n",
    "\n",
    "\n",
    "#######################################################################################################################\n",
    "# Input-Output defs\n",
    "\n",
    "input_shape = np.shape(X_train)[1]   # Input layer takes [number of vectors, size of vectors, features-per-vector] = [#size_dataset, 1000, 1]\n",
    "num_classes = 3 # RGB\n",
    "\n",
    "CHANNEL_DEPTH = 500   # Each histograms has 500 channels\n",
    "num_sensors = 1       # We have only 1 histogram per data\n",
    "\n",
    "#######################################################################################################################\n",
    "# Defining the model\n",
    "#######################################################################################################################\n",
    "\n",
    "# Instantiate it\n",
    "model_m = Sequential()\n",
    "\n",
    "#############\n",
    "# input\n",
    "model_m.add(Input(shape=(input_shape, )))\n",
    "\n",
    "#############\n",
    "# Hidden Layers\n",
    "\n",
    "\n",
    "########################## YOUR CODE HERE ########################\n",
    "\n",
    "\n",
    "\n",
    "# Output layer\n",
    "model_m.add(Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "###########\n",
    "# Summary\n",
    "print(model_m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91646440",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# Training - prerequisites\n",
    "loss_used = 'binary_crossentropy'\n",
    "\n",
    "model_m.compile(loss=loss_used,\n",
    "                 optimizer='adam',\n",
    "                 )\n",
    "# We need to pick the size of the batch used for the training\n",
    "BATCH_SIZE = 200 \n",
    "# And the number of epochs on which we train the dataset\n",
    "EPOCHS = 20 \n",
    "\n",
    "import datetime\n",
    "starting_time = datetime.datetime.now() \n",
    "\n",
    "print('\\nStart training model: \\n')\n",
    "\n",
    "##########################################################\n",
    "# TRAINING\n",
    "\n",
    "history = model_m.fit(X_train, \n",
    "                      y_train,batch_size=BATCH_SIZE,\n",
    "                      epochs=EPOCHS,\n",
    "                       validation_data=(X_test, y_test),\n",
    "                       verbose=1)\n",
    "bins=np.linspace(0, 3000, 300)\n",
    "plt.hist (X_train.sum(axis=1))\n",
    "plt.xlabel (\"Number of photons\")\n",
    "plt.ylabel (\"Number of pixels\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist (X_train.flatten(), bins=np.linspace(0, 500, 501))\n",
    "plt.xlabel (\"Number of photons\")\n",
    "plt.ylabel (\"Number of ADC channels per pixel\")\n",
    "plt.yscale ('log')\n",
    "plt.show()\n",
    "print('\\n\\nTraining done...\\n')\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "total_training_time = now-starting_time\n",
    "print('Total training time: ', total_training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dec8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.title('Model loss', fontsize=20,  pad=10)\n",
    "\n",
    "plt.ylabel('loss', fontsize=18)\n",
    "plt.xlabel('epoch', fontsize=18)\n",
    "\n",
    "plt.legend(['loss', 'val_loss'], loc='upper right', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98248690",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# Training - prerequisites\n",
    "loss_used = 'binary_crossentropy'\n",
    "\n",
    "model_m.compile(loss=loss_used,\n",
    "                 optimizer='adam',\n",
    "                 )\n",
    "# We need to pick the size of the batch used for the training\n",
    "BATCH_SIZE = 200 \n",
    "# And the number of epochs on which we train the dataset\n",
    "EPOCHS = 20 \n",
    "\n",
    "import datetime\n",
    "starting_time = datetime.datetime.now() \n",
    "\n",
    "print('\\nStart training model: \\n')\n",
    "\n",
    "##########################################################\n",
    "# TRAINING\n",
    "\n",
    "history_m = model_m.fit(X_train,\n",
    "                       y_train,\n",
    "                       batch_size=BATCH_SIZE,\n",
    "                       epochs=EPOCHS,\n",
    "                       #callbacks=callbacks_list,\n",
    "                       validation_data=(X_test, y_test),\n",
    "                       verbose=1)\n",
    "\n",
    "print('\\n\\nTraining done...\\n')\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "total_training_time = now-starting_time\n",
    "print('Total training time: ', total_training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3077865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "filename = \"Rawdata+RGB/Putto_Area1_fiori\"\n",
    "xrf_img = np.load(f\"{filename}.npz\")['img']\n",
    "print (f\"Shape of the loaded XRF image: {xrf_img.shape}\")\n",
    "true_img = np.asarray(PIL.Image.open(f\"{filename}.jpg\"))\n",
    "print (f\"Shape of the loaded RGB image (true): {true_img.shape}\")\n",
    "\n",
    "pixels = xrf_img.reshape ((-1, 500))\n",
    "rgb_pred = model_m.predict (pixels).reshape (xrf_img[:,:,:3].shape)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16,16), dpi=100)\n",
    "plt.subplot(121)\n",
    "plt.title (\"Flowers (true)\")\n",
    "plt.imshow ( true_img )\n",
    "plt.subplot(122)\n",
    "plt.title (\"Flowers (reconstructed)\")\n",
    "plt.imshow ( np.flip(rgb_pred, 1) )\n",
    "# Make space for title\n",
    "plt.subplots_adjust(top=0.85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ccd9d4",
   "metadata": {},
   "source": [
    "## Exercise 2: Deep CNN; hints from ImageNet Large Scale Visual Recognition Challenge winners - AlexNet & VGG\n",
    "\n",
    "The **ImageNet** project is a large visual database designed for use in visual object recognition software research. More than 14 million images have been hand-annotated by the project. \n",
    "\n",
    "Since 2010, a competition was launch, dubbed *ImageNet Large Scale Visual Recognition Challenge* (ILSVRC), where researchers from all around the globe have to design a novel Deep Learning architecture to label the dataset. From the ImageNet homepage [1]:\n",
    "\n",
    "*The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) evaluates algorithms for object detection and image classification at large scale. One high level motivation is to allow researchers to compare progress in detection across a wider variety of objects -- taking advantage of the quite expensive labeling effort. Another motivation is to measure the progress of computer vision for large scale image indexing for retrieval and annotation.*\n",
    "\n",
    "### AlexNet\n",
    "\n",
    "On 30 September 2012, a convolutional neural network (CNN) called **AlexNet** [3] achieved a top-5 error of 15.3% in the ImageNet 2012 Challenge, more than 10.8 percentage points lower than that of the runner up. \n",
    "\n",
    "AlexNet is considered one of the most influential papers published in computer vision, having spurred many more papers published employing CNNs and GPUs to accelerate deep learning. As of 2021, the AlexNet paper has been cited over 80,000 times according to Google Scholar. \n",
    "\n",
    "<img src=\"./Assets/Images/AlexNet.png\" alt=\"Drawing\" style=\"float: center; margin-top: 15px\"/>\n",
    "\n",
    "AlexNet contained eight layers; the first five were convolutional layers, some of them followed by max-pooling layers, and the last three were fully connected layers. It used the non-saturating ReLU activation function, which showed improved training performance over tanh and sigmoid. In detail [3]:\n",
    "\n",
    "- 1 Convolutional layer with kernel size (11,11) and 96 filters, followed by a (3,3)-MaxPooling;\n",
    "- 1 Convolutional layer with kernel size (5,5) and 256 filters, followed by a (3,3)-MaxPooling;\n",
    "- 1 Convolutional layer with kernel size (3,3) and 128 filters;\n",
    "- 2 Convolutional layers with kernel size (3,3) and 192 filters each, followed by a Flatten layer to feed the dense layer;\n",
    "- 2 Dense layers with 4096 neurons and a 0.5-probability drop layer (see ex.1)\n",
    "- the Output layer which may label 10 categories.\n",
    "\n",
    "In code [4]: \n",
    "\n",
    "```\n",
    "AlexNet_model = keras.models.Sequential(\n",
    "    [\n",
    "    # 1°: (11x11)\n",
    "    keras.layers.Conv2D(filters=96, \n",
    "                            kernel_size=(11,11), strides=(4,4), activation='relu', \n",
    "                            input_shape=(227,227,3)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "\n",
    "    # 2° (5x5)\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "\n",
    "    # 3,4,5° (3x3)\n",
    "    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization()\n",
    "    ,\n",
    "    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Flatten(),\n",
    "\n",
    "    # Dense layers\n",
    "    keras.layers.Dense(4096, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(4096, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "\n",
    "    # Output\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "### VGG\n",
    "\n",
    "In 2014, a novel architecture developed by the Oxford's *Visual Geometry Group* (VGG), dubbed *Very Deep Convolutional Networks for Large-Scale Visual Recognition* (shortened in VGG-16) was presented at ILSVRC; citing the VGG page [5] \n",
    "\n",
    "*Our main contribution is a rigorous evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by increasing the depth to 16-19 weight layers, which is substantially deeper than what has been used in the prior art. To reduce the number of parameters in such very deep networks, we use very small 3×3 filters in all convolutional layers (the convolution stride is set to 1). Please see our publication for more details.*\n",
    "\n",
    "*The very deep ConvNets were the basis of our ImageNet ILSVRC-2014 submission, where our team (VGG) secured the first and the second places in the localisation and classification tasks respectively.*\n",
    "\n",
    "<img src=\"./Assets/Images/VGG163d.png\" alt=\"Drawing\" style=\"float: center; margin-top: 15px\"/>\n",
    "\n",
    "The VGG architecture (in his base form, VGG-16, and its evolution, VGG-19) is [6]:\n",
    "\n",
    "<img src=\"./Assets/Images/VGG1619.png\" alt=\"Drawing\" style=\"float: center; margin-top: 15px; width: 50%\"/>\n",
    "\n",
    "### EXERCISE:\n",
    "\n",
    "Choose one of the two architectures, and write down an analougus 1-dimensional model. Keep the numbers low, you should avoid either overfitting (see ex.1) as well as a too long training time. \n",
    "\n",
    "After that, try adapting the various (hyper)parameters to lower the loss, and, when you find something plausible, try to recolor the XRF image, as in the lecture. \n",
    "\n",
    "-------------------\n",
    "[1] https://image-net.org/challenges/LSVRC/\n",
    "\n",
    "[2] O. Russakovsky et al., *ImageNet Large Scale Visual Recognition Challenge* https://arxiv.org/abs/1409.0575\n",
    "\n",
    "[3] A. Krizhevsky, I. Sutskever, G. E. Hinton, *ImageNet Classification with Deep ConvolutionalNeural Networks*, doi:10.1145/3065386 https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\n",
    "\n",
    "[4] https://towardsdatascience.com/implementing-alexnet-cnn-architecture-using-tensorflow-2-0-and-keras-2113e090ad98\n",
    "\n",
    "[5] https://www.robots.ox.ac.uk/~vgg/research/very_deep/\n",
    "\n",
    "[6] K. Simonyan, A. Zisserman, *Very Deep Convolutional Networks for Large-Scale Image Recognition*, https://arxiv.org/abs/1409.1556\n",
    "\n",
    "For an extensive recap on ILSVRC and more: https://medium.com/analytics-vidhya/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4125fbb",
   "metadata": {},
   "source": [
    "## Exercise 3: Design your own DNN\n",
    "\n",
    "**EXERCISE**\n",
    "\n",
    "Build your own Deep Neural Network. Use anything you like from the lecture. \n",
    "\n",
    "PS: a suggestion: try to use the keras functional API, and a non-linear topology. Google \"Deep Learning\" + a title of a famous Christopher Nolan's movie with Di Caprio and Joseph Gordon-Levitt. It could give you inspiration. \n",
    "<img src=\"./Assets/Images/inception_film.jpg\" alt=\"Drawing\" style=\"float: center; margin-top: 15px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32da3996",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
