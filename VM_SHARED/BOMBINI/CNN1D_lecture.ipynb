{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "847ce1ce",
   "metadata": {},
   "source": [
    "<img src=\"./Assets/Pics/EOSc-Pillar_logo_final.png\"/>\n",
    "\n",
    "# Applied Physics and 1D Convolutional Neural Networks\n",
    "\n",
    "<img src=\"./Assets/Pics/Logo_INFN_CHNet_esteso.png\" alt=\"Drawing\" style=\"width: 200px; float: right\"/>\n",
    "\n",
    "The goal of this lecture is to give a brief introduction to how to apply Deep Neural Networks on Physical Imaging of Cultural Heritages, such as drawings, paintings or frescoes. \n",
    "\n",
    "## 1. Physical Imaging using spectroscopic techniques\n",
    "\n",
    "<img src=\"./Assets/Pics/labec.png\" alt=\"Drawing\" style=\"width: 180px; float: left; margin-right: 10px; margin-top: 1em\"/>\n",
    "\n",
    "Here at the INFN, sezione di Firenze, there is the **LABEC**, the *Laboratorio di tecniche nucleari applicate all'Ambiente e ai Beni Culturali*, where a lot of nuclear techniques are applied to the field of Cultural Heritages, from Radiocarbon Dating to Neutron Muography; LABEC is the funder node of the internal network of INFN devoted to application of nuclear techniques to Cultural Heritage, the so-called *Cultural Heritage Network* (**CHNet**).\n",
    "\n",
    "From all of these techniques, there are some which may be used for Imaging: those are the *Ion Beam Analysis* (IBA), such as the *Particle Induced X-ray Emission* (PIXE) and the *Particle Induces Gamma-ray Emission* (PIGE), and the *X-ray Fluorescence*. \n",
    "\n",
    "For the former, an accelerator machine is required, since we need to bombard the analyzed item with nucleons (usually protons, i.e. ionized ${}^1H$, sometimes $\\alpha$-particles, i.e. fully-ionized ${}^4_2 He$.\n",
    "\n",
    "For the latter, instead, we need an *X-ray tube*: it consists in a vacuum tube, with an metallic filament cathode and with a single-element anode; by thermally heating the metallic filament cathod, it starts to emit electrons via a thermoionic process. Those electrons are thus accelerated by a Voltage in the tube ($\\sim 10-20 \\,\\mathrm{KeV}$ ) towards the anode. Hitting the anode, those electrons lose energy by emitting\n",
    "- *Bremsstrahlung* radiation\n",
    "- typical Fluorescence lines (see below) of the element(s) constituting the anode. \n",
    "\n",
    "<div style=\"width:100%\">\n",
    "<img src=\"./Assets/Images/XRF_apparatus_schema.png\" alt=\"Drawing\" style=\"float: left; width: 40%\"/>\n",
    "<img src=\"./Assets/Images/X-tray-tube-schema.png\" alt=\"Drawing\" style=\"float: right; width: 40%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6453074",
   "metadata": {},
   "source": [
    "The image on the left, is a photo of the Movable XRF scanner for in-situ analysis of CHNet, Firenze. The image on the right is a photo of MACHINA, *Movable Accelerator for Cultural Heritage In-situ Non-destructive Analysis*, developed jointly by INFN, Firenze and CERN. \n",
    "\n",
    "<div style=\"width:100%; margin-top: 3em; margin-bottom: 3em\">\n",
    "<img src=\"./Assets/Images/XRF_instr.png\" alt=\"Drawing\" style=\"float: left; width: 40%\"/>\n",
    "<img src=\"./Assets/Images/machina_foto.jpg\" alt=\"Drawing\" style=\"float: right; width: 55%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba8bfec",
   "metadata": {},
   "source": [
    "### 1.1 X-ray Fluorescence\n",
    "\n",
    "Among the many physical techniques applied at LABEC , I will mainly focus on the Stimulated X-Ray Fluorescence (XRF); it is the emission of characteristic \"secondary\" (or fluorescent) X-rays from a material that has been excited by being bombarded with high-energy X-rays.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/41/X-ray_fluorescence_simple_figure.svg\" alt=\"Drawing\" style=\"width: 250px; float: right;\"/>\n",
    "\n",
    "We use a tube emitting a(n almost) uniform electromagnetic radiation, which hits elements in the painted surface; since the incoming radiation has high frequencies (X-ray band, from 0.5 KeV to 29-30 KeV circa), via the *photoelectric effect*, it can excite an electron from an inner orbital, ionizing the atom. This induces the electrons belonging to outer orbitals to fall to the empty inner orbital to fill the hole; in this process, the atom emits a photon, the energy of which is equal to the energy difference of the two orbitals involved. This process is called *fluorescence*. \n",
    "\n",
    "\n",
    "Among all the possibile matter/radiation processes, the *photoelectric effect* is the dominant one for energies in this range; its cross section highly depends on $Z$, via its fifth power $Z^5$, and it is inversely proportional to the Energy of the X-ray to the power of $7/2$, i.e.\n",
    "\n",
    "$$\\sigma_{\\mathrm{ph}} \\propto \\frac{Z^5}{E_X^{7/2}} \\,.$$\n",
    "\n",
    "For a given electronic level (which is a characteristic of the element(s) comprising the pigment), the probability is peaked around the characteristic ionization energy, as usual. \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/28/XRFScan.jpg\" alt=\"Drawing\"/>\n",
    "\n",
    "This easily implies that the received spectrum after the X-ray fluorescence is comprised by a set of bands, peaked around the characteristic levels of the possible ionizations of all elements which are present in the pigment(s).\n",
    "\n",
    "Each element has more than one possible \"resonance\" in the X-ray window; those are usually labelled by the *Siegbahn notation* [1];\n",
    "\n",
    "<div style=\"width:100%\">\n",
    "<img src=\"./Assets/Images/X-ray_nomenclature.png\" alt=\"Drawing\" style=\"float: left; width: 50%\"/>\n",
    "<img src=\"./Assets/Images/X-ray_nomenclature_2.png\" alt=\"Drawing\" style=\"float: right; width: 50%\"/>\n",
    "</div>\n",
    "\n",
    "<img src=\"./Assets/Images/Orbitals_line_spectroscopy.png\" alt=\"Drawing\" style=\"width: 40%\"/>\n",
    "\n",
    "Values of different kinds of transition energies like $K_α$, $K_β$, $L_α$, $L_β$ etc. for different elements can be found in the NIST X-Ray Transition Energies Database [2].\n",
    "\n",
    "\n",
    "--------------\n",
    "[1] http://old.iupac.org/reports/V/spectro/partVIII.pdf\n",
    "\n",
    "[2] https://physics.nist.gov/PhysRefData/XrayTrans/Html/search.html\n",
    "\n",
    "For an extensive review on PIXE, PIGE and XRF, see\n",
    "\n",
    "[3] P. A. Mandò, *Particle‐Induced X‐Ray Emission (PIXE)* https://doi.org/10.1002/9780470027318.a6210.pub3\n",
    "\n",
    "[4] C. Ruberto, *Elemental maps with X-ray fluorescence*, Ph.D. Thesis, https://core.ac.uk/download/pdf/301572417.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c921435a",
   "metadata": {},
   "source": [
    "## 1.2. How an XRF raw data is organized \n",
    "\n",
    "As explained above, the Raw data obtained from an XRF analysis encodes a set of spectra; in particoular, a raw data encodes a 3-indeces tensor $\\mathcal{I}_{x,y; n}$ which can be seen in two ways - depending on how you slice it. \n",
    "\n",
    "Fixing $(x,y) = (\\bar x, \\bar y)$ indeces, $\\mathcal{I}_{\\bar x, \\bar y; n}$ represents the histograms of counts in the X-ray spectrum of the elements on that \"pixel\":\n",
    "\n",
    "$$\\mathcal{I}_{\\bar x, \\bar y; n} = \\mathrm{hist}_{\\bar x, \\bar y} [n] \\in \\mathbb{N}^N \\,,$$\n",
    "\n",
    "where $N$ is the number of bins comprising the histogram. \n",
    "\n",
    "If, instead, we fix $n = \\bar n$, we get a 2D image at a specific wavelength:\n",
    "\n",
    "$$\\mathrm{pixel} [x,y] = \\mathcal{I}_{x, y; \\bar n}  \\,.$$\n",
    "\n",
    "Since each (spectral line of an) element is spread over certain range of energy, the images of abundance of certain element $i$ are obtained by summing over certain intervals of $n$, $\\mathcal{D}_i$:\n",
    "\n",
    "$$\\mathrm{pixel}_{i} [x,y] = \\sum_{n \\in \\mathcal{D}_i} \\mathcal{I}_{x, y; n}  \\,.$$\n",
    "\n",
    "<div style=\"width:100%\">\n",
    "<img src=\"./Assets/Images/Entire_spectrum.png\" alt=\"Drawing\" style=\"float: left; width: 33%\"/>\n",
    "<div>\n",
    "<img src=\"./Assets/Images/Ca.png\" alt=\"Drawing\" style=\"float: left; width: 33%\"/>\n",
    "<img src=\"./Assets/Images/Fe.png\" alt=\"Drawing\" style=\"width: 33%\"/>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e517af3b",
   "metadata": {},
   "source": [
    "### 1.2.1  Raw data structure\n",
    "\n",
    "Here at LABEC, CHNet Firenze, we have a native XRF raw data [1], for (almost optimal) hard-disk size compression of the data files; \n",
    "\n",
    "The key element to notice is that the tensor $\\mathcal{I}$ above is a *sparse matrix*, i.e. a matrix in which most of the elements are zero. \n",
    "\n",
    "Also, since the dimensions of such tensor is $\\mathrm{dim} \\, \\mathcal{I} = (\\mathrm{width} , \\mathrm{height}, N) $ $\\sim (O(2\\cdot 10^2), O(2\\cdot 10^2), O(2\\cdot 10^4))$, it implies it has $O(10^9)$ entries. If each entry is has a size of a **int**, it means that the phtysical size of \n",
    "$$10^9 \\times 4 \\, \\mathrm{bytes} \\sim 4 \\,\\, \\mathrm{Gb} \\,,$$\n",
    "\n",
    "plus all the necessary matadata for ordering the tensor, which may almost double the actual size of the tensor. (This is not exactly true, since there are plenty of optimization methods when handling tensor, but let us skip that for the moment)\n",
    "\n",
    "#### 1.2.1.1 Raw data: explicit realization\n",
    "Thus, here at LABEC, we use a custom raw data system in order to compress the physical size on disk of the raw data: \n",
    "\n",
    "<img src=\"./Assets/Images/Raw_data_structure.png\" alt=\"Drawing\" style=\"float: center; margin-top: 15px\"/>\n",
    "\n",
    "there is an XML header, containing all relevant information about the analysis set-up, comprising the calibration parameters which allows us to pass from the \"Counts vs ADC channels\" unit of measure (the one unfortunately employed in this lecture), to the \"Counts vs Energy (KeV)\", via the linear law\n",
    "\n",
    "$$ E = \\mathrm{calibration\\_kev/ch} \\cdot \\mathrm{ADC} + \\mathrm{calibration\\_Offset} \\,.$$\n",
    "\n",
    "After that, we have a set of lines containing integers numbers; the integers either represent:\n",
    "- an *X-header*: it is a metadata informing us of the x-pixel position; it is a string made by\n",
    "$$ 50000000 +  (x \\cdot \\mathrm{Xstep}) \\,,$$\n",
    "where $\\mathrm{Xstep}$ is the step size passed to the XRF machine. \n",
    "- a *Y-header*: it is a metadata informing us of the y-pixel position; it is a string made by\n",
    "$$ 60000000 +  (y \\cdot \\mathrm{Ystep}) \\,,$$\n",
    "where $\\mathrm{Ystep}$ is the step size passed to the XRF machine. \n",
    "- a *count*; it represents the number of the ADC channel that get a count; \n",
    "\n",
    "In the example above we thus have, for the pixel $(x,y) = (2,2)$:\n",
    "-  1 count for the channel 1227;\n",
    "-  1 count for the channel 2226;\n",
    "-  1 count for the channel 1227;\n",
    "-  1 count for the channel 4580;\n",
    "-  2 counts for the channel 5492;\n",
    "\n",
    "and so on.\n",
    "\n",
    "<img src=\"./Assets/Images/big_raw_data.jpg\" alt=\"Drawing\" style=\"float: center; margin-top: 15px; width:45%\"/>\n",
    "\n",
    "\n",
    "### 1.2.2  Analysing the raw data: web edition\n",
    "\n",
    "The internal CHNet group devoted to the development of softwares and digital infrastructures for the network, the *Digital Heritage Laboratory* (**DHLAB**), has developed a web service, host in the CHNet cloud, for re-analyse the XRF raw data; in the gif below it is shown how it works. \n",
    "\n",
    "<img src=\"./Assets/Video/Prova_XRF_no_audio.gif\" alt=\"Drawing\" style=\"float: center; margin-top: 15px\"/>\n",
    "\n",
    "\n",
    "\n",
    "--------------\n",
    "\n",
    "[1] http://chnet.infn.it/wp-content/uploads/2017/06/Molab_XRF_imaging.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39429513",
   "metadata": {},
   "source": [
    "### DISCLAIMER \n",
    "\n",
    "All the raw data used here are digitally pre-treated by me; the original raw data are from two paintings of a private collection, whose kind owner, who wants to remain anonymous, granted us the right to use them for educational purposes only.\n",
    "\n",
    "\n",
    "<img src=\"./Rawdata+RGB/putto_VISscalato.png\" alt=\"Drawing\" style=\"float: left; margin-bottom: 18em; width: 50%\"/>\n",
    "<img src=\"./Rawdata+RGB/ragtriste_VISscalato.png\" alt=\"Drawing\" style=\"float: right;  margin-bottom: 4em; width: 50%\"/>\n",
    "\n",
    "    \n",
    "The XRF raw data were obtained by analysis performed at LABEC, Florence, in the context of two master thesis in the LM \"Scienze e materiali per la conservazione e il restauro\", of the University of Florence; \n",
    "the two authors are **Laura Gagliani** and **Mirella Ahmetovic**. The thesis are:\n",
    "\n",
    "[1] Mirella Ahmetovic, *Multi-analytical approach for the study of a XVII century Florentinepainting: complementarity and data-crossing of the results of non-invasive diagnostics aimed at attribution and conservation*\n",
    "\n",
    "[2] Laura Gagliani, *Multi-technique investigations on a XIX century painting for the non-invasive characterization of visible and hidden materials and pictorial layers*\n",
    "\n",
    "I thank the two authors for the concession of using their data. I also thank their advisor, Piero Mandò, and the two co-advisors, Chiara Ruberto and Anna Mazzinghi.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5153645",
   "metadata": {},
   "source": [
    "## 2. Code Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71386d4e",
   "metadata": {},
   "source": [
    "### 2.1 Prerequisites\n",
    "\n",
    "#### Loading some useful libraries discussed in previous sessions of the hackathon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e25f503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OS is the built-in python library for exploring the file system\n",
    "## from Standard Python Library\n",
    "import os \n",
    "import math\n",
    "\n",
    "# Numpy, Pandas and matplotlib are must have libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6802a858",
   "metadata": {},
   "source": [
    "#### Image processing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936c6bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps \n",
    "from mpl_toolkits.axes_grid1 import ImageGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd180d48",
   "metadata": {},
   "source": [
    "#### Machine Learning libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5793ab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"-1\"\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a63230",
   "metadata": {},
   "source": [
    "### Ensure we are not using a GPU\n",
    "\n",
    "This application may be very intensive on GPU, which is a shared resource we don't want to saturate during the hackathon by mistake. \n",
    "So by default we ensure we are not running on GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e4e636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU usage; see https://www.tensorflow.org/guide/gpu\n",
    "print('Check on not using GPU through TensorFlow\\n')\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "assert len(gpus) == 0, \"Using GPU is disabled by default.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa1596a",
   "metadata": {},
   "source": [
    "### 2.2 Importing Dataset\n",
    "\n",
    "We have preprocessed the data into a csv file containing the raw XRF data and the corresponding RGB color for a large set of pixels.\n",
    "\n",
    "The dataset is a bit strange for a csv file because it associate a sequence of counts to a sequence representing the RGB color. Hence, some gymnastic is needed to parse the dataset into two numpy arrays representing the XRF raw data and the corresponding RGB codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001c78c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset: pick a path to .csv file\n",
    "path = './Training_dataset/dataset.csv'\n",
    "\n",
    "if not os.path.isfile(path):\n",
    "    try:\n",
    "        # wget\n",
    "        !wget -O {path} https://pandora.infn.it/public/b0a561/dl/dataset.csv\n",
    "    except:\n",
    "        !wget  https://pandora.infn.it/public/b0a561/dl/dataset.csv\n",
    "        path = path.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1140f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to import csv\n",
    "df_train_dataset = pd.read_csv(path, index_col=0)\n",
    "\n",
    "# Transform the strings representing the histograms into lists\n",
    "df_train_dataset['X'] = df_train_dataset['X'].str[1:-1].str.split(\" \")\n",
    "df_train_dataset['y'] = df_train_dataset['y'].str[1:-1].str.split(\" \")\n",
    "\n",
    "# Transfrom the lists into arrays, cleaning empty entries \n",
    "hists = []\n",
    "rgbs  = []\n",
    "from tqdm import tqdm \n",
    "for idx_, (X, y) in tqdm(df_train_dataset[['X', 'y']].iterrows(), total=len(df_train_dataset), desc=\"Loading...\"):\n",
    "    hists.append (np.array([float(v) for v in X if v not in [\"\\n\", \"\", \" \"]]))\n",
    "    rgbs.append (np.array([float(v) for v in y if v not in [\"\\n\", \"\", \" \"]]))   \n",
    "    \n",
    "hists = np.array(hists)\n",
    "rgbs = np.array(rgbs)\n",
    "del df_train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f57699",
   "metadata": {},
   "source": [
    "Let's explore the loaded dataset. \n",
    "We can start noticing that we imported floating point data, represented with 64bit each.\n",
    "Each histogram is composed of 500 ADC channels.\n",
    "As a sanity check we notice that the number of entries, 25974, is the same for both the raw data and the RGB labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e7854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"Type and shape of hists: {hists.dtype} {hists.shape}\")\n",
    "print (f\"Type and shape of rgb: {rgbs.dtype} {rgbs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1761a0",
   "metadata": {},
   "source": [
    "Before proceeding with machine learning stuff, we can have a look to the data to see how they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3296e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us see how these histograms look like using matplotlib-pyplot\n",
    "# Pick a random row number\n",
    "numb_entry_random = np.random.randint(len(hists))\n",
    "print('From row number ', numb_entry_random)\n",
    "step0 = hists[numb_entry_random]\n",
    "print('Len: ', len(step0))\n",
    "print('Max count: ', np.amax(np.array(step0)))\n",
    "print('Mean count: ', np.mean(np.array(step0)))\n",
    "plt.plot( step0 )\n",
    "plt.xlabel (\"ADC bin\")\n",
    "plt.ylabel (\"Counted photons\")\n",
    "plt.title (f\"RGB code: {rgbs[numb_entry_random][0]:.2f} ; {rgbs[numb_entry_random][1]:.2f} ; {rgbs[numb_entry_random][2]:.2f}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd40f96",
   "metadata": {},
   "source": [
    "#### 2.2.1 Training/Test split\n",
    "Here we define a custom function to split our dataset into test/train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848b7ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_dataset(X, y, percentage = 0.8):\n",
    "    # Shuffle the dataframe\n",
    "    assert len(X)==len(y), \"Inconsistent number of rows in X and y\"\n",
    "    \n",
    "    N = len(X)\n",
    "    indices = np.random.permutation (N)\n",
    "    \n",
    "    n = int(N*percentage)\n",
    "    train = indices[:n]\n",
    "    test = indices[n:]\n",
    "    \n",
    "    return X[train], y[train], X[test], y[test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d38508c",
   "metadata": {},
   "source": [
    "And then we call it and check the shape of the obtained splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c80e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = prepare_training_dataset(hists, rgbs, percentage = 0.8)\n",
    "\n",
    "print('Train elements: ', np.shape(X_train) )\n",
    "print('Test Elements: ', np.shape(X_test) )\n",
    "\n",
    "print('Train/test features: ', len(X_train[0]) )\n",
    "print(f'Shape X_train: {(np.shape(X_train)[0], np.shape(X_train)[1]) }')\n",
    "print(f'Shape y_train: {(np.shape(y_train)[0], np.shape(y_train)[1]) }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecae7d0",
   "metadata": {},
   "source": [
    "#### 2.2.2 Some statistics\n",
    "Finally, let's display some statistics on the dataset before going on: \n",
    "\n",
    "Let us look at\n",
    "1. Average RGB color in y_train and the average count per pixel, integrated over the \"energy of the photon\" in X_train, i.e. sumnming, pixel by pixel, over the ADC channel (which is in 1-to-1 correspondence with an energy \"level\")\n",
    "2. the plots of: \n",
    "- The distribution of total counts by pixels in X_train;\n",
    "- The distribution of counts per channel in X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c4056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Average RGB color\n",
    "print(f'Average RGB color: {y_train.mean(axis=0)}')\n",
    "\n",
    "### Average Count per pixel\n",
    "print(f'Average count per pixel (integrated over photon energy): {X_train.sum(axis=1).mean():.2f} photons')\n",
    "\n",
    "\n",
    "plt.hist (X_train.sum(axis=1), bins=np.linspace(0, 3000, 300))\n",
    "plt.xlabel (\"Number of photons\")\n",
    "plt.ylabel (\"Number of pixels\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist (X_train.flatten(), bins=np.linspace(0, 500, 501))\n",
    "plt.xlabel (\"Number of photons\")\n",
    "plt.ylabel (\"Number of ADC channels per pixel\")\n",
    "plt.yscale ('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d28ef73",
   "metadata": {},
   "source": [
    "We can also try to represent the whole dataset to highlight common features of the XRF histograms as a function of the pixel considered. We clearly see XRF peaks as vertical bands that change their relative intensity from pixel to pixel.\n",
    "\n",
    "The horizontal stripes are due to the procedure use to build the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7833d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure (dpi=150)\n",
    "plt.imshow(np.log(1 + hists), aspect='auto', cmap='binary')\n",
    "plt.xlabel (\"ADC channel\")\n",
    "plt.ylabel (\"Pixel ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1439865d",
   "metadata": {},
   "source": [
    "We clearly see that we have two highest peaks, around the channels 140 and 170; those are the $L_\\alpha$ and $L_\\beta$ lines of Lead (${}^{82}$Pb);\n",
    "\n",
    "This is clearly expected, since the most common pigment made with lead is the so-called *biacca* (in english, **white lead**); usually, painters prepare the canvas by covering the whole surface with the white paint, creating the *preparatory layer*.\n",
    "\n",
    "\n",
    "<a href=\"./Assets/Images/energie_X.pdf\"> Link to X-ray energy table.</a> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5560ec4",
   "metadata": {},
   "source": [
    "## 3. The Deep Neural Network Model\n",
    "\n",
    "The goal in this section is to employ a Deep Neural Network to *artificially recolor* an XRF image. This is an example of a possible application of Machine Learning to physical technologies. \n",
    "\n",
    "This is a case study, which goes in the direction of a *digital restoration* of a painted surface via the application of physical technologies. It could be relevant in case the case, e.g., of submerged paintings, i.e. a pictoric layer hidden \"behind\" the outermost, visible layer. \n",
    "\n",
    "<img src=\"./Assets/Images/DNN_all_hist.png\" alt=\"Drawing\" style=\"float: center; margin-top: 15px; width: 75%\"/>\n",
    "\n",
    "But, first: what is an Artificial Deep Neural Network?\n",
    "\n",
    "<img src=\"./Assets/Images/XKCD_2.png\" alt=\"Drawing\" style=\"float: center; margin-top: 15px; width: 35%\"/>\n",
    "\n",
    "### 3.1. Basics of Artificial Neural Networks\n",
    "\n",
    "Artificial neural networks (ANN or NN) are computing systems that are inspired by, but not identical to, biological neural networks that constitute animal brains. Such systems learn to perform tasks by considering examples, generally without being programmed with task-specific rules.\n",
    "\n",
    "A NN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. The basic example is the perceptron [1]. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron that receives a signal then processes it and can signal neurons connected to it [2].\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg\" alt=\"Drawing\" style=\"float: center; margin-top: 15px\"/>\n",
    "\n",
    "In ANN implementations, the \"signal\" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times. \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./Assets/Images/Dense.gif\" alt=\"Drawing\" style=\"float: center; margin-top: 15px\"/>\n",
    "\n",
    "\n",
    "Stated differently, Artificial Neural Networks are complex non-linear parametric interpolating functions, which maps the vectorial space (actually, a differential manifod) $\\mathcal{M}$ (usually locally isomorphic to $\\mathbb{R}^m$ of the *features* vector, into the vectorial space (actually, a differential manifod) $\\mathcal{N}$ (usually locally isomorphic to $\\mathbb{R}^n$ of the *labels* vector:\n",
    "$$\\mathrm{ANN} : \\mathcal{M} \\mapsto \\mathcal{N}\\,, $$\n",
    "where $\\mathrm{ANN}$ is realized as a non-linear function $F$, which depends on a (huge) set of parameters, called *trainable parameters*, $\\{\\mathbf{a}\\}$, \n",
    "\n",
    "$$F_{\\mathbf{a}} (x) = y \\,,$$\n",
    "\n",
    "where $x \\in \\mathcal{M}$, $y \\in \\mathcal{N}$ and, crucially, $F_{\\mathbf{a}}$ is a long composition of non-linear functions, called **layers**, $\\lambda^{(i)}_{\\mathbf{a_i} }$, with $i=0, \\ldots, N$, where $N$ denotes the depth of the Neural Network. If $N$ is large, it is a *deep* neural network:\n",
    "\n",
    "$$F_{\\mathbf{a}} = \\lambda^{(N)}_{\\mathbf{a_N} } \\circ \\lambda^{(N-1)}_{\\mathbf{a_{N-1}} } \\circ \\cdots \\circ \\lambda^{(0)}_{\\mathbf{a_0} } \\, .$$\n",
    "\n",
    "In this way, the input $x$ is *feed-forward* the composing functions, until the target space. \n",
    "\n",
    "The way the compising functions $\\lambda^{(i)}_{\\mathbf{a_i} }$, or *layers*, are defined and assembled, defines the *architecture* of the Neural Network; In this section and in the following, we will explore two of them: the **dense, multi-layered perceptron**, and the **convolutional neural network**. \n",
    "\n",
    "The **training** of the ANN is the procedure by which the set of interpolating paramters $\\{\\mathbf{a}\\}$ is fixed, by minimising a certain error function, called *cost function* $\\mathcal{L}$, which computes $\\mathcal{L}[F_{\\mathbf{a}}(x), y]$, some kind of distance between the predicted output, $F_{\\mathbf{a}}(x) = \\bar y$, and the actual output, $y$.\n",
    "\n",
    "Since $F_{\\mathbf{a}}$ is a composition, thanks to the chain rule of differential analysis, it is easy to update the parameters warking backwards, from the outer layer to the inner one; this procedure is called *back-propagation*; in the following we will discuss it in more depth. \n",
    "\n",
    "<img src=\"./Assets/Images/XKCD_3.png\" alt=\"Drawing\" style=\"float: center; margin-top: 15px; width: 35%\"/>\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "[1] https://en.wikipedia.org/wiki/Perceptron\n",
    "\n",
    "[2] https://en.wikipedia.org/wiki/Artificial_neural_network\n",
    "\n",
    "[3] https://www.saedsayad.com/artificial_neural_network_bkp.htm\n",
    "\n",
    "[4] https://iamtrask.github.io/2015/07/12/basic-python-network/\n",
    "\n",
    "[5] https://www.freecodecamp.org/news/building-a-neural-network-from-scratch/\n",
    "\n",
    "[6] https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e5bfea",
   "metadata": {},
   "source": [
    "### 3.2 Perceptron\n",
    "\n",
    "The perceptron is a basic function that mimics the human neuron. It receives $n$ inputs, associated to the dendrites inputs to the neuron. Each dendrite, due to *lernging*, is weighted by a number that signals its input relevance for the neuron [1]. \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/a9/Complete_neuron_cell_diagram_en.svg\" alt=\"Drawing\" style=\"float: center; margin-top: 15px\"/>\n",
    "\n",
    "The signal is thus elaborated and passed through the *axon* to others neurons [2]; actually, the neurons *fires* the signal only if the elaborated inputs have surpassed a certain threshold; this is a spiking neuron [3].\n",
    "\n",
    "The perceptron wants to mimic it. Receinving a vector (i.e. array) $x_i$ of signals, where $i$ stands for the $i$-th dendrites, it weights each of them by a vector of weights $W_i$. It adds also a *bias* to remove near-zero issues (the bias shifts the decision boundary away from the origin and does not depend on any input value), i.e.\n",
    "$$x_i \\mapsto W_i{}^j x_j + b_i$$\n",
    "\n",
    "Also, the perceptron ignite an output through an activation function that is usually a *sigmoid* function [4]\n",
    "$$ \\varphi (x) = \\frac{1}{1+e^{-x}} \\,,$$\n",
    "or a *Rectified Linear Unit* (ReLU)\n",
    "$$\\varphi(x) = \\mathrm{max}[0, x] \\,,$$\n",
    "\n",
    "<div style=\"margin-top: 3em; margin-bottom: 30em; width:100%\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg\"  alt=\"Drawing\" style=\"float: left; width: 50%; height: 300px\"/>\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/6/6c/Rectifier_and_softplus_functions.svg\"  alt=\"Drawing\" style=\"float: right; width: 50%; height: 300px\"/>  \n",
    "</div>\n",
    "\n",
    "\n",
    "so that the output $O(x_i)$ of the perceptron is given by\n",
    "\n",
    "$$O (x_i) = \\varphi \\left( \\Sigma_{i=1}^{n} W_i{}^j \\, x_j + b_i   \\right) ,$$\n",
    "\n",
    "or, in vectorial representation \n",
    "\n",
    "$$O(x) = \\varphi \\left(\\mathbf{W}^T \\cdot \\mathbf{x} + b   \\right) $$\n",
    "\n",
    "<img src=\"./Assets/Images/Perceptron.gif\" alt=\"Drawing\" style=\"float: center; margin-top: 15px\"/>\n",
    "\n",
    "-----\n",
    "[1] https://en.wikipedia.org/wiki/Dendrite\n",
    "\n",
    "[2] https://en.wikipedia.org/wiki/Neuron\n",
    "\n",
    "[3] https://icwww.epfl.ch/~gerstner/BUCH.html\n",
    "\n",
    "[4] https://en.wikipedia.org/wiki/Rectifier_(neural_networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f622bac4",
   "metadata": {},
   "source": [
    "### 3.3. Multilayer perceptrons: the Dense Layer\n",
    "\n",
    "A standard Artificial Neural Network will be made of multiple layers:\n",
    "1. An **Input Layer**, that pass the features to the NN\n",
    "2. An arbitrary number of **Hidden Layers**, containing an arbitrary number of neurons for each layer, that receives the inputs and elaborate them. We will introduce Hidden Layers with ReLU activator, since in the *hidden* part of the NN we don't need the output to be contained in the $[0,1]$ range. \n",
    "3. An **Output Layer**: these layers contains a number of neurons equal to the number of possible labels we want to have a prediction to (so, in our case, 3: R, G, B); usually, this is because the output of the NN is thus a vector whose dimension is the same as the cardinality of the set of labels, and its entries are the *probability* for each label for the element whose feateures we have passed to the NN. So, a specific activation function is usually employed, dubbed *softmax* (see https://keras.io/api/layers/activations/), which is a function which converts a real vector to a vector of categorical probabilities so that the elements of the output vector are in range $[0, 1]$ and sum to $1$.\n",
    "\n",
    "Instead, we will use a sigmoid activator to the Output layer, so we squeeze each perceptron's output between 0 and 1, without imposing any relation among the R,G,B labels of the final output.\n",
    "\n",
    "<img src=\"./Assets/Images/Dense.gif\" alt=\"Drawing\" style=\"float: center; margin-top: 15px\"/>\n",
    "\n",
    "A (hidden) layer is made by $p$ perceptrons in parrallel, each of which receives the same input (that is the output of the previous layer). The layer will have $p$ perceptrons, labelled by $a=1,\\ldots ,p$. Thus the output of the whole layer is a matrix ${O}^{(a)}$ given by\n",
    "$$O^{(a)} (x_i) = \\varphi \\left( {}^{(a)}W_i{}^j \\, x_j + {}^{(a)}b_i   \\right) ,$$\n",
    "\n",
    "If we start having multiple layers, let us say $N$, we have an additional label $A=1, \\ldots, N$ so that \n",
    "\n",
    "$$ O^{(a_A)} (x_i) = \\varphi_{(A)} \\left( {}^{(a_A)} W_i{}^j \\, x_j + {}^{(a_A)} b_i   \\right) ,$$\n",
    "\n",
    "where we have inserted the label also to the activation function that may depend on which layer we are considering!\n",
    "\n",
    "In the notation introduced at the beginning of §3.1, we may read the Artificial Neural Network of $N$ of the above layers as\n",
    "\n",
    "$$F_{\\mathbf{a}} =  O^{(a_N)} \\circ  O^{(a_{N-1})} \\circ \\cdots \\circ  O^{(a_0)} \\,.  $$\n",
    "\n",
    "### 3.3.1. The *Keras* Dense Layer\n",
    "In Keras, a Dense layer is implemented by calling the *Dense* class: \n",
    "```\n",
    "tf.keras.layers.Dense(\n",
    "    units,\n",
    "    activation=None,\n",
    "    use_bias=True,\n",
    "    kernel_initializer=\"glorot_uniform\",\n",
    "    bias_initializer=\"zeros\",\n",
    "    kernel_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    **kwargs\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "See https://keras.io/api/layers/core_layers/dense/\n",
    "\n",
    "Example: \n",
    "```\n",
    ">>> # Create a `Sequential` model and add a Dense layer as the first layer.  \n",
    ">>> model = tf.keras.models.Sequential()\n",
    ">>> model.add(tf.keras.Input(shape=(16,)))\n",
    ">>> model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    ">>> # Now the model will take as input arrays of shape (None, 16)  \n",
    ">>> # and output arrays of shape (None, 32).  \n",
    ">>> # Note that after the first layer, you don't need to specify  \n",
    ">>> # the size of the input anymore:  \n",
    ">>> model.add(tf.keras.layers.Dense(32))\n",
    ">>> model.output_shape\n",
    "(None, 32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf734821",
   "metadata": {},
   "source": [
    "### 3.4. Training: Backpropagation and Gradient descent [extra]\n",
    "\n",
    "For adjusting the trainable parameters $\\{W\\}$ and $\\{b\\}$ during the training process, we need to implement the *backpropagation* algorithm. We want to minimize a certain **cost function**, e.g.\n",
    "$$\\mathcal{L}(y,\\bar{y})=|y-\\bar{y}|^2\\,,$$\n",
    "where $y$ is the output of the output layer while $\\bar{y}$ is the actual label; in order to do so, we start the *gradient descent*, which means that we see the cost function as a function of the trainable parameters $\\mathbf{W}$ such as $\\{W\\}$ and $\\{b\\}$, we compute the gradient - gradient that can be seen as the slope of the multidimensional graph [3] - and we subtract it from the randomly initialized set, as\n",
    "$$\\mathbf{W}'_n =  \\mathbf{W}_n - \\eta \\nabla \\mathcal{L}(\\mathbf{W}_n)  \\,,$$\n",
    "moving thus towards the *optima*, or global minimum, of the cost function. In the formula above $\\eta$ is the **learning rate** of the ANN [4] \n",
    "\n",
    "\n",
    "This is pictorially represented in the following gif [5]:\n",
    "\n",
    "<img src=\"./Assets/Images/gradient_descent.gif\" alt=\"Drawing\" style=\"float: center; margin-top: 15px\"/>\n",
    "\n",
    "Explicitly, the $\\alpha$-th hidden layer is defined by a *matrix* $W_{\\alpha}^{i_{\\alpha} i_{\\alpha-1}}$ and a vector $b_\\alpha^{i_\\alpha}$, and its output is\n",
    "$$z_\\alpha^{i_\\alpha} = \\varphi ( W_{\\alpha}^{i_{\\alpha} i_{\\alpha-1}} \\, z^{i_{\\alpha-1}}_{\\alpha-1} + b_\\alpha^{i_\\alpha} ) \\equiv \\varphi (\\zeta^{i_\\alpha}_\\alpha)  .$$\n",
    "\n",
    "Notice that we have used the greek letter to define the value of the output before employing the activation function, i.e. the greek letters refer to the output of the *soma*, while the latin letter are the output of the *axon*. \n",
    "\n",
    "**NB:** Notice that the activation function is crucial in order to make the DNN a *non-linear* interpolating function.\n",
    "\n",
    "We need thus to compute the gradient $\\nabla \\mathcal{L}$, where each element is $\\partial \\mathcal{L} / \\partial z_\\alpha^{i_\\alpha}$; by applying the chain rule repeatedly we get \n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial z_{\\alpha}^{i_\\alpha}} = \\frac{\\partial \\mathcal{L}}{\\partial z_{\\alpha+1}^{i_{\\alpha+1}} } \\cdot \\frac{\\partial z_{\\alpha+1}^{i_{\\alpha+1}} }{\\partial z_{\\alpha}^{i_\\alpha}} = \\frac{\\partial \\mathcal{L}}{\\partial z_{\\alpha+1}^{i_{\\alpha+1}} } \\cdot W_{\\alpha}^{i_{\\alpha+1} i_\\alpha} \\varphi' (z_{\\alpha}^{i_\\alpha}) \\,.$$\n",
    "\n",
    "Define **the error** for the $\\alpha$-th layer as \n",
    "$$\\delta_\\alpha^{i_\\alpha} = \\frac{\\partial \\mathcal{L}}{\\partial z_{\\alpha}^{i_\\alpha}}$$\n",
    "we see that, working *backwards*, we may obtain it from the previously computed $(\\alpha+1)$-th error via [6]\n",
    "$$\\delta_{\\alpha}^{i_\\alpha}  = {\\large \\Sigma}_{{i_\\alpha}}  \\delta_{\\alpha+1}^{i_\\alpha+1}  \\, W_{\\alpha}^{i_{\\alpha+1} i_\\alpha} \\, \\varphi' (z_{\\alpha}^{i_\\alpha}) \\,.$$\n",
    "\n",
    "This means that working iteratively, starting from the Output layer, we may easly compute the errors. \n",
    "\n",
    "Now, the goal of this whole procedure is to go towards a global minimum (or *optima*) of $\\mathcal{L}$, seen as a function of the parametes $W_\\alpha^{i_{\\alpha+1} i_\\alpha}$ and $b_\\alpha^{i_{\\alpha}}$; \n",
    "\n",
    "So, after having shifted the parameters as dictated by our gradient descent equation, we need to compute the variation of the cost function under the variation of parameters, to see if we have moved toward a decreasing cost-function path; in order to do so, we apply again the chain rule\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial W_\\alpha^{i_{\\alpha+1} i_\\alpha} } = \\frac{\\partial \\mathcal{L}}{\\partial z_{\\alpha}^{i_\\alpha}}  \\cdot \\frac{\\partial z_\\alpha^{i_\\alpha} }{\\partial W_\\alpha^{i_{\\alpha+1} i_\\alpha} }  = \\delta_\\alpha^{i_\\alpha} \\cdot z_{\\alpha-1}^{i_\\alpha-1} \\varphi'(\\zeta_\\alpha^{i_\\alpha} ) ,$$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial b_\\alpha^{i_{\\alpha}} } =  \\frac{\\partial \\mathcal{L}}{\\partial z_{\\alpha}^{i_\\alpha}}  \\cdot \\frac{\\partial z_\\alpha^{i_\\alpha} }{\\partial b_\\alpha^{i_{\\alpha}} } =  \\delta_\\alpha^{i_\\alpha} \\cdot  \\varphi'(\\zeta_\\alpha^{i_\\alpha} ) . $$\n",
    "\n",
    "<img src=\"./Assets/Images/training_back.gif\" alt=\"Drawing\" style=\"float: center; margin-top: 30px; margin-bottom: 2em; width: 60%\"/>\n",
    "\n",
    "\n",
    "#### 3.4.1. The algorithm\n",
    "We now recap what we have shown: \n",
    "* **Input**: Set up the inputs $z_0^{i_0}$;\n",
    "* **Feed Forward**: Computes the output of the $\\alpha$-th layer $z_\\alpha^{i_\\alpha}$ via the formula \n",
    "$$z_\\alpha^{i_\\alpha} = \\varphi ( W_{\\alpha}^{i_{\\alpha} i_{\\alpha-1}} \\, z^{i_{\\alpha-1}}_{\\alpha-1} + b_\\alpha^{i_\\alpha} ) \\equiv \\varphi (\\zeta^{i_\\alpha}_\\alpha) ,$$\n",
    "up to the output layer.\n",
    "* **Compute the errors**: compute the error of the last layer via the formula \n",
    "$$\\delta^N_{i_N} = \\frac{\\partial \\mathcal{L}}{\\partial z_N^{i_N} } \\,.$$\n",
    "\n",
    "*  **Backpropagate the Error**: For each layer $\\alpha= N-1, \\ldots, 2$ compute \n",
    "$$\\delta_{\\alpha}^{i_\\alpha}  = {\\large \\Sigma}_{{i_\\alpha}}  \\delta_{\\alpha+1}^{i_\\alpha+1}  \\, W_{\\alpha}^{i_{\\alpha+1} i_\\alpha} \\, \\varphi' (z_{\\alpha}^{i_\\alpha}) \\,.$$\n",
    "* **Output**: the gradient of the cost function is now computed by the formulae\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial W_\\alpha^{i_{\\alpha+1} i_\\alpha} } = \\delta_\\alpha^{i_\\alpha} \\cdot z_{\\alpha-1}^{i_\\alpha-1} \\varphi'(\\zeta_\\alpha^{i_\\alpha} ) ,$$\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial b_\\alpha^{i_{\\alpha}} } =  \\delta_\\alpha^{i_\\alpha} \\cdot  \\varphi'(\\zeta_\\alpha^{i_\\alpha} ) . $$\n",
    "\n",
    "So the Deep Neural Network is an algorithm that will perform the previous task in the followin manner: \n",
    "1. **Input a set of training examples** or *batches*.\n",
    "2. **For each training batch** $x_{i_0}$: set the corresponding input activation $z_1^{i_1} = \\varphi(W_1^{i_1 i_0} x_{i_0} + b_1^{i_1})$, and then perform the following steps: \n",
    "    1. **Feedforward**: $\\forall \\alpha=2,\\ldots N$ compute \n",
    "    $$z_\\alpha^{i_\\alpha} = \\varphi ( W_{\\alpha}^{i_{\\alpha} i_{\\alpha-1}} \\, z^{i_{\\alpha-1}}_{\\alpha-1} + b_\\alpha^{i_\\alpha} ) \\equiv \\varphi (\\zeta^{i_\\alpha}_\\alpha) ,$$\n",
    "    $$y = f( W_{Out}^{i_n} z_n^{i_n} + b_{Out} ) $$\n",
    "    2. **Compute the outer layer Error**: Compute the vector\n",
    "    $$\\delta^{Out} = \\frac{\\partial \\mathcal{L}}{\\partial z_{Out} } \\cdot f' (z_{Out}) .$$\n",
    "    3. **Backpropagate the error**: for each $\\alpha = N, N-1, \\ldots 2$ compute \n",
    "    $$\\delta_{\\alpha}^{i_\\alpha}  = {\\large \\Sigma}_{{i_\\alpha}}  \\delta_{\\alpha+1}^{i_\\alpha+1}  \\, W_{\\alpha}^{i_{\\alpha+1} i_\\alpha} \\, \\varphi' (z_{\\alpha}^{i_\\alpha}) \\,.$$\n",
    "3. **Gradient Descent**: for each $\\alpha = N, N-1, \\ldots 2$ update the weights $\\{ W_\\alpha^{i_\\alpha i_{\\alpha-1} } \\,, b_\\alpha^{i_\\alpha} \\}$ via the rule\n",
    "$$W_\\alpha^{i_\\alpha i_{\\alpha-1} } \\mapsto W_\\alpha^{i_\\alpha i_{\\alpha-1} } - \\eta {\\large \\Sigma}_{i_\\alpha} \\delta_\\alpha^{i_\\alpha} \\cdot z_{\\alpha-1}^{i_{\\alpha-1}} \\,,$$\n",
    "$$b_\\alpha^{i_\\alpha  } \\mapsto b_\\alpha^{i_\\alpha  } - \\eta {\\large \\Sigma}_{i_\\alpha} \\delta_\\alpha^{i_\\alpha} \\,.$$\n",
    "\n",
    "<img src=\"./Assets/Images/Grad_desc.gif\" alt=\"Drawing\" style=\"float: center; margin-top: 15px; margin-bottom: 5em; width: 49%\"/>\n",
    "\n",
    "-------\n",
    "[1] http://cs231n.github.io/neural-networks-1/\n",
    "\n",
    "[2] https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/\n",
    "\n",
    "[3] https://www.nature.com/articles/323533a0 (original paper) https://en.wikipedia.org/wiki/Gradient_descent (Wiki article)\n",
    "\n",
    "[4] https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n",
    "\n",
    "[5] https://hackernoon.com/gradient-descent-aynk-7cbe95a778da\n",
    "\n",
    "[6] http://neuralnetworksanddeeplearning.com/chap2.html\n",
    "\n",
    "[7] https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/\n",
    "\n",
    "My simple notebook on how to implement a DNN from scratch:\n",
    "https://www.kaggle.com/androbomb/simple-nn-with-python-multi-layer-perceptron\n",
    "\n",
    "A very nice set of introductory youtube lectures with amazing gifs (that I used): https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8c4f69",
   "metadata": {},
   "source": [
    "### 3.5: Coding the Deep Neural Network\n",
    "\n",
    "We will build now a simple network wich employs Dense layers to interpret histograms of XRF counts and returning an RGB vector\n",
    "\n",
    "<img src=\"./Assets/Images/DNN_single_hist.png\" alt=\"Drawing\" style=\"float: center; margin-top: 15px; width: 75%\"/>\n",
    "\n",
    "#### 3.5.1. Defining the model\n",
    "\n",
    "Here we define a simple, three-layer deep neural network using Tensorflow and Keras:\n",
    "\n",
    "https://www.tensorflow.org/\n",
    "\n",
    "https://keras.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce017e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "# Import relevant Keras classes\n",
    "\n",
    "# Input layer \n",
    "from keras.layers import InputLayer, Input\n",
    "# The Dense Layer\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Sequential class for instantiating the Model\n",
    "from keras.models import Sequential\n",
    "from keras import Model # If we want to use the Keras API\n",
    "\n",
    "# To get the Activation functions\n",
    "from keras.layers import Activation\n",
    "\n",
    "\n",
    "#######################################################################################################################\n",
    "# Input-Output defs\n",
    "\n",
    "input_shape = np.shape(X_train)[1]   # Input layer takes [number of vectors, size of vectors, features-per-vector] = [#size_dataset, 1000, 1]\n",
    "num_classes = 3 # RGB\n",
    "\n",
    "CHANNEL_DEPTH = 500   # Each histograms has 500 channels\n",
    "num_sensors = 1       # We have only 1 histogram per data\n",
    "\n",
    "#######################################################################################################################\n",
    "# Defining the model\n",
    "#######################################################################################################################\n",
    "\n",
    "# Instantiate it\n",
    "model_m = Sequential()\n",
    "\n",
    "#############\n",
    "# input\n",
    "model_m.add(Input(shape=(input_shape, )))\n",
    "\n",
    "#############\n",
    "# Hidden Layers\n",
    "\n",
    "# 1°\n",
    "model_m.add(Dense(250, \n",
    "                 activation='relu'))\n",
    "# 2°\n",
    "model_m.add(Dense(120, \n",
    "                 activation='relu'))\n",
    "# 3° \n",
    "model_m.add(Dense(60, \n",
    "                 activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "model_m.add(Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "###########\n",
    "# Summary\n",
    "print(model_m.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fd8d45",
   "metadata": {},
   "source": [
    "We use now graphviz ( https://graphviz.org/ ) to graphically illustrate the model we have defined: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef74e7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.utils.plot_model(model_m, show_shapes=True, dpi=76, to_file=\"dense_lecture.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cf3275",
   "metadata": {},
   "source": [
    "#### 3.5.2. Training the model\n",
    "\n",
    "We have to define the function computing the loss. \n",
    "\n",
    "Usually, a *Mean Squared Error* function is employed (the one shown before as example):\n",
    "$$\\mathcal{L}(y,\\bar{y})=|y-\\bar{y}|^2\\,.$$\n",
    "\n",
    "Here instead, we use the *binary cross-entropy* function (https://keras.io/api/losses/probabilistic_losses/#binarycrossentropy-class ; for other loss functions, see https://keras.io/api/losses/); it is defined via\n",
    "$$\\mathcal{L}(y,\\bar{y})= - \\sum_{i=R,G,B} y_i \\log \\bar{y}_i  \\,.$$\n",
    "\n",
    "This is useful for us, since $\\mathcal{L} \\in [0,1]$. \n",
    "\n",
    "Also, we will have to fix the *batch size* and the number of *epochs* for the training. \n",
    "\n",
    "**Batch size**:\n",
    "\n",
    "The batch size is a hyperparameter that defines the number of samples to work through before updating the internal model parameters. Think of a batch as a for-loop iterating over one or more samples and making predictions. At the end of the batch, the predictions are compared to the expected output variables and an error is calculated. From this error, the update algorithm is used to improve the model, e.g. move down along the error gradient. [1]\n",
    "\n",
    "Usually, a set of names for the descent algorithm is employed, in relation to the batch size:\n",
    "\n",
    "- Batch Gradient Descent. Batch Size = Size of Training Set\n",
    "- Stochastic Gradient Descent. Batch Size = 1\n",
    "- Mini-Batch Gradient Descent. 1 < Batch Size < Size of Training Set\n",
    "\n",
    "**Epochs**:\n",
    "\n",
    "The number of epochs is a hyperparameter that defines the number times that the learning algorithm will work through the entire training dataset. One epoch means that each sample in the training dataset has had an opportunity to update the internal model parameters. An epoch is comprised of one or more batches.\n",
    "\n",
    "\n",
    "Keras also allows us to use some callbacks while training, which will not use them now. (see https://keras.io/api/callbacks/)\n",
    "\n",
    "##### 3.5.2.1: Optimazer: ADAM Algorithm  [EXTRA]\n",
    "\n",
    "Here we instantiate the model; we will use the ADAM optimizer [2]; The Adam optimization algorithm is an extension to stochastic gradient descen; the latter  maintains a single learning rate for all weight updates and the learning rate does not change during training; In ADAM, instead, A learning rate is maintained for each network weight (parameter) and separately adapted as learning unfolds.\n",
    "\n",
    "**ADAM Algorithm**\n",
    "\n",
    "Adam, is an algorithm for stochastic optimization. $g_t^2$ indicates the elementwise square $g_t \\otimes g_t$. Good default settings for the tested machine learning problems are $\\alpha=0.001$, $\\beta_1=0.9$, $β_2=0.999$ and $\\epsilon=10^{−8}$. All operations on vectors are element-wise. The pseudo-code implementation of the ADAM algorithm is the following: \n",
    "\n",
    "\n",
    "**Require**: $\\alpha$: Stepsize\n",
    "\n",
    "**Require**: $\\beta_1, \\beta_2 \\in [0,1)$: Exponential decay rates for the moment estimates\n",
    "\n",
    "**Require**: $f(\\theta)$: Stochastic objective function with parameters $\\theta$\n",
    "\n",
    "**Require**: $\\theta_0$: Initial parameter vector\n",
    "\n",
    "- $m_0 \\leftarrow 0$ (Initialize 1st moment vector)\n",
    "- $v_0 \\leftarrow 0$ (Initialize 2nd moment vector)\n",
    "- $t \\leftarrow 0$ (Initialize timestep)\n",
    "\n",
    "**while** $\\theta_t$ not converged **do**\n",
    "- $t \\leftarrow t + 1$   \n",
    "- $g_t \\leftarrow ∇_\\theta f_t(\\theta_{t−1})$ (Get gradients w.r.t. stochastic objective at timestep t)   \n",
    "- $m_t \\leftarrow \\beta_1 \\cdot m_{t − 1} + (1 − \\beta_1) \\cdot g_t$ (Update biased first moment estimate)\n",
    "- $v_t \\leftarrow \\beta_2 \\cdot v_{t − 1} + (1 − \\beta_2) \\cdot g_t^2$ (Update biased second raw moment estimate)\n",
    "- $\\hat{m}_t \\leftarrow m_t/(1 − \\beta_t^2)$ (Compute bias-corrected first moment estimate)\n",
    "- $\\hat{v}_t \\leftarrow v_t/(1 − \\beta_t^2)$ (Compute bias-corrected second raw moment estimate)   \n",
    "- $\\theta_t \\leftarrow \\theta_{t − 1} − \\alpha \\cdot \\hat{m}_t/(\\sqrt{\\hat{v}_t} + \\epsilon)$ (Update parameters)\n",
    "\n",
    "    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **end while** return $\\theta_t$ (Resulting parameters)\n",
    "\n",
    "\n",
    "--------\n",
    "[1] https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/\n",
    "\n",
    "[2] https://arxiv.org/abs/1412.6980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421875db",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Training - prerequisites\n",
    "loss_used = 'binary_crossentropy'\n",
    "\n",
    "model_m.compile(loss=loss_used,\n",
    "                 optimizer='adam',\n",
    "                 )\n",
    "# We need to pick the size of the batch used for the training\n",
    "BATCH_SIZE = 200 \n",
    "# And the number of epochs on which we train the dataset\n",
    "EPOCHS = 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e8d7dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "starting_time = datetime.datetime.now() \n",
    "\n",
    "print('\\nStart training model: \\n')\n",
    "\n",
    "##########################################################\n",
    "# TRAINING\n",
    "\n",
    "history = model_m.fit(X_train,\n",
    "                       y_train,\n",
    "                       batch_size=BATCH_SIZE,\n",
    "                       epochs=EPOCHS,\n",
    "                       #callbacks=callbacks_list,\n",
    "                       validation_data=(X_test, y_test),\n",
    "                       verbose=1)\n",
    "\n",
    "print('\\n\\nTraining done...\\n')\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "total_training_time = now-starting_time\n",
    "print('Total training time: ', total_training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c41ea9",
   "metadata": {},
   "source": [
    "#### 3.5.3. Plot the training history and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61415a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.title('Model loss', fontsize=20,  pad=10)\n",
    "\n",
    "plt.ylabel('loss', fontsize=18)\n",
    "plt.xlabel('epoch', fontsize=18)\n",
    "\n",
    "plt.legend(['loss', 'val_loss'], loc='upper right', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab62364",
   "metadata": {},
   "source": [
    "#### 3.5.4 Saving the model\n",
    "\n",
    "We save the model to both HDF and JSON. \n",
    "The former is a compact data format which encapsulate both the architecture and the weights of the dataset, while the latter is text-based, human-readable data format and is often used to describe the architecture only. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c839dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# SAVING\n",
    "\n",
    "# serialize weights to HDF5\n",
    "time_string = now.strftime(\"%Y-%m-%d-%H-%M-%S\")    \n",
    "name_to_save = 'model_Dense_w_batchsize' + str(BATCH_SIZE) + '_epochs' + str(EPOCHS) +  '_v' + time_string \n",
    "path_to_model = 'Model_data/' +  name_to_save + '.h5'\n",
    "model_m.save(path_to_model)\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30b2a68",
   "metadata": {},
   "source": [
    "### 3.6. Predictions of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3a6906",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model_m.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d404d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iFirst in range(0, 500, 50):\n",
    "    plt.figure(figsize=(12,1), dpi = 120)\n",
    "    plt.scatter (np.arange(50), np.ones(50), c=y_test[iFirst:iFirst+50], s=150)\n",
    "    plt.scatter (np.arange(50), np.zeros(50), c=prediction[iFirst:iFirst+50], s=150)\n",
    "    plt.xlim (-8, None)\n",
    "    plt.ylim (-1, 2)\n",
    "    plt.xticks ([])\n",
    "    plt.yticks ([])\n",
    "\n",
    "    \n",
    "\n",
    "    plt.text (-6, 1, \"True color\")\n",
    "    plt.text (-6, 0, \"Predicted\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576545f1",
   "metadata": {},
   "source": [
    "<img src=\"./Rawdata+RGB/putto_VISscalato.png\" alt=\"Drawing\" style=\"float: left; margin-bottom: 18em; width: 25%\"/>\n",
    "<img src=\"./Rawdata+RGB/ragtriste_VISscalato.png\" alt=\"Drawing\" style=\"float: right;  margin-bottom: 4em; width: 25%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92552e4",
   "metadata": {},
   "source": [
    "### 3.7. Apply the saved model to the Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b687b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Rawdata+RGB/Putto_Area1_fiori\"\n",
    "\n",
    "if not os.path.isfile(filename+\".npz\"):\n",
    "    try:\n",
    "        # wget\n",
    "        !wget -O {filename}.npz https://pandora.infn.it/public/141790/dl/Putto_Area1_fiori.npz\n",
    "    except:\n",
    "        !wget  https://pandora.infn.it/public/141790/dl/Putto_Area1_fiori.npz\n",
    "        filename = filename.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad4021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "\n",
    "xrf_img = np.load(f\"{filename}.npz\")['img']\n",
    "print (f\"Shape of the loaded XRF image: {xrf_img.shape}\")\n",
    "true_img = np.asarray(PIL.Image.open(f\"{filename}.jpg\"))\n",
    "print (f\"Shape of the loaded RGB image (true): {true_img.shape}\")\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "plt.title (\"Flowers (true)\")\n",
    "plt.imshow (true_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980461a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels = xrf_img.reshape ((-1, 500))\n",
    "rgb_pred = model_m.predict (pixels).reshape (xrf_img[:,:,:3].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb1ea34",
   "metadata": {},
   "source": [
    "#### 3.7.1. Show recoloring results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4b037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16), dpi=100)\n",
    "plt.subplot(221)\n",
    "plt.title (\"Flowers (true)\")\n",
    "plt.imshow (true_img)\n",
    "plt.subplot(222)\n",
    "plt.title (\"Flowers (reconstructed)\")\n",
    "plt.imshow (rgb_pred)\n",
    "\n",
    "plt.tight_layout()\n",
    "# Make space for title\n",
    "plt.subplots_adjust(top=0.85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a8e7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "recolored_image = rgb_pred[:,::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eb1e28",
   "metadata": {},
   "source": [
    "### 3.8 DNN: conclusion\n",
    "\n",
    "In this section we have seen how a Deep Neural Network works and how to implement it in Keras, and we applied it to an XRF raw data. \n",
    "\n",
    "Let us compute few errors on the recoloring procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35acb3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function computes the mean squared error and root mean squared error between two images \n",
    "def distance_in_RGB(pred, test):\n",
    "    import math\n",
    "    if (pred.shape == test.shape):\n",
    "        distance_tot =  np.sqrt( np.square(pred - test).mean() ) \n",
    "        \n",
    "        # Now we compute color by color\n",
    "        distance_col = []\n",
    "        for col in range(0, 3):\n",
    "            pred_col = pred[:, :, col] # pick only the color channel matrix\n",
    "            test_col = test[:, :, col] # pick only the color channel matrix\n",
    "            \n",
    "            distance_col.append( np.sqrt( np.square(pred_col - test_col).mean() ) )\n",
    "                        \n",
    "        print(f'Root Mean squared error: {distance_tot}\\n')    \n",
    "        print(f'Root Mean squared error by color: {distance_col}\\n')\n",
    "        \n",
    "        print(f'In 255 scale: \\n')\n",
    "        print(f'Root Mean squared error: {math.floor(distance_tot*255)}\\n')    \n",
    "        print(f'Root Mean squared error by color: { np.floor( np.array(distance_col)*255 ) }\\n')\n",
    "        \n",
    "        return distance_tot, np.array(distance_col)\n",
    "    else: \n",
    "        print('something wrong')\n",
    "        return -1, np.array([-1, -1, -1])\n",
    "        \n",
    "# Now compute the Cross entropy\n",
    "def compute_cross_entropy(pred, test, epsilon=1e-12):\n",
    "    \"\"\"\n",
    "    Computes cross entropy between targets (encoded as one-hot vectors)\n",
    "    and predictions. \n",
    "    Input: predictions (N, k, 3) ndarray\n",
    "           targets (N, k, 3) ndarray        \n",
    "    Returns: scalar\n",
    "    \"\"\"\n",
    "    if (pred.shape == test.shape):\n",
    "        # We clips the values to avoid numerical errors\n",
    "        predictions = np.clip(pred, epsilon, 1. - epsilon)\n",
    "        divisor = np.prod(predictions.shape)\n",
    "        ce = - np.sum( test * np.log(predictions) ) / divisor\n",
    "    \n",
    "        print(f'Binary cross entropy: {ce}\\n')    \n",
    "        return ce\n",
    "    else: \n",
    "        print('something wrong')\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8dd0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original Images: ')\n",
    "print( np.array(true_img).shape)\n",
    "print(np.array(np.flip(recolored_image, axis=1)).shape)\n",
    "\n",
    "print('\\n\\nResized:')\n",
    "print( np.array(true_img)[0:255, : , :].shape)\n",
    "print(np.array(np.flip(recolored_image, axis=1)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45778bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_distance_error, RGB_error = distance_in_RGB(\n",
    "    np.flip(recolored_image, axis=1), \n",
    "    np.array(true_img)[0:255, : , :]/255   # The two images are not exactly the same size, so we drop the extra lines we have\n",
    ")\n",
    "\n",
    "returned_ce = compute_cross_entropy(\n",
    "    np.flip(recolored_image, axis=1), \n",
    "    np.array(true_img)[0:255, : , :]/255   # The two images are not exactly the same size, so we drop the extra lines we have\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd67a0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_computed_pixel_vs_true_pixel(predicted_pixel, test_pixel):\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "\n",
    "    fig.suptitle('Reconstructed Pixel vs True Pixel', fontsize=20)\n",
    "    \n",
    "    # Predicted pixel pixel\n",
    "    pred = np.array(predicted_pixel)\n",
    "    ax1.set_title('Reconstructed Pixel', fontsize=14)\n",
    "    ax1.set_xlabel(str(np.around(pred, 3) ), fontsize=12)\n",
    "    # Display image, `aspect='auto'` makes it fill the whole `axes` (ax3)\n",
    "    ax1.imshow(\n",
    "        [[ pred ]], \n",
    "        aspect='auto')\n",
    "    \n",
    "    # True pixel\n",
    "    ax2.set_title('True Pixel', fontsize=14)\n",
    "    ax2.set_xlabel(str(np.around(np.array(test_pixel), 3 ) ), fontsize=12)\n",
    "    # Display image, `aspect='auto'` makes it fill the whole `axes` (ax3)\n",
    "    ax2.imshow(\n",
    "        [[test_pixel]], \n",
    "         aspect='auto')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # Make space for title\n",
    "    plt.subplots_adjust(top=0.85)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2066222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random RGB pixel\n",
    "original_test = np.random.randint(0, 255, size=3)/255\n",
    "print(f'Random pixel: {original_test}')\n",
    "\n",
    "# Shift it\n",
    "shifted_test = np.clip(original_test - RGB_error, 0, 1.)\n",
    "\n",
    "# Show them\n",
    "print('Subtracting:')\n",
    "show_computed_pixel_vs_true_pixel(shifted_test, original_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665d0ca2",
   "metadata": {},
   "source": [
    "\n",
    "The result is quite intriguing, even though clearly unsatisfactory. But, it is quite remarkable that such an easy, non-deep network is still able to *recolor* an XRF image!\n",
    "\n",
    "Let us move to a more complicated Neural Network architecture, that could help us in this task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba83b0d0",
   "metadata": {},
   "source": [
    "## 4. (1D) Convolutional Neural Networks\n",
    "\n",
    "A possible way to generalize the Deep Neural Network based on Dense, multi-perceptron layers, is to employ different kinds of layers, each of which represents different operations on the inputs. \n",
    "\n",
    "A **Convolutional Neural Network (CNN)** consists of an input and an output layer, as well as multiple hidden layers, as the DNN; but, in contrast with the DNN, the hidden layers of a CNN typically consist of a series of **Convolutional layers**, that are employed to learn the spatial correlation within the inputs by convolving them via a multiplication with a certain *kernel* operator, and whose behaviour we shall briefly describe in the following, and **Pooling layers**, whose goal is to reduce the size of the dataset, maintaining the relevant informations there encoded. \n",
    "\n",
    "The activation function is commonly a *REctified Linear Unit (RELU) layer*, and, in classification tasks, Convolution/Pooling layers are subsequently followed by additional **fully connected layers** [1];\n",
    "\n",
    "<img src=\"./Assets/Images/CNN_arch_example.jpeg\" alt=\"Drawing\" style=\"float: center; width: 90%\"/>\n",
    "\n",
    "To recap, a CNN (usually) consists of\n",
    "1. Convolutional Layers\n",
    "2. Pooling Layers\n",
    "3. Fully Connected Layers\n",
    "\n",
    "<img src=\"./Assets/Video/CNN_example.gif\" alt=\"Drawing\" style=\"float: center; width: 75%\"/>\n",
    "\n",
    "\n",
    "### 4.1. Convolution Layer\n",
    "\n",
    "A convolutional layer is a layer which applies a *convolution* to the input data [2]; it can be seen as a non-linear operator $\\mathbf{K}$ which maps a vector belonging to a vector space $\\mathbb{R}^m$ to a tensor belonging in a subspace of the cartesian product $\\bigotimes_{a=1}^{\\# \\mathrm{filters}} \\mathbb{R}^m$, i.e.\n",
    "\n",
    "$$\\mathbf{K} :  \\mathbb{R}^m \\mapsto \\bigotimes_{a=1}^{\\# \\mathrm{filters}} \\mathbb{R}^m \\,,$$\n",
    "\n",
    "such that, if $v = v^j \\partial_j \\in \\mathbb{R}^m$\n",
    "\n",
    "$$\\mathbf{K} [v] = \\bigotimes_{a=1}^{\\# \\mathrm{filters}} \\sum_{j \\in \\mathscr{I}_j} K^{\\alpha}{}_{j} [a] \\,\\, v^j \\, \\partial_\\alpha \\in \\bigotimes_{a=1}^{\\# \\mathrm{filters}} \\mathbb{R}^m \\,,$$\n",
    "\n",
    "where $\\# \\mathrm{filters}$ is an hyperparameters describing how many copies of the filter are applied to the input vector (each of which are stochastically initialized), and $\\mathscr{I}_j$ is an interval, signaling on how many near elements are convolved to extract their correlation, called **kernel size**; \n",
    "\n",
    "**Question 1:** *Does this formula reminds you of something?* [A]\n",
    "\n",
    "Explicitly, as the gif shows (for the case where $\\# \\mathrm{filters}=1$ and $\\mathscr{I}_j = \\{ j-1, j, j+1 \\}$:\n",
    "\n",
    "<img src=\"https://gitlab.com/alessandro.bombini.fi/cnn1d_tutorial/-/raw/master/Assets/Images/1D-Convolution.gif\" alt=\"Drawing\"/>\n",
    "\n",
    "In this case \n",
    "$$\\mathbf{K} :  \\mathbb{R}^7 \\mapsto \\mathbb{R}^7 \\,,$$\n",
    "with $\\mathbf{K} = [1/3, 1/3, 1/3]$. \n",
    "\n",
    "Sometimes, $\\mathbf{K}[v]_0$ and $\\mathbf{K}[v]_{6=\\mathrm{dim} \\, \\mathbb{R}^7 -1}$ are conventionally set to be equal to the input vector, i.e.  $\\mathbf{K}[v]_0 = v_0$, $\\mathbf{K}[v]_{\\mathrm{dim}\\, \\mathbb{R}^m - 1} = v_{\\mathrm{dim}\\, \\mathbb{R}^m - 1}$ . With this convention, the Convolutional Layer *does not* change the size of the dimension of the vector space (output and input vectors have the same number of entries).\n",
    "\n",
    "\n",
    "#### 4.1.1: generalization to higher dimensions: the 2D Convolutional Layer\n",
    "\n",
    "It is quite easy to generalize to higher dimensions: a $n$D convolutional layer is a non-linear operator which maps the \n",
    "\n",
    "$$\\mathbf{K} :  \\otimes_{I=1,\\ldots, n} \\mathbb{R}^{m_I} \\mapsto \\bigotimes_{a=1}^{\\# \\mathrm{filters}} \\otimes_{I=1,\\ldots, n} \\mathbb{R}^{m_I} \\,,$$\n",
    "\n",
    "such that, if $M = M^{j_1 \\ldots j_n} \\, \\partial_{j_1} \\cdots \\partial_{j_n} \\in \\otimes_{I=1,\\ldots, n} \\mathbb{R}^{m_I}$, \n",
    "\n",
    "$$\\mathbf{K} [v] = \\bigotimes_{a=1}^{\\# \\mathrm{filters}} \\sum_{j_1 \\in \\mathscr{I}_{j_1}} \\cdots \\sum_{j_n \\in \\mathscr{I}_{j_n}} K^{\\alpha_1 \\cdots \\alpha_n}{}_{j_1 \\cdots j_n} [a] \\, \\, M^{j_1 \\ldots j_n} \\, \\partial_{\\alpha_1} \\cdots \\partial_{\\alpha_n} \\in \\bigotimes_{a=1}^{\\# \\mathrm{filters}}\\otimes_{I=1,\\ldots, n} \\mathbb{R}^{m_I}\\,,$$\n",
    "\n",
    "\n",
    "<img src=\"https://gitlab.com/alessandro.bombini.fi/cnn1d_tutorial/-/raw/master/Assets/Images/Conv2D.gif\" alt=\"Drawing\"/>\n",
    "\n",
    "-------\n",
    "\n",
    "[1] François Chollet, *Deep Learning with Python*, https://www.manning.com/books/deep-learning-with-python\n",
    "\n",
    "[2] Mohamed Elgendy, *Deep Learning for Vision Systems*, https://www.manning.com/books/deep-learning-for-vision-systems\n",
    "\n",
    "\n",
    "-------\n",
    "[A] It should remind you a lot of stuff; e.g., in the path-integral formulation of Quantum Mechanics, it is possible to use the superposition principle to write down the most general solution of the Schrödinger equation\n",
    "$$\\psi (x, t) = \\int \\mathbf{K} (x, t ; \\, y, t_0) \\, \\psi_0 (y) dy \\, .$$\n",
    "\n",
    "see https://userswww.pd.infn.it/~feruglio/rattazzi.pdf .\n",
    "\n",
    "This is a natural consequence of differential analysis, where we can obtain a solution of a forced differential equation from a certain solution of the homogeneus one by applying to it the differential operator's Green function; i.e. the solution of \n",
    "$$\\mathbf{L} [u(x)] = f(x) \\,,$$\n",
    "is, \n",
    "$$u(x) = \\int G(x,s) f(s) ds \\,,$$\n",
    "where $G(x,s)$ is the *Green's operator*, satisfying \n",
    "$$\\mathbf{L}_x [G(x,s)] = \\delta (x-s) \\,.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae84c751",
   "metadata": {},
   "source": [
    "#### 4.1.2: Padding and Strides [EXTRA]\n",
    "\n",
    "**Padding**\n",
    "\n",
    "As described above, one tricky issue when applying convolutional layers is that we tend to lose pixels on the perimeter of our image. Since we typically use small kernels, for any given convolution, we might only lose a few pixels, but this can add up as we apply many successive convolutional layers. One straightforward solution to this problem is to add extra pixels of filler around the boundary of our input image, thus increasing the effective size of the image. \n",
    "\n",
    "<img src=\"./Assets/Images/conv-pad.png\" alt=\"Drawing\" style=\"width: 45%\"/>\n",
    "\n",
    "**Strides**\n",
    "\n",
    "When computing the cross-correlation, we start with the convolution window at the top-left corner of the input tensor, and then slide it over all locations both down and to the right. In previous examples, we default to sliding one element at a time. However, sometimes, either for computational efficiency or because we wish to downsample, we move our window more than one element at a time, skipping the intermediate locations.\n",
    "\n",
    "Usually, we refer to the number of rows and columns traversed per slide as the *stride*.\n",
    "\n",
    "<img src=\"./Assets/Images/conv-stride.png\" alt=\"Drawing\" style=\"width: 40%\"/>\n",
    "\n",
    "A small Gif explaining the role of the two:\n",
    "\n",
    "<img src=\"./Assets/Images/strides_padding.gif\" alt=\"Drawing\" style=\"width: 95%\"/>\n",
    "\n",
    "-------\n",
    "[1] http://d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2fb740",
   "metadata": {},
   "source": [
    "### 4.2 Pooling layer\n",
    "\n",
    "Convolutional networks may include local or global pooling layers to streamline the underlying computation. Pooling layers reduce the dimensions of the data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer. Local pooling combines small clusters.\n",
    "\n",
    "The Pooling is thus a non-linear, irreversible operation which project the input vector into a lower-dimensional vector space; it can be modelled by a *projection* operator $\\mathbf{P} : \\mathbb{R}^m \\mapsto \\mathbb{R}^n$, where $n$ is usually a divisor of $m$. \n",
    "\n",
    "There are multiple types of Pooling layers; the most used ones are the **Max Pooling** and the **Average Pooling**.\n",
    "\n",
    "#### 4.2.1 Max Pooling\n",
    "\n",
    "The Max Pooling is a procedure to create a reduced vector, by replacing a sub-space vector with its max value:\n",
    "\n",
    "<img src=\"./Assets/Images/maxpool.gif\" alt=\"Drawing\" style=\"width: 35%\"/>\n",
    "\n",
    "i.e., $\\mathbf{P} : \\mathbb{R}^m \\mapsto \\mathbb{R}^n$ such that\n",
    "$$\\mathbf{P} [v] = \\bigotimes_{k=1}^{n} \\mathrm{max}_{i\\in \\mathscr{I}_k} [v_i] \\,. $$\n",
    "\n",
    "#### 4.2.2 Average Pooling\n",
    "a\n",
    "The Average Pooling is a procedure to create a reduced vector, by replacing a sub-space vector with its average value:\n",
    "\n",
    "<img src=\"./Assets/Images/Avg_vs_Max_pooling.jpg\" alt=\"Drawing\" style=\"width: 35%\"/>\n",
    "\n",
    "i.e., $\\mathbf{P} : \\mathbb{R}^m \\mapsto \\mathbb{R}^n$ such that\n",
    "$$\\mathbf{P} [v] = \\bigotimes_{k=1}^{n} \\frac{1}{\\mathrm{dim} \\mathscr{I}_k } \\sum_{i\\in \\mathscr{I}_k} v_i \\,. $$\n",
    "\n",
    "\n",
    "-------\n",
    "\n",
    "[1] https://medium.com/@adityaraj_64455/it-all-started-with-cnns-alexnet-3023b21bb891"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e095f5",
   "metadata": {},
   "source": [
    "### 4.3 Applied Convolutions: Edge Detections with Sobel Filter [extra]\n",
    "\n",
    "One may ask why Convolution layers are relevant in Neural Networks, or in general. \n",
    "\n",
    "The fact is that Convolution Layers have been introduced in the context of computer vision, starting from the introduction of the *neocognitron* of Kunihiko Fukushima in 1980 [1] [2], where classical algorithms involving convolutions are usually employed. \n",
    "\n",
    "One interesting example I want to show you, is the *Sobel filter* for Edge dectection [3]. It consists in convolving two filters across an image to find the horizontal and vertical vector gradients for each pixel, and then calculating the magnitude (length) of each vector gradient.\n",
    "\n",
    "The Sobel filter apply two filers $G_x$ and $G_y$ to the images; those are\n",
    "$$G_x = \\left[\\begin{matrix} +1 & +2 & +1 \\\\ 0 & 0 & 0 \\\\ -1 & -2 & -1  \\end{matrix}\\right] \\,,$$\n",
    "$$G_y = \\left[\\begin{matrix} -1 & 0 & +1 \\\\ -2 & 0 & +2 \\\\ -1 & 0 & +1  \\end{matrix}\\right] \\,. $$\n",
    "\n",
    "Since the Sobel kernels can be decomposed as the products of an averaging and a differentiation kernel, they compute the gradient with smoothing. In fact, they can be written as \n",
    "\n",
    "$$G_x = \\left[ \\begin{matrix} 1 \\\\ 2 \\\\ 1   \\end{matrix}  \\right] \\otimes  \\left[  \\begin{matrix} +1 & 0 & -1   \\end{matrix}  \\right] \\,, \\, G_y =  \\left[ \\begin{matrix} +1 \\\\ 0 \\\\ -1   \\end{matrix}  \\right] \\otimes  \\left[  \\begin{matrix} 1 & 2 & 1   \\end{matrix}  \\right] \\,.$$\n",
    "\n",
    "And then the filtered images has gradient magnitude\n",
    "$$G = \\sqrt{G_x^2 + G_y^2} \\,,$$\n",
    "the gradient direction is\n",
    "$$\\Theta = \\arctan \\frac{G_y}{G_x} \\,.$$\n",
    "\n",
    "\n",
    "The pseudocode implementation is [4]\n",
    "\n",
    "function sobel(A : as two dimensional image array)\n",
    "\t\n",
    "    Gx=[-1 0 1; -2 0 2; -1 0 1]\n",
    "\tGy=[-1 -2 -1; 0 0 0; 1 2 1]\n",
    "\t\n",
    "\trows = size(A,1)\n",
    "\tcolumns = size(A,2)\n",
    "\tmag=zeros(A)\n",
    "\n",
    "\tfor i=1:rows-2\n",
    "\t\tfor j=1:columns-2\n",
    "\t\t\tS1=sum(sum(Gx.*A(i:i+2,j:j+2)))\n",
    "\t\t\tS2=sum(sum(Gy.*A(i:i+2,j:j+2)))\n",
    "\n",
    "\t\t\tmag(i+1,j+1)=sqrt(S1.^2+S2.^2)\n",
    "\t\tend for\n",
    "\tend for\n",
    "\t\n",
    "\tthreshold = 70 %varies for application [0 255]\n",
    "\toutput_image = max(mag,threshold)\n",
    "\toutput_image(output_image==round(threshold))=0;\n",
    "\treturn output_image\n",
    "end function\n",
    "\n",
    "-------\n",
    "[1] K. Fukushima: \"Neocognitron: A hierarchical neural network capable of visual pattern recognition\", Neural Networks, 1[2], pp. 119-130 (1988). \n",
    "\n",
    "[2] http://www.scholarpedia.org/article/Neocognitron\n",
    "\n",
    "[3] Irwin Sobel, 2014, History and Definition of the Sobel Operator https://www.researchgate.net/publication/239398674_An_Isotropic_3x3_Image_Gradient_Operator \n",
    "\n",
    "[4] https://en.wikipedia.org/wiki/Sobel_operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2d7680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "# I now try to do my sobel\n",
    "def edge_sobel(img, threshold = 70):\n",
    "    import numpy as np\n",
    "    \n",
    "    image = rgb2gray(img) # Convert color image to gray scale\n",
    "    len_x = image.shape[0]\n",
    "    len_y = image.shape[1]\n",
    "    \n",
    "    Gx = np.array([[+1, +2, +1], [0,0,0], [-1,-2,-1]])\n",
    "    Gy = np.array([[-1,0,+1],[-2,0,+2],[-1,0,+1]])\n",
    "    \n",
    "    mag = np.zeros((len_x, len_y))\n",
    "    step=0\n",
    "    \n",
    "    for i in np.arange(1, len_x-1):\n",
    "        for j in np.arange(1, len_y-1):\n",
    "            S1=0;\n",
    "            S2=0;\n",
    "            for n in np.arange(-1,2):\n",
    "                for m in np.arange(-1,2):\n",
    "                    S1+= Gx[n,m]*image[i+n,j+m]\n",
    "                    S2+= Gy[n,m]*image[i+n,j+m]\n",
    "                    #print(np.sqrt(S1**2 + S2**2))\n",
    "\n",
    "            step=max(np.around(np.sqrt(S1**2 + S2**2), 0), threshold)\n",
    "            \n",
    "            if (step>255):\n",
    "                mag[i,j] = 255\n",
    "            elif (step<0):\n",
    "                mag[i,j] = 0\n",
    "            else :\n",
    "                mag[i,j] = step\n",
    "    \n",
    "    # Normalize the image\n",
    "    mag = mag.astype(np.uint8)\n",
    "    return mag\n",
    "\n",
    "# importing the image\n",
    "image_file = \"Assets/Images/cms_2.jpg\"\n",
    "image = Image.open(image_file)\n",
    "\n",
    "sobel_img = edge_sobel(np.array(image))\n",
    "\n",
    "# Display it\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot original image\n",
    "a=fig.add_subplot(1, 2, 1)\n",
    "image_plot_1 = plt.imshow(image)\n",
    "a.set_title(\"Original\")\n",
    "\n",
    "# Plot image after Sobel algorithm\n",
    "a=fig.add_subplot(1, 2, 2)\n",
    "image_plot_2 = plt.imshow(sobel_img, cmap=\"gray\")\n",
    "a.set_title(\"Sobel Edge Detection\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e421e8b9",
   "metadata": {},
   "source": [
    "## 4.4. Creating a simple CNN\n",
    "\n",
    "We will build an easy CNN, but we will use a different grammar, still in Keras: the Keras' *Functional API* [1], instead of the one used above, the *Sequential API* [2]; \n",
    "\n",
    "As stated in the Keras' documentation:\n",
    "\n",
    "*The Keras functional API is a way to create models that are more flexible than the tf.keras.Sequential API. The functional API can handle models with non-linear topology, shared layers, and even multiple inputs or outputs.*\n",
    "\n",
    "\n",
    "---------\n",
    "[1] https://keras.io/guides/functional_api/\n",
    "\n",
    "[2] https://hanifi.medium.com/sequential-api-vs-functional-api-model-in-keras-266823d7cd5e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebed310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "# Import relevant Keras classes\n",
    "\n",
    "# Input layer \n",
    "from keras.layers import InputLayer, Input\n",
    "# The Dense Layer\n",
    "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Reshape, AveragePooling1D\n",
    "\n",
    "# Sequential class for instantiating the Model\n",
    "from keras.models import Sequential\n",
    "from keras import Model # If we want to use the Keras API\n",
    "\n",
    "# To get the Activation functions\n",
    "from keras.layers import Activation\n",
    "\n",
    "\n",
    "#######################################################################################################################\n",
    "# Input-Output defs\n",
    "\n",
    "input_shape = np.shape(X_train)[1]   # Input layer takes [number of vectors, size of vectors, features-per-vector] = [#size_dataset, 1000, 1]\n",
    "num_classes = 3 # RGB\n",
    "\n",
    "CHANNEL_DEPTH = 500   # Each histograms has 500 channels\n",
    "num_sensors = 1       # We have only 1 histogram per data\n",
    "\n",
    "#######################################################################################################################\n",
    "# Defining the model\n",
    "#######################################################################################################################\n",
    "\n",
    "#######################################################################################################################\n",
    "# Input\n",
    "\n",
    "input_shape = np.shape(X_train)[1]   # Input layer takes [number of vectors, size of vectors, features-per-vector] = [#size_dataset, 1000, 1]\n",
    "num_classes = 3 # RGB\n",
    "\n",
    "CHANNEL_DEPTH = 500\n",
    "num_sensors = 1\n",
    "\n",
    "\n",
    "#############\n",
    "# input\n",
    "inputs = Input(shape=(input_shape, ))\n",
    "x = Reshape( (CHANNEL_DEPTH, num_sensors), input_shape=(input_shape, )) (inputs)\n",
    "\n",
    "#############\n",
    "# CNN entry part; 3x (2xConv1D + BatchNorm + MaxPool)\n",
    "\n",
    "# 1.1 - 2 conv1D\n",
    "x = Conv1D(filters=32, kernel_size=3, activation='relu', \n",
    "                   input_shape = (CHANNEL_DEPTH, num_sensors)\n",
    "                  )(x)\n",
    "x = Conv1D(filters=32, kernel_size=3, activation='relu')(x)\n",
    "x = MaxPooling1D(2)(x) \n",
    "\n",
    "\n",
    "# 1.2 - 2 conv1D\n",
    "x = Conv1D(filters=64, kernel_size=3, activation='relu')(x)\n",
    "x = Conv1D(filters=64, kernel_size=3, activation='relu')(x)\n",
    "x = MaxPooling1D(2)(x)\n",
    "\n",
    "\n",
    "# 1.3 - 2 conv1D\n",
    "x = Conv1D(filters=128, kernel_size=3, activation='relu')(x)\n",
    "x = Conv1D(filters=128, kernel_size=3, activation='relu')(x)\n",
    "x = Conv1D(filters=128, kernel_size=3, activation='relu')(x)\n",
    "x = MaxPooling1D(2)(x)\n",
    "\n",
    "\n",
    "#############\n",
    "# Final pooling\n",
    "x = AveragePooling1D(pool_size=16, strides=1, padding='valid')(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "#############\n",
    "#  Last layer - output\n",
    "outputs = Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "# Final model definition\n",
    "model_CNN = Model(inputs=inputs, outputs=outputs, name=\"my_first_1DCNN\")\n",
    "\n",
    "print(model_CNN.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d99e12",
   "metadata": {},
   "source": [
    "We use again graphviz to show graphically illustrate the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e630c881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "tf.keras.utils.plot_model(model_CNN, show_shapes=True, dpi=76, to_file=\"CNN_lecture.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf583e9",
   "metadata": {},
   "source": [
    "#### 4.4.1 Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff134db",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Training - prerequisites\n",
    "loss_used = 'binary_crossentropy'\n",
    "\n",
    "model_CNN.compile(loss=loss_used,\n",
    "                 optimizer='adam',\n",
    "                 )\n",
    "# We need to pick the size of the batch used for the training\n",
    "BATCH_SIZE = 200 \n",
    "# And the number of epochs on which we train the dataset\n",
    "EPOCHS = 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaa1c5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "starting_time = datetime.datetime.now() \n",
    "\n",
    "print('\\nStart training model: \\n')\n",
    "\n",
    "##########################################################\n",
    "# TRAINING\n",
    "\n",
    "history_CNN = model_CNN.fit(X_train,\n",
    "                       y_train,\n",
    "                       batch_size=BATCH_SIZE,\n",
    "                       epochs=EPOCHS,\n",
    "                       #callbacks=callbacks_list,\n",
    "                       validation_data=(X_test, y_test),\n",
    "                       verbose=1)\n",
    "\n",
    "print('\\n\\nTraining done...\\n')\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "total_training_time = now-starting_time\n",
    "print('Total training time: ', total_training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a137ff",
   "metadata": {},
   "source": [
    "#### 4.4.2 Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1055c56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "plt.plot(history_CNN.history['loss'])\n",
    "plt.plot(history_CNN.history['val_loss'])\n",
    "\n",
    "plt.title('Model loss', fontsize=20,  pad=10)\n",
    "\n",
    "plt.ylabel('loss', fontsize=18)\n",
    "plt.xlabel('epoch', fontsize=18)\n",
    "\n",
    "plt.legend(['loss', 'val_loss'], loc='upper right', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e45d8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# SAVING\n",
    "\n",
    "# serialize weights to HDF5\n",
    "time_string = now.strftime(\"%Y-%m-%d-%H-%M-%S\")    \n",
    "name_to_save = 'model_CNN_w_batchsize' + str(BATCH_SIZE) + '_epochs' + str(EPOCHS) +  '_v' + time_string \n",
    "path_to_model = 'Model_data/' +  name_to_save + '.h5'\n",
    "model_CNN.save(path_to_model)\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8925b06b",
   "metadata": {},
   "source": [
    "#### 4.4.3 Recolor the image with trained CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8511de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "filename = \"Rawdata+RGB/Putto_Area1_fiori\"\n",
    "xrf_img = np.load(f\"{filename}.npz\")['img']\n",
    "print (f\"Shape of the loaded XRF image: {xrf_img.shape}\")\n",
    "true_img = np.asarray(PIL.Image.open(f\"{filename}.jpg\"))\n",
    "print (f\"Shape of the loaded RGB image (true): {true_img.shape}\")\n",
    "\n",
    "pixels = xrf_img.reshape ((-1, 500))\n",
    "CNN_recolored_image = model_CNN.predict (pixels).reshape (xrf_img[:,:,:3].shape)[:,::-1]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10), dpi=100)\n",
    "plt.subplot(121)\n",
    "plt.title (\"Flowers (true)\")\n",
    "plt.imshow (true_img)\n",
    "plt.subplot(122)\n",
    "plt.title (\"Flowers (reconstructed with CNN)\")\n",
    "plt.imshow (CNN_recolored_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c0a495",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_distance_error, RGB_error = distance_in_RGB(\n",
    "    np.flip(CNN_recolored_image, axis=1), \n",
    "    np.array(true_img)[0:255, : , :]/255   # The two images are not exactly the same size, so we drop the extra lines we have\n",
    ")\n",
    "\n",
    "returned_ce = compute_cross_entropy(\n",
    "    np.flip(CNN_recolored_image, axis=1), \n",
    "    np.array(true_img)[0:255, : , :]/255   # The two images are not exactly the same size, so we drop the extra lines we have\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f14b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random RGB pixel\n",
    "original_test = np.random.randint(0, 255, size=3)/255\n",
    "print(f'Random pixel: {original_test}')\n",
    "\n",
    "# Shift it\n",
    "shifted_test = np.clip(original_test - RGB_error, 0, 1.)\n",
    "\n",
    "# Show them\n",
    "print('Subtracting:')\n",
    "show_computed_pixel_vs_true_pixel(shifted_test, original_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41445552",
   "metadata": {},
   "source": [
    "## 5. Conclusions & Outlook\n",
    "\n",
    "We have seen how to use Dense Deep Neural Network and Convolutionale Neural Network applied on Physical Imaging raw data, in order to start a process of digital restoration of cultural heritage; \n",
    "\n",
    "One may ask what is the goal of \"recolor\" an XRF (or PIXE, PIGE, FTIR, etc) raw data, if the painted surface is just in fron of our eyes. \n",
    "\n",
    "Well, that's not always true. One of the properties of physical imaging, is its ability to penetrate into the target; the depth of the penetration is related to the energy of the incoming X-ray via the *stopping power factor*; this means we are able to receive information about the interal layer of the painting (if present) .\n",
    "\n",
    "<div style=\"margin-bottom: 40em;\">\n",
    "<img src=\"./Rawdata+RGB/ragtriste_VISscalato.png\" alt=\"Drawing\" style=\"float: left; margin-top: 15px; width: 45%\"/>\n",
    "<img src=\"./Rawdata+RGB/ragazzo_triste_dietro.jpg\" alt=\"Drawing\" style=\"float: right; margin-top: 15px; width: 45%\"/>\n",
    "</div>\n",
    "    \n",
    "This would thus imply that, in principle, we are able to explore previous, submersed pictoric layer of the painting!\n",
    "\n",
    "The goal is now clear: try to use a sufficiently deep neural network to *reconstruct* the submersed layers and thus artificially restore the forgotten image, without the need of invasive operations on the painting!\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./Assets/Images/XKCD_1.jpg\" alt=\"Drawing\" style=\"float: center; margin-top: 15px; width: 35%\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "--------------\n",
    "\n",
    "<div>\n",
    "<img src=\"./Assets/Pics/Logo_INFN_CHNet_esteso.png\" alt=\"Drawing\" style=\"float: left; width: 250px;\"/>\n",
    "<img src=\"./Assets/Pics/EOSc-Pillar_logo_final_Squared.png\" alt=\"Drawing\" style=\"float: right; width: 250px;\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f050f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b320e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
