{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26f37f1a",
   "metadata": {},
   "source": [
    "# Exercises: Hands on DNN and 1DCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6187d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check on GPU using TensorFlow\n",
      "\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# OS is the built-in python library for exploring the file system\n",
    "import os \n",
    "# Numpy, Pandas and Math are must have libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# PIL is the built-in library for trating images; it may be give useful methods as Image, ImageOps \n",
    "from PIL import Image, ImageOps \n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "# GPU usage; see https://www.tensorflow.org/guide/gpu\n",
    "print('Check on GPU using TensorFlow\\n')\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "memory_limit = 1024 # 1 Gb\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memory_limit)]  # here the limit\n",
    "        )\n",
    "        \n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "if \"GPU\" not in device_name:\n",
    "    print(\"GPU device not found\")\n",
    "else:\n",
    "    print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b2a0deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# Progress bar\n",
    "import sys\n",
    "\n",
    "def printProgressBar(i,max,postText):\n",
    "    n_bar =10 #size of progress bar\n",
    "    j= i/max\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write(f\"[{'=' * int(n_bar * j):{n_bar}s}] {int(100 * j)}%  {postText}\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# We define two functions that check the type of (sub)strings \n",
    "# by raising error if such string cannot be parsed \n",
    "def RepresentsInt(s):\n",
    "    try: \n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def RepresentsFloat(s):\n",
    "    try: \n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "# After that, we define the function that parses a string into a numpy array\n",
    "# We will use the Python built in library for Regular Expression, re (see https://docs.python.org/3/library/re.html)\n",
    "def reinterpret_hist_as_vector(histogram):\n",
    "    import re\n",
    "\n",
    "    step = re.split(\n",
    "        '\\[|\\]',\n",
    "        histogram\n",
    "    )\n",
    "    step = re.split(\n",
    "        '\\n',\n",
    "        step[1]\n",
    "    )\n",
    "    step = ' '.join(step)\n",
    "    step = step.split(' ')\n",
    "    \n",
    "    hist_els = []\n",
    "    \n",
    "    for el in step:\n",
    "        if RepresentsFloat(el):\n",
    "            hist_els.append(\n",
    "                (float(el))\n",
    "            )\n",
    "      \n",
    "    return hist_els\n",
    "\n",
    "# Here we define a custom function to split our dataset into test/train\n",
    "def prepare_training_dataset(df_entry, percentage = 0.8):\n",
    "    # Shuffle the dataframe\n",
    "    df = df_entry.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # Select percentage\n",
    "    train_size = math.floor(percentage * len( df ) )\n",
    "    \n",
    "    # splitting dataframe by row index \n",
    "    df_train = df.iloc[:train_size,:] \n",
    "    df_test = df.iloc[train_size:,:] \n",
    "    \n",
    "    # Initializing X,y <- faster version ->\n",
    "    #X_train = df_train['X'].to_numpy()\n",
    "    #y_train = df_train['y'].to_numpy()\n",
    "    #X_test = df_test['X'].to_numpy()\n",
    "    #y_test = df_test['y'].to_numpy()\n",
    "\n",
    "    # Initializing X,y \n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Interpret the TRAINING vectors\n",
    "    print('Preparing the TRAINING dataset...\\n')\n",
    "    for index, row in df_train.iterrows():\n",
    "        # Progress bar\n",
    "        percentage = math.floor(index/len(df_train.index) * 100)\n",
    "        if ( percentage % 10 == 0 ):\n",
    "            print_train_in_progress_bar = str(percentage) + '%'\n",
    "            printProgressBar(index, len(df_train.index), print_train_in_progress_bar)\n",
    "        # Histogram\n",
    "        X_train.append( row['X'] )\n",
    "        #RGB \n",
    "        y_train.append( row['y'] )\n",
    "        \n",
    "    print('\\n\\nPreparing the TEST dataset...\\n')\n",
    "    # Interpret the TEST vectors    \n",
    "    for index, row in df_test.iterrows():\n",
    "        # Progress bar\n",
    "        percentage = math.floor(((index-len(df_train.index))/len(df_test.index)) * 100)\n",
    "        if ( percentage % 10 == 0 ):\n",
    "            print_test_in_progress_bar = str(percentage) + '%'\n",
    "            printProgressBar(\n",
    "                index-len(df_train.index), \n",
    "                len(df_test.index), \n",
    "                print_test_in_progress_bar\n",
    "            )\n",
    "        # Histogram \n",
    "        X_test.append( row['X'] )\n",
    "        \n",
    "        #RGB \n",
    "        y_test.append( row['y']  )\n",
    "    \n",
    "    print('\\nReturining Train/Test datasets\\n')\n",
    "    # returning\n",
    "    return np.array(X_train), np.array(y_train), np.array(X_test), np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890c532b",
   "metadata": {},
   "source": [
    "## Exercise 1: Counteract Overfitting - DropOut layers\n",
    "\n",
    "<img src=\"https://gitlab.com/alessandro.bombini.fi/cnn1d_tutorial/-/raw/master/Assets/Images/overfitting_1.png\" alt=\"Drawing\" style=\"float: center; margin-top: 15px\"/>\n",
    "\n",
    "In statistics, overfitting is:\n",
    "\n",
    "*\"the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably.\"*\n",
    "\n",
    "<img src=\"https://gitlab.com/alessandro.bombini.fi/cnn1d_tutorial/-/raw/master/Assets/Images/overfitting-comics.jpg\" alt=\"Drawing\" style=\"float: center; margin-top: 15px\"/>\n",
    "\n",
    "In machine learning models (which are statistical models), overfitting appears when there is an umbalance between the number of trainable parameters and the size of the training dataset; it becomes manifest during the training process when the model performs poorly on the test/validation dataset, w.r.t. the training dataset, i.e. when the validation scores starts increasing in epochs, while training scores still decreases\n",
    "\n",
    "<img src=\"https://gitlab.com/alessandro.bombini.fi/cnn1d_tutorial/-/raw/master/Assets/Images/overfitting_2.png\" alt=\"Drawing\" style=\"float: center; margin-top: 15px; width: 400px\"/>\n",
    "\n",
    "A way to counteract this event in Deep Neural Networks is to add **drop layers** in the DNN architecture. [2], [3], [4]\n",
    "\n",
    "Dropout is a regularization method that approximates training a large number of neural networks with different architectures in parallel. During training, some number of layer outputs are randomly ignored or “dropped out”. Dropout has the effect of making the training process noisy, forcing nodes within a layer to probabilistically take on more or less responsibility for the inputs.\n",
    "\n",
    "<img src=\"https://gitlab.com/alessandro.bombini.fi/cnn1d_tutorial/-/raw/master/Assets/Images/DropLayer.gif\" alt=\"Drawing\" style=\"float: center; margin-top: 15px;\"/>\n",
    "\n",
    "In Keras, dropout layers are asily implemented by the *Dropout* class [5]\n",
    "\n",
    "tf.keras.layers.Dropout(rate, noise_shape=None, seed=None, **kwargs)\n",
    "\n",
    "that:\n",
    "\n",
    "*The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged.*\n",
    "\n",
    "**EXERCISE**\n",
    "\n",
    "Build a Deep Dense Neural Network with (at least) 5 layers, and apply it to our model. (Keep an eye on the number of parameters, and on how long does it take to train an epoch).\n",
    "\n",
    "See if it shows overfitting; if not, deepen it, until it shows it. \n",
    "\n",
    "Then, create a new network with Dense layers, and see if it is now free of overfitting. \n",
    "\n",
    "-------\n",
    "[1] https://www.lexico.com/definition/overfitting\n",
    "\n",
    "[2] https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/\n",
    "\n",
    "[3] https://machinelearningmastery.com/how-to-reduce-overfitting-with-dropout-regularization-in-keras/\n",
    "\n",
    "[4] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov, *Dropout: A Simple Way to Prevent Neural Networks from Overfitting* https://jmlr.org/papers/v15/srivastava14a.html\n",
    "\n",
    "[5] https://keras.io/api/layers/regularization_layers/dropout/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7de968f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c11875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91646440",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dec8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f910afd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98248690",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9724c68",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03ccd9d4",
   "metadata": {},
   "source": [
    "## Exercise 2: Deep CNN; hints from ImageNet Large Scale Visual Recognition Challenge winners - AlexNet & VGG\n",
    "\n",
    "The **ImageNet** project is a large visual database designed for use in visual object recognition software research. More than 14 million images have been hand-annotated by the project. \n",
    "\n",
    "Since 2010, a competition was launch, dubbed *ImageNet Large Scale Visual Recognition Challenge* (ILSVRC), where researchers from all around the globe have to design a novel Deep Learning architecture to label the dataset. From the ImageNet homepage [1]:\n",
    "\n",
    "*The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) evaluates algorithms for object detection and image classification at large scale. One high level motivation is to allow researchers to compare progress in detection across a wider variety of objects -- taking advantage of the quite expensive labeling effort. Another motivation is to measure the progress of computer vision for large scale image indexing for retrieval and annotation.*\n",
    "\n",
    "### AlexNet\n",
    "\n",
    "On 30 September 2012, a convolutional neural network (CNN) called **AlexNet** [3] achieved a top-5 error of 15.3% in the ImageNet 2012 Challenge, more than 10.8 percentage points lower than that of the runner up. \n",
    "\n",
    "AlexNet is considered one of the most influential papers published in computer vision, having spurred many more papers published employing CNNs and GPUs to accelerate deep learning. As of 2021, the AlexNet paper has been cited over 80,000 times according to Google Scholar. \n",
    "\n",
    "<img src=\"https://gitlab.com/alessandro.bombini.fi/cnn1d_tutorial/-/raw/master/Assets/Images/AlexNet.png\" alt=\"Drawing\" style=\"float: center; margin-top: 15px\"/>\n",
    "\n",
    "AlexNet contained eight layers; the first five were convolutional layers, some of them followed by max-pooling layers, and the last three were fully connected layers. It used the non-saturating ReLU activation function, which showed improved training performance over tanh and sigmoid. In detail [3]:\n",
    "\n",
    "- 1 Convolutional layer with kernel size (11,11) and 96 filters, followed by a (3,3)-MaxPooling;\n",
    "- 1 Convolutional layer with kernel size (5,5) and 256 filters, followed by a (3,3)-MaxPooling;\n",
    "- 1 Convolutional layer with kernel size (3,3) and 128 filters;\n",
    "- 2 Convolutional layers with kernel size (3,3) and 192 filters each, followed by a Flatten layer to feed the dense layer;\n",
    "- 2 Dense layers with 4096 neurons and a 0.5-probability drop layer (see ex.1)\n",
    "- the Output layer which may label 10 categories.\n",
    "\n",
    "In code [4]: \n",
    "\n",
    "```\n",
    "AlexNet_model = keras.models.Sequential(\n",
    "    [\n",
    "    # 1°: (11x11)\n",
    "    keras.layers.Conv2D(filters=96, \n",
    "                            kernel_size=(11,11), strides=(4,4), activation='relu', \n",
    "                            input_shape=(227,227,3)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "\n",
    "    # 2° (5x5)\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "\n",
    "    # 3,4,5° (3x3)\n",
    "    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization()\n",
    "    ,\n",
    "    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Flatten(),\n",
    "\n",
    "    # Dense layers\n",
    "    keras.layers.Dense(4096, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(4096, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "\n",
    "    # Output\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "### VGG\n",
    "\n",
    "In 2014, a novel architecture developed by the Oxford's *Visual Geometry Group* (VGG), dubbed *Very Deep Convolutional Networks for Large-Scale Visual Recognition* (shortened in VGG-16) was presented at ILSVRC; citing the VGG page [5] \n",
    "\n",
    "*Our main contribution is a rigorous evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by increasing the depth to 16-19 weight layers, which is substantially deeper than what has been used in the prior art. To reduce the number of parameters in such very deep networks, we use very small 3×3 filters in all convolutional layers (the convolution stride is set to 1). Please see our publication for more details.*\n",
    "\n",
    "*The very deep ConvNets were the basis of our ImageNet ILSVRC-2014 submission, where our team (VGG) secured the first and the second places in the localisation and classification tasks respectively.*\n",
    "\n",
    "<img src=\"https://gitlab.com/alessandro.bombini.fi/cnn1d_tutorial/-/raw/master/Assets/Images/VGG163d.png\" alt=\"Drawing\" style=\"float: center; margin-top: 15px\"/>\n",
    "\n",
    "The VGG architecture (in his base form, VGG-16, and its evolution, VGG-19) is [6]:\n",
    "\n",
    "<img src=\"https://gitlab.com/alessandro.bombini.fi/cnn1d_tutorial/-/raw/master/Assets/Images/VGG1619.png\" alt=\"Drawing\" style=\"float: center; margin-top: 15px; width: 50%\"/>\n",
    "\n",
    "### EXERCISE:\n",
    "\n",
    "Choose one of the two architectures, and write down an analougus 1-dimensional model. Keep the numbers low, you should avoid either overfitting (see ex.1) as well as a too long training time. \n",
    "\n",
    "After that, try adapting the various (hyper)parameters to lower the loss, and, when you find something plausible, try to recolor the XRF image, as in the lecture. \n",
    "\n",
    "-------------------\n",
    "[1] https://image-net.org/challenges/LSVRC/\n",
    "\n",
    "[2] O. Russakovsky et al., *ImageNet Large Scale Visual Recognition Challenge* https://arxiv.org/abs/1409.0575\n",
    "\n",
    "[3] A. Krizhevsky, I. Sutskever, G. E. Hinton, *ImageNet Classification with Deep ConvolutionalNeural Networks*, doi:10.1145/3065386 https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\n",
    "\n",
    "[4] https://towardsdatascience.com/implementing-alexnet-cnn-architecture-using-tensorflow-2-0-and-keras-2113e090ad98\n",
    "\n",
    "[5] https://www.robots.ox.ac.uk/~vgg/research/very_deep/\n",
    "\n",
    "[6] K. Simonyan, A. Zisserman, *Very Deep Convolutional Networks for Large-Scale Image Recognition*, https://arxiv.org/abs/1409.1556\n",
    "\n",
    "For an extensive recap on ILSVRC and more: https://medium.com/analytics-vidhya/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7cd2ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e570160f",
   "metadata": {},
   "source": [
    "### AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed46de23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1141eb88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d0859f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9254d45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6a8ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51181c54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b48aacf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6728633",
   "metadata": {},
   "source": [
    "### VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a326308f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030b5ca1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4eabb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e727a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c94ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33955017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df24907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4125fbb",
   "metadata": {},
   "source": [
    "## Exercise 3: Design your own DNN\n",
    "\n",
    "**EXERCISE**\n",
    "\n",
    "Build your own Deep Neural Network. Use anything you like from the lecture. \n",
    "\n",
    "PS: a suggestion: try to use the keras functional API, and a non-linear topology. Google \"Deep Learning\" + a title of a famous Christopher Nolan's movie with Di Caprio and Joseph Gordon-Levitt. It could give you inspiration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32da3996",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
